{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Python: Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression with one variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first part of the exercise, we're tasked with implementing linear regression with one variable to predict profits for a food truck. Suppose you are the CEO of a restaurant franchise and are considering different cities for opening a new outlet. The chain already has trucks in various cities and you have data for profits and populations from the cities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing some libraries and examining the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data from the CSV file using Panda library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'DLAV-2025' already exists and is not an empty directory.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Population</th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.1101</td>\n",
       "      <td>17.5920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.5277</td>\n",
       "      <td>9.1302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.5186</td>\n",
       "      <td>13.6620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0032</td>\n",
       "      <td>11.8540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.8598</td>\n",
       "      <td>6.8233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Population   Profit\n",
       "0      6.1101  17.5920\n",
       "1      5.5277   9.1302\n",
       "2      8.5186  13.6620\n",
       "3      7.0032  11.8540\n",
       "4      5.8598   6.8233"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!git clone https://github.com/vita-epfl/DLAV-2025.git\n",
    "path = os.getcwd() + '/DLAV-2025/homeworks/hw1/data/ex1data1.txt'\n",
    "data = pd.read_csv(path, header=None, names=['Population', 'Profit'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Population</th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>97.000000</td>\n",
       "      <td>97.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.159800</td>\n",
       "      <td>5.839135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.869884</td>\n",
       "      <td>5.510262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.026900</td>\n",
       "      <td>-2.680700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.707700</td>\n",
       "      <td>1.986900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.589400</td>\n",
       "      <td>4.562300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.578100</td>\n",
       "      <td>7.046700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>22.203000</td>\n",
       "      <td>24.147000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Population     Profit\n",
       "count   97.000000  97.000000\n",
       "mean     8.159800   5.839135\n",
       "std      3.869884   5.510262\n",
       "min      5.026900  -2.680700\n",
       "25%      5.707700   1.986900\n",
       "50%      6.589400   4.562300\n",
       "75%      8.578100   7.046700\n",
       "max     22.203000  24.147000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot it to get a better idea of what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='Population', ylabel='Profit'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.plot(kind='scatter', x='Population', y='Profit', figsize=(12,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's implement linear regression using gradient descent to minimize the cost function.  The equations implemented in the following code samples are detailed in \"ex1.pdf\" in the \"exercises\" folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll create a function to compute the cost of a given solution (characterized by the parameters theta). The cost function is the Mean Sqaured error in matrix form: \n",
    "\n",
    "$$ MSE(\\theta) = \\frac{1}{N}\\sum_n^N [ y_n-x_n^T*\\theta]^2 $$\n",
    "\n",
    "where $\\theta$ and $x_n$ are vectors\n",
    "\n",
    "__Hint__: Use the matrix form of the cost function and make use of numpy operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCost(x, y, theta):\n",
    "    N = y.shape[0]\n",
    "    theta = theta.reshape(-1,1)\n",
    "    y = y.reshape(-1, 1)\n",
    "    # print(x@theta)\n",
    "    # print(y)\n",
    "    cost = 1/N * np.sum(np.square(y - x@theta))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a column of ones to the training set so we can use a vectorized solution to computing the cost and gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot insert Ones, already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13604\\617517762.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Ones'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\jeffr\\anaconda3\\envs\\dlav\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, loc, column, value, allow_duplicates)\u001b[0m\n\u001b[0;32m   5154\u001b[0m                 \u001b[1;34m\"'self.flags.allows_duplicate_labels' is False.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5155\u001b[0m             \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5156\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mallow_duplicates\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5157\u001b[0m             \u001b[1;31m# Should this be a different kind of error??\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5158\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33mf\"\u001b[0m\u001b[1;33mcannot insert \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m, already exists\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5159\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5160\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loc must be int\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5161\u001b[0m         \u001b[1;31m# convert non stdlib ints to satisfy typing checks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot insert Ones, already exists"
     ]
    }
   ],
   "source": [
    "data.insert(0, 'Ones', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do some variable initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set X (training data) and y (target variable)\n",
    "cols = data.shape[1]\n",
    "X = data.iloc[:,0:cols-1]\n",
    "y = data.iloc[:,cols-1:cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look to make sure X (training set) and y (target variable) look correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ones</th>\n",
       "      <th>Population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6.1101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5.5277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>8.5186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>7.0032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5.8598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Ones  Population\n",
       "0     1      6.1101\n",
       "1     1      5.5277\n",
       "2     1      8.5186\n",
       "3     1      7.0032\n",
       "4     1      5.8598"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.5920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.1302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.6620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.8540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.8233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Profit\n",
       "0  17.5920\n",
       "1   9.1302\n",
       "2  13.6620\n",
       "3  11.8540\n",
       "4   6.8233"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert X and Y to numpy array for better manipulation. Initiliaze Theta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(X.values)\n",
    "y = np.array(y.values).flatten()\n",
    "theta = np.array([0,0])\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at the shape of our matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((97, 2), (2,), (97,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, theta.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the cost for our initial solution (0 values for theta)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y:  [17.592    9.1302  13.662   11.854    6.8233  11.886    4.3483  12.\n",
      "  6.5987   3.8166   3.2522  15.505    3.1551   7.2258   0.71618  3.5129\n",
      "  5.3048   0.56077  3.6518   5.3893   3.1386  21.767    4.263    5.1875\n",
      "  3.0825  22.638   13.501    7.0467  14.692   24.147   -1.22     5.9966\n",
      " 12.134    1.8495   6.5426   4.5623   4.1164   3.3928  10.117    5.4974\n",
      "  0.55657  3.9115   5.3854   2.4406   6.7318   1.0463   5.1337   1.844\n",
      "  8.0043   1.0179   6.7504   1.8396   4.2885   4.9981   1.4233  -1.4211\n",
      "  2.4756   4.6042   3.9624   5.4141   5.1694  -0.74279 17.929   12.054\n",
      " 17.054    4.8852   5.7442   7.7754   1.0173  20.992    6.6799   4.0259\n",
      "  1.2784   3.3411  -2.6807   0.29678  3.8845   5.7014   6.7526   2.0576\n",
      "  0.47953  0.20421  0.67861  7.5435   5.3436   4.2415   6.7981   0.92695\n",
      "  0.152    2.8214   1.8451   4.2959   7.2029   1.9869   0.14454  9.0551\n",
      "  0.61705]\n",
      "X:  [[ 1.      6.1101]\n",
      " [ 1.      5.5277]\n",
      " [ 1.      8.5186]\n",
      " [ 1.      7.0032]\n",
      " [ 1.      5.8598]\n",
      " [ 1.      8.3829]\n",
      " [ 1.      7.4764]\n",
      " [ 1.      8.5781]\n",
      " [ 1.      6.4862]\n",
      " [ 1.      5.0546]\n",
      " [ 1.      5.7107]\n",
      " [ 1.     14.164 ]\n",
      " [ 1.      5.734 ]\n",
      " [ 1.      8.4084]\n",
      " [ 1.      5.6407]\n",
      " [ 1.      5.3794]\n",
      " [ 1.      6.3654]\n",
      " [ 1.      5.1301]\n",
      " [ 1.      6.4296]\n",
      " [ 1.      7.0708]\n",
      " [ 1.      6.1891]\n",
      " [ 1.     20.27  ]\n",
      " [ 1.      5.4901]\n",
      " [ 1.      6.3261]\n",
      " [ 1.      5.5649]\n",
      " [ 1.     18.945 ]\n",
      " [ 1.     12.828 ]\n",
      " [ 1.     10.957 ]\n",
      " [ 1.     13.176 ]\n",
      " [ 1.     22.203 ]\n",
      " [ 1.      5.2524]\n",
      " [ 1.      6.5894]\n",
      " [ 1.      9.2482]\n",
      " [ 1.      5.8918]\n",
      " [ 1.      8.2111]\n",
      " [ 1.      7.9334]\n",
      " [ 1.      8.0959]\n",
      " [ 1.      5.6063]\n",
      " [ 1.     12.836 ]\n",
      " [ 1.      6.3534]\n",
      " [ 1.      5.4069]\n",
      " [ 1.      6.8825]\n",
      " [ 1.     11.708 ]\n",
      " [ 1.      5.7737]\n",
      " [ 1.      7.8247]\n",
      " [ 1.      7.0931]\n",
      " [ 1.      5.0702]\n",
      " [ 1.      5.8014]\n",
      " [ 1.     11.7   ]\n",
      " [ 1.      5.5416]\n",
      " [ 1.      7.5402]\n",
      " [ 1.      5.3077]\n",
      " [ 1.      7.4239]\n",
      " [ 1.      7.6031]\n",
      " [ 1.      6.3328]\n",
      " [ 1.      6.3589]\n",
      " [ 1.      6.2742]\n",
      " [ 1.      5.6397]\n",
      " [ 1.      9.3102]\n",
      " [ 1.      9.4536]\n",
      " [ 1.      8.8254]\n",
      " [ 1.      5.1793]\n",
      " [ 1.     21.279 ]\n",
      " [ 1.     14.908 ]\n",
      " [ 1.     18.959 ]\n",
      " [ 1.      7.2182]\n",
      " [ 1.      8.2951]\n",
      " [ 1.     10.236 ]\n",
      " [ 1.      5.4994]\n",
      " [ 1.     20.341 ]\n",
      " [ 1.     10.136 ]\n",
      " [ 1.      7.3345]\n",
      " [ 1.      6.0062]\n",
      " [ 1.      7.2259]\n",
      " [ 1.      5.0269]\n",
      " [ 1.      6.5479]\n",
      " [ 1.      7.5386]\n",
      " [ 1.      5.0365]\n",
      " [ 1.     10.274 ]\n",
      " [ 1.      5.1077]\n",
      " [ 1.      5.7292]\n",
      " [ 1.      5.1884]\n",
      " [ 1.      6.3557]\n",
      " [ 1.      9.7687]\n",
      " [ 1.      6.5159]\n",
      " [ 1.      8.5172]\n",
      " [ 1.      9.1802]\n",
      " [ 1.      6.002 ]\n",
      " [ 1.      5.5204]\n",
      " [ 1.      5.0594]\n",
      " [ 1.      5.7077]\n",
      " [ 1.      7.6366]\n",
      " [ 1.      5.8707]\n",
      " [ 1.      5.3054]\n",
      " [ 1.      8.2934]\n",
      " [ 1.     13.394 ]\n",
      " [ 1.      5.4369]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(64.14546775491135)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"y: \", y)\n",
    "print(\"X: \", X)\n",
    "computeCost(X, y, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far so good.  Now we need to define a function to perform gradient descent on the parameters theta using the update rules. Write first a function that computes the gradient of a matrix and then use it in the gradientDescent function.\n",
    "\n",
    "The gradient descent formula is:\n",
    "\n",
    "$$\\theta^{t+1} = \\theta^{t} - \\alpha*\\nabla MSE(\\theta^{t})$$\n",
    "\n",
    "where $\\nabla MSE(\\theta^{t})$ is the gradient of the cost function at $\\theta^{t}$\n",
    "\n",
    "__Hint__: Use the matrix form of the gradient and make use of numpy operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute gradient and loss\n",
    "    n = y.shape[0]\n",
    "    # print(n)\n",
    "    # print(\"this thing\", tx@w)\n",
    "    tx = tx.reshape(-1, 2)\n",
    "    gradient = -1/n * tx.T @ (y-tx@w)\n",
    "\n",
    "    # gradient = 2*(y-tx@w)/n\n",
    "    # ***************************************************\n",
    "    print(gradient)\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X, y, theta, alpha,max_iters):\n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [theta]\n",
    "    cost = np.zeros(max_iters)\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: compute gradient and loss\n",
    "        grad = compute_gradient(y, X, theta)\n",
    "        # print(grad)\n",
    "        loss = computeCost(X, y, theta)\n",
    "        print(loss)\n",
    "        # ***************************************************\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: update theta by gradient\n",
    "        print(grad)\n",
    "        theta = theta - (alpha * grad)\n",
    "        # ***************************************************\n",
    "        # store w and loss\n",
    "        ws.append(theta)\n",
    "        cost[n_iter] = loss\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=theta[0], w1=theta[1]))\n",
    "\n",
    "    return theta, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize some additional variables - the learning rate alpha, and the number of iterations to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "iters = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the gradient descent algorithm to fit our parameters theta to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -5.83913505 -65.32884975]\n",
      "64.14546775491135\n",
      "[ -5.83913505 -65.32884975]\n",
      "Gradient Descent(0/999): loss=64.14546775491135, w0=0.0583913505154639, w1=0.6532884974555672\n",
      "[ -0.45004022 -11.67212851]\n",
      "13.474380929740013\n",
      "[ -0.45004022 -11.67212851]\n",
      "Gradient Descent(1/999): loss=13.474380929740013, w0=0.06289175271039381, w1=0.7700097825599365\n",
      "[ 0.50688252 -2.13383331]\n",
      "11.863187137209911\n",
      "[ 0.50688252 -2.13383331]\n",
      "Gradient Descent(2/999): loss=11.863187137209911, w0=0.057822927461428086, w1=0.7913481156584673\n",
      "[ 0.67593023 -0.43816946]\n",
      "11.802309414162776\n",
      "[ 0.67593023 -0.43816946]\n",
      "Gradient Descent(3/999): loss=11.802309414162776, w0=0.05106362516077811, w1=0.795729810284954\n",
      "[ 0.70492468 -0.1366368 ]\n",
      "11.790457172888441\n",
      "[ 0.70492468 -0.1366368 ]\n",
      "Gradient Descent(4/999): loss=11.790457172888441, w0=0.04401437836500256, w1=0.7970961782721866\n",
      "[ 0.70902472 -0.0829295 ]\n",
      "11.780189886234659\n",
      "[ 0.70902472 -0.0829295 ]\n",
      "Gradient Descent(5/999): loss=11.780189886234659, w0=0.03692413114216258, w1=0.7979254732843951\n",
      "[ 0.70870136 -0.07327662]\n",
      "11.770008316887294\n",
      "[ 0.70870136 -0.07327662]\n",
      "Gradient Descent(6/999): loss=11.770008316887294, w0=0.029837117577144797, w1=0.7986582394519285\n",
      "[ 0.70759357 -0.07145517]\n",
      "11.759864960982837\n",
      "[ 0.70759357 -0.07145517]\n",
      "Gradient Descent(7/999): loss=11.759864960982837, w0=0.022761181894038807, w1=0.7993727912003019\n",
      "[ 0.70634823 -0.07102607]\n",
      "11.74975818952515\n",
      "[ 0.70634823 -0.07102607]\n",
      "Gradient Descent(8/999): loss=11.74975818952515, w0=0.015697699574200107, w1=0.8000830518518655\n",
      "[ 0.70508033 -0.07084465]\n",
      "11.73968782361277\n",
      "[ 0.70508033 -0.07084465]\n",
      "Gradient Descent(9/999): loss=11.73968782361277, w0=0.008646896228913507, w1=0.8007914983590768\n",
      "[ 0.70381031 -0.07070746]\n",
      "11.72965373062586\n",
      "[ 0.70381031 -0.07070746]\n",
      "Gradient Descent(10/999): loss=11.72965373062586, w0=0.0016087930989843345, w1=0.8014985729280016\n",
      "[ 0.7025418  -0.07057831]\n",
      "11.719655779864363\n",
      "[ 0.7025418  -0.07057831]\n",
      "Gradient Descent(11/999): loss=11.719655779864363, w0=-0.005416624870320673, w1=0.8022043560583255\n",
      "[ 0.70127543 -0.07045079]\n",
      "11.70969384114458\n",
      "[ 0.70127543 -0.07045079]\n",
      "Gradient Descent(12/999): loss=11.70969384114458, w0=-0.012429379151800795, w1=0.8029088639482521\n",
      "[ 0.70001132 -0.07032374]\n",
      "11.699767784753174\n",
      "[ 0.70001132 -0.07032374]\n",
      "Gradient Descent(13/999): loss=11.699767784753174, w0=-0.019429492325268343, w1=0.8036121013621462\n",
      "[ 0.69874948 -0.07019697]\n",
      "11.689877481444068\n",
      "[ 0.69874948 -0.07019697]\n",
      "Gradient Descent(14/999): loss=11.689877481444068, w0=-0.026416987133500148, w1=0.8043140710284592\n",
      "[ 0.69748992 -0.07007043]\n",
      "11.680022802436723\n",
      "[ 0.69748992 -0.07007043]\n",
      "Gradient Descent(15/999): loss=11.680022802436723, w0=-0.03339188631448144, w1=0.8050147753103407\n",
      "[ 0.69623263 -0.06994412]\n",
      "11.670203619414451\n",
      "[ 0.69623263 -0.06994412]\n",
      "Gradient Descent(16/999): loss=11.670203619414451, w0=-0.04035421257164589, w1=0.8057142165026173\n",
      "[ 0.6949776  -0.06981804]\n",
      "11.660419804522776\n",
      "[ 0.6949776  -0.06981804]\n",
      "Gradient Descent(17/999): loss=11.660419804522776, w0=-0.04730398856864608, w1=0.8064123968845912\n",
      "[ 0.69372484 -0.06969218]\n",
      "11.650671230367726\n",
      "[ 0.69372484 -0.06969218]\n",
      "Gradient Descent(18/999): loss=11.650671230367726, w0=-0.05424123692848457, w1=0.8071093187294315\n",
      "[ 0.69247433 -0.06956656]\n",
      "11.640957770014198\n",
      "[ 0.69247433 -0.06956656]\n",
      "Gradient Descent(19/999): loss=11.640957770014198, w0=-0.06116598023341996, w1=0.8078049843058496\n",
      "[ 0.69122608 -0.06944116]\n",
      "11.631279296984308\n",
      "[ 0.69122608 -0.06944116]\n",
      "Gradient Descent(20/999): loss=11.631279296984308, w0=-0.06807824102501056, w1=0.8084993958784036\n",
      "[ 0.68998008 -0.06931598]\n",
      "11.621635685255738\n",
      "[ 0.68998008 -0.06931598]\n",
      "Gradient Descent(21/999): loss=11.621635685255738, w0=-0.07497804180418251, w1=0.8091925557075582\n",
      "[ 0.68873632 -0.06919103]\n",
      "11.612026809260088\n",
      "[ 0.68873632 -0.06919103]\n",
      "Gradient Descent(22/999): loss=11.612026809260088, w0=-0.0818654050313021, w1=0.8098844660497012\n",
      "[ 0.68749481 -0.06906631]\n",
      "11.602452543881256\n",
      "[ 0.68749481 -0.06906631]\n",
      "Gradient Descent(23/999): loss=11.602452543881256, w0=-0.08874035312624869, w1=0.8105751291571527\n",
      "[ 0.68625553 -0.06894181]\n",
      "11.5929127644538\n",
      "[ 0.68625553 -0.06894181]\n",
      "Gradient Descent(24/999): loss=11.5929127644538, w0=-0.09560290846848762, w1=0.8112645472781728\n",
      "[ 0.68501849 -0.06881754]\n",
      "11.583407346761307\n",
      "[ 0.68501849 -0.06881754]\n",
      "Gradient Descent(25/999): loss=11.583407346761307, w0=-0.10245309339714317, w1=0.8119527226569687\n",
      "[ 0.68378368 -0.06869349]\n",
      "11.573936167034791\n",
      "[ 0.68378368 -0.06869349]\n",
      "Gradient Descent(26/999): loss=11.573936167034791, w0=-0.10929093021107116, w1=0.8126396575337026\n",
      "[ 0.6825511  -0.06856966]\n",
      "11.564499101951078\n",
      "[ 0.6825511  -0.06856966]\n",
      "Gradient Descent(27/999): loss=11.564499101951078, w0=-0.11611644116893159, w1=0.813325354144498\n",
      "[ 0.68132073 -0.06844606]\n",
      "11.555096028631192\n",
      "[ 0.68132073 -0.06844606]\n",
      "Gradient Descent(28/999): loss=11.555096028631192, w0=-0.1229296484892611, w1=0.8140098147214483\n",
      "[ 0.68009259 -0.06832268]\n",
      "11.545726824638761\n",
      "[ 0.68009259 -0.06832268]\n",
      "Gradient Descent(29/999): loss=11.545726824638761, w0=-0.1297305743505453, w1=0.8146930414926227\n",
      "[ 0.67886665 -0.06819952]\n",
      "11.536391367978425\n",
      "[ 0.67886665 -0.06819952]\n",
      "Gradient Descent(30/999): loss=11.536391367978425, w0=-0.13651924089129094, w1=0.8153750366820742\n",
      "[ 0.67764293 -0.06807658]\n",
      "11.52708953709424\n",
      "[ 0.67764293 -0.06807658]\n",
      "Gradient Descent(31/999): loss=11.52708953709424, w0=-0.143295670210098, w1=0.8160558025098472\n",
      "[ 0.67642142 -0.06795387]\n",
      "11.517821210868094\n",
      "[ 0.67642142 -0.06795387]\n",
      "Gradient Descent(32/999): loss=11.517821210868094, w0=-0.15005988436573162, w1=0.8167353411919838\n",
      "[ 0.6752021  -0.06783137]\n",
      "11.508586268618155\n",
      "[ 0.6752021  -0.06783137]\n",
      "Gradient Descent(33/999): loss=11.508586268618155, w0=-0.1568119053771939, w1=0.8174136549405314\n",
      "[ 0.67398498 -0.0677091 ]\n",
      "11.499384590097256\n",
      "[ 0.67398498 -0.0677091 ]\n",
      "Gradient Descent(34/999): loss=11.499384590097256, w0=-0.1635517552237955, w1=0.8180907459635505\n",
      "[ 0.67277006 -0.06758705]\n",
      "11.490216055491368\n",
      "[ 0.67277006 -0.06758705]\n",
      "Gradient Descent(35/999): loss=11.490216055491368, w0=-0.1702794558452274, w1=0.8187666164651208\n",
      "[ 0.67155733 -0.06746522]\n",
      "11.481080545418024\n",
      "[ 0.67155733 -0.06746522]\n",
      "Gradient Descent(36/999): loss=11.481080545418024, w0=-0.17699502914163215, w1=0.8194412686453493\n",
      "[ 0.67034678 -0.06734361]\n",
      "11.471977940924763\n",
      "[ 0.67034678 -0.06734361]\n",
      "Gradient Descent(37/999): loss=11.471977940924763, w0=-0.18369849697367513, w1=0.8201147047003767\n",
      "[ 0.66913842 -0.06722221]\n",
      "11.462908123487583\n",
      "[ 0.66913842 -0.06722221]\n",
      "Gradient Descent(38/999): loss=11.462908123487583, w0=-0.1903898811626158, w1=0.8207869268223855\n",
      "[ 0.66793223 -0.06710104]\n",
      "11.453870975009403\n",
      "[ 0.66793223 -0.06710104]\n",
      "Gradient Descent(39/999): loss=11.453870975009403, w0=-0.19706920349037874, w1=0.8214579371996062\n",
      "[ 0.66672822 -0.06698008]\n",
      "11.444866377818514\n",
      "[ 0.66672822 -0.06698008]\n",
      "Gradient Descent(40/999): loss=11.444866377818514, w0=-0.2037364856996245, w1=0.822127738016325\n",
      "[ 0.66552638 -0.06685934]\n",
      "11.435894214667057\n",
      "[ 0.66552638 -0.06685934]\n",
      "Gradient Descent(41/999): loss=11.435894214667057, w0=-0.21039174949382042, w1=0.8227963314528903\n",
      "[ 0.6643267  -0.06673882]\n",
      "11.426954368729499\n",
      "[ 0.6643267  -0.06673882]\n",
      "Gradient Descent(42/999): loss=11.426954368729499, w0=-0.21703501653731125, w1=0.8234637196857207\n",
      "[ 0.66312919 -0.06661852]\n",
      "11.418046723601098\n",
      "[ 0.66312919 -0.06661852]\n",
      "Gradient Descent(43/999): loss=11.418046723601098, w0=-0.22366630845538965, w1=0.8241299048873114\n",
      "[ 0.66193384 -0.06649843]\n",
      "11.409171163296396\n",
      "[ 0.66193384 -0.06649843]\n",
      "Gradient Descent(44/999): loss=11.409171163296396, w0=-0.23028564683436667, w1=0.8247948892262416\n",
      "[ 0.66074064 -0.06637856]\n",
      "11.400327572247713\n",
      "[ 0.66074064 -0.06637856]\n",
      "Gradient Descent(45/999): loss=11.400327572247713, w0=-0.23689305322164195, w1=0.8254586748671812\n",
      "[ 0.65954959 -0.06625891]\n",
      "11.39151583530363\n",
      "[ 0.65954959 -0.06625891]\n",
      "Gradient Descent(46/999): loss=11.39151583530363, w0=-0.24348854912577386, w1=0.8261212639708981\n",
      "[ 0.65836069 -0.06613947]\n",
      "11.382735837727504\n",
      "[ 0.65836069 -0.06613947]\n",
      "Gradient Descent(47/999): loss=11.382735837727504, w0=-0.25007215601654953, w1=0.8267826586942655\n",
      "[ 0.65717393 -0.06602025]\n",
      "11.37398746519596\n",
      "[ 0.65717393 -0.06602025]\n",
      "Gradient Descent(48/999): loss=11.37398746519596, w0=-0.25664389532505477, w1=0.8274428611902682\n",
      "[ 0.65598931 -0.06590124]\n",
      "11.365270603797414\n",
      "[ 0.65598931 -0.06590124]\n",
      "Gradient Descent(49/999): loss=11.365270603797414, w0=-0.2632037884437438, w1=0.8281018736080105\n",
      "[ 0.65480683 -0.06578245]\n",
      "11.356585140030584\n",
      "[ 0.65480683 -0.06578245]\n",
      "Gradient Descent(50/999): loss=11.356585140030584, w0=-0.2697518567265089, w1=0.8287596980927223\n",
      "[ 0.65362648 -0.06566387]\n",
      "11.347930960803012\n",
      "[ 0.65362648 -0.06566387]\n",
      "Gradient Descent(51/999): loss=11.347930960803012, w0=-0.27628812148874987, w1=0.8294163367857669\n",
      "[ 0.65244825 -0.0655455 ]\n",
      "11.339307953429591\n",
      "[ 0.65244825 -0.0655455 ]\n",
      "Gradient Descent(52/999): loss=11.339307953429591, w0=-0.28281260400744346, w1=0.8300717918246473\n",
      "[ 0.65127215 -0.06542735]\n",
      "11.330716005631107\n",
      "[ 0.65127215 -0.06542735]\n",
      "Gradient Descent(53/999): loss=11.330716005631107, w0=-0.28932532552121265, w1=0.8307260653430135\n",
      "[ 0.65009817 -0.06530941]\n",
      "11.322155005532759\n",
      "[ 0.65009817 -0.06530941]\n",
      "Gradient Descent(54/999): loss=11.322155005532759, w0=-0.2958263072303958, w1=0.8313791594706695\n",
      "[ 0.64892631 -0.06519169]\n",
      "11.313624841662719\n",
      "[ 0.64892631 -0.06519169]\n",
      "Gradient Descent(55/999): loss=11.313624841662719, w0=-0.30231557029711564, w1=0.8320310763335801\n",
      "[ 0.64775655 -0.06507417]\n",
      "11.305125402950667\n",
      "[ 0.64775655 -0.06507417]\n",
      "Gradient Descent(56/999): loss=11.305125402950667, w0=-0.30879313584534807, w1=0.8326818180538778\n",
      "[ 0.64658891 -0.06495687]\n",
      "11.296656578726362\n",
      "[ 0.64658891 -0.06495687]\n",
      "Gradient Descent(57/999): loss=11.296656578726362, w0=-0.315259024960991, w1=0.8333313867498697\n",
      "[ 0.64542337 -0.06483978]\n",
      "11.288218258718183\n",
      "[ 0.64542337 -0.06483978]\n",
      "Gradient Descent(58/999): loss=11.288218258718183, w0=-0.32171325869193307, w1=0.8339797845360446\n",
      "[ 0.64425994 -0.0647229 ]\n",
      "11.279810333051708\n",
      "[ 0.64425994 -0.0647229 ]\n",
      "Gradient Descent(59/999): loss=11.279810333051708, w0=-0.328155858048122, w1=0.8346270135230796\n",
      "[ 0.6430986  -0.06460623]\n",
      "11.27143269224827\n",
      "[ 0.6430986  -0.06460623]\n",
      "Gradient Descent(60/999): loss=11.27143269224827, w0=-0.3345868440016331, w1=0.835273075817847\n",
      "[ 0.64193935 -0.06448977]\n",
      "11.263085227223543\n",
      "[ 0.64193935 -0.06448977]\n",
      "Gradient Descent(61/999): loss=11.263085227223543, w0=-0.34100623748673753, w1=0.8359179735234217\n",
      "[ 0.64078219 -0.06437352]\n",
      "11.254767829286108\n",
      "[ 0.64078219 -0.06437352]\n",
      "Gradient Descent(62/999): loss=11.254767829286108, w0=-0.3474140593999704, w1=0.8365617087390872\n",
      "[ 0.63962712 -0.06425748]\n",
      "11.246480390136051\n",
      "[ 0.63962712 -0.06425748]\n",
      "Gradient Descent(63/999): loss=11.246480390136051, w0=-0.3538103306001988, w1=0.8372042835603427\n",
      "[ 0.63847413 -0.06414165]\n",
      "11.238222801863555\n",
      "[ 0.63847413 -0.06414165]\n",
      "Gradient Descent(64/999): loss=11.238222801863555, w0=-0.3601950719086897, w1=0.8378457000789108\n",
      "[ 0.63732322 -0.06402603]\n",
      "11.22999495694748\n",
      "[ 0.63732322 -0.06402603]\n",
      "Gradient Descent(65/999): loss=11.22999495694748, w0=-0.36656830410917784, w1=0.8384859603827427\n",
      "[ 0.63617438 -0.06391062]\n",
      "11.221796748253968\n",
      "[ 0.63617438 -0.06391062]\n",
      "Gradient Descent(66/999): loss=11.221796748253968, w0=-0.37293004794793316, w1=0.8391250665560263\n",
      "[ 0.63502762 -0.06379541]\n",
      "11.21362806903506\n",
      "[ 0.63502762 -0.06379541]\n",
      "Gradient Descent(67/999): loss=11.21362806903506, w0=-0.37928032413382856, w1=0.8397630206791926\n",
      "[ 0.63388292 -0.06368041]\n",
      "11.205488812927292\n",
      "[ 0.63388292 -0.06368041]\n",
      "Gradient Descent(68/999): loss=11.205488812927292, w0=-0.38561915333840713, w1=0.8403998248289223\n",
      "[ 0.63274029 -0.06356562]\n",
      "11.19737887395032\n",
      "[ 0.63274029 -0.06356562]\n",
      "Gradient Descent(69/999): loss=11.19737887395032, w0=-0.39194655619594954, w1=0.8410354810781527\n",
      "[ 0.63159971 -0.06345104]\n",
      "11.189298146505518\n",
      "[ 0.63159971 -0.06345104]\n",
      "Gradient Descent(70/999): loss=11.189298146505518, w0=-0.3982625533035412, w1=0.8416699914960846\n",
      "[ 0.63046119 -0.06333667]\n",
      "11.181246525374647\n",
      "[ 0.63046119 -0.06333667]\n",
      "Gradient Descent(71/999): loss=11.181246525374647, w0=-0.4045671652211394, w1=0.8423033581481884\n",
      "[ 0.62932473 -0.06322249]\n",
      "11.173223905718434\n",
      "[ 0.62932473 -0.06322249]\n",
      "Gradient Descent(72/999): loss=11.173223905718434, w0=-0.41086041247163996, w1=0.8429355830962117\n",
      "[ 0.62819031 -0.06310853]\n",
      "11.165230183075245\n",
      "[ 0.62819031 -0.06310853]\n",
      "Gradient Descent(73/999): loss=11.165230183075245, w0=-0.41714231554094433, w1=0.8435666683981857\n",
      "[ 0.62705793 -0.06299477]\n",
      "11.157265253359704\n",
      "[ 0.62705793 -0.06299477]\n",
      "Gradient Descent(74/999): loss=11.157265253359704, w0=-0.42341289487802614, w1=0.8441966161084314\n",
      "[ 0.6259276  -0.06288122]\n",
      "11.149329012861356\n",
      "[ 0.6259276  -0.06288122]\n",
      "Gradient Descent(75/999): loss=11.149329012861356, w0=-0.42967217089499776, w1=0.8448254282775671\n",
      "[ 0.62479931 -0.06276787]\n",
      "11.141421358243287\n",
      "[ 0.62479931 -0.06276787]\n",
      "Gradient Descent(76/999): loss=11.141421358243287, w0=-0.4359201639671768, w1=0.8454531069525142\n",
      "[ 0.62367305 -0.06265472]\n",
      "11.133542186540806\n",
      "[ 0.62367305 -0.06265472]\n",
      "Gradient Descent(77/999): loss=11.133542186540806, w0=-0.44215689443315237, w1=0.8460796541765048\n",
      "[ 0.62254882 -0.06254178]\n",
      "11.125691395160102\n",
      "[ 0.62254882 -0.06254178]\n",
      "Gradient Descent(78/999): loss=11.125691395160102, w0=-0.44838238259485136, w1=0.8467050719890875\n",
      "[ 0.62142661 -0.06242904]\n",
      "11.117868881876884\n",
      "[ 0.62142661 -0.06242904]\n",
      "Gradient Descent(79/999): loss=11.117868881876884, w0=-0.45459664871760447, w1=0.8473293624261347\n",
      "[ 0.62030643 -0.06231651]\n",
      "11.110074544835085\n",
      "[ 0.62030643 -0.06231651]\n",
      "Gradient Descent(80/999): loss=11.110074544835085, w0=-0.4607997130302122, w1=0.8479525275198488\n",
      "[ 0.61918827 -0.06220418]\n",
      "11.102308282545506\n",
      "[ 0.61918827 -0.06220418]\n",
      "Gradient Descent(81/999): loss=11.102308282545506, w0=-0.4669915957250108, w1=0.8485745692987691\n",
      "[ 0.61807212 -0.06209205]\n",
      "11.094569993884512\n",
      "[ 0.61807212 -0.06209205]\n",
      "Gradient Descent(82/999): loss=11.094569993884512, w0=-0.47317231695793777, w1=0.8491954897877779\n",
      "[ 0.61695799 -0.06198012]\n",
      "11.086859578092703\n",
      "[ 0.61695799 -0.06198012]\n",
      "Gradient Descent(83/999): loss=11.086859578092703, w0=-0.47934189684859757, w1=0.8498152910081079\n",
      "[ 0.61584586 -0.0618684 ]\n",
      "11.079176934773615\n",
      "[ 0.61584586 -0.0618684 ]\n",
      "Gradient Descent(84/999): loss=11.079176934773615, w0=-0.4855003554803273, w1=0.8504339749773481\n",
      "[ 0.61473574 -0.06175687]\n",
      "11.071521963892407\n",
      "[ 0.61473574 -0.06175687]\n",
      "Gradient Descent(85/999): loss=11.071521963892407, w0=-0.4916477129002617, w1=0.8510515437094506\n",
      "[ 0.61362762 -0.06164555]\n",
      "11.06389456577455\n",
      "[ 0.61362762 -0.06164555]\n",
      "Gradient Descent(86/999): loss=11.06389456577455, w0=-0.4977839891193989, w1=0.8516679992147372\n",
      "[ 0.6125215  -0.06153443]\n",
      "11.056294641104543\n",
      "[ 0.6125215  -0.06153443]\n",
      "Gradient Descent(87/999): loss=11.056294641104543, w0=-0.5039092041126652, w1=0.8522833434999061\n",
      "[ 0.61141737 -0.06142351]\n",
      "11.048722090924612\n",
      "[ 0.61141737 -0.06142351]\n",
      "Gradient Descent(88/999): loss=11.048722090924612, w0=-0.5100233778189799, w1=0.8528975785680377\n",
      "[ 0.61031523 -0.06131279]\n",
      "11.041176816633426\n",
      "[ 0.61031523 -0.06131279]\n",
      "Gradient Descent(89/999): loss=11.041176816633426, w0=-0.5161265301413209, w1=0.8535107064186024\n",
      "[ 0.60921508 -0.06120226]\n",
      "11.033658719984805\n",
      "[ 0.60921508 -0.06120226]\n",
      "Gradient Descent(90/999): loss=11.033658719984805, w0=-0.5222186809467889, w1=0.8541227290474656\n",
      "[ 0.60811691 -0.06109194]\n",
      "11.02616770308645\n",
      "[ 0.60811691 -0.06109194]\n",
      "Gradient Descent(91/999): loss=11.02616770308645, w0=-0.5282998500666722, w1=0.8547336484468955\n",
      "[ 0.60702072 -0.06098182]\n",
      "11.018703668398663\n",
      "[ 0.60702072 -0.06098182]\n",
      "Gradient Descent(92/999): loss=11.018703668398663, w0=-0.5343700572965113, w1=0.8553434666055688\n",
      "[ 0.60592651 -0.06087189]\n",
      "11.011266518733077\n",
      "[ 0.60592651 -0.06087189]\n",
      "Gradient Descent(93/999): loss=11.011266518733077, w0=-0.5404293223961635, w1=0.8559521855085775\n",
      "[ 0.60483427 -0.06076216]\n",
      "11.003856157251398\n",
      "[ 0.60483427 -0.06076216]\n",
      "Gradient Descent(94/999): loss=11.003856157251398, w0=-0.5464776650898668, w1=0.8565598071374354\n",
      "[ 0.603744   -0.06065263]\n",
      "10.99647248746413\n",
      "[ 0.603744   -0.06065263]\n",
      "Gradient Descent(95/999): loss=10.99647248746413, w0=-0.5525151050663047, w1=0.8571663334700841\n",
      "[ 0.60265569 -0.0605433 ]\n",
      "10.989115413229332\n",
      "[ 0.60265569 -0.0605433 ]\n",
      "Gradient Descent(96/999): loss=10.989115413229332, w0=-0.5585416619786697, w1=0.8577717664809001\n",
      "[ 0.60156935 -0.06043417]\n",
      "10.981784838751354\n",
      "[ 0.60156935 -0.06043417]\n",
      "Gradient Descent(97/999): loss=10.981784838751354, w0=-0.5645573554447276, w1=0.8583761081407008\n",
      "[ 0.60048496 -0.06032523]\n",
      "10.97448066857961\n",
      "[ 0.60048496 -0.06032523]\n",
      "Gradient Descent(98/999): loss=10.97448066857961, w0=-0.5705622050468814, w1=0.8589793604167508\n",
      "[ 0.59940253 -0.06021649]\n",
      "10.967202807607302\n",
      "[ 0.59940253 -0.06021649]\n",
      "Gradient Descent(99/999): loss=10.967202807607302, w0=-0.5765562303322347, w1=0.8595815252727688\n",
      "[ 0.59832205 -0.06010794]\n",
      "10.959951161070224\n",
      "[ 0.59832205 -0.06010794]\n",
      "Gradient Descent(100/999): loss=10.959951161070224, w0=-0.5825394508126558, w1=0.8601826046689336\n",
      "[ 0.59724352 -0.05999959]\n",
      "10.952725634545482\n",
      "[ 0.59724352 -0.05999959]\n",
      "Gradient Descent(101/999): loss=10.952725634545482, w0=-0.5885118859648409, w1=0.8607826005618906\n",
      "[ 0.59616693 -0.05989143]\n",
      "10.945526133950302\n",
      "[ 0.59616693 -0.05989143]\n",
      "Gradient Descent(102/999): loss=10.945526133950302, w0=-0.5944735552303778, w1=0.8613815149047582\n",
      "[ 0.59509228 -0.05978347]\n",
      "10.938352565540796\n",
      "[ 0.59509228 -0.05978347]\n",
      "Gradient Descent(103/999): loss=10.938352565540796, w0=-0.6004244780158086, w1=0.861979349647134\n",
      "[ 0.59401957 -0.05967571]\n",
      "10.931204835910716\n",
      "[ 0.59401957 -0.05967571]\n",
      "Gradient Descent(104/999): loss=10.931204835910716, w0=-0.6063646736926934, w1=0.8625761067351013\n",
      "[ 0.59294879 -0.05956814]\n",
      "10.924082851990272\n",
      "[ 0.59294879 -0.05956814]\n",
      "Gradient Descent(105/999): loss=10.924082851990272, w0=-0.6122941615976734, w1=0.8631717881112356\n",
      "[ 0.59187994 -0.05946076]\n",
      "10.916986521044917\n",
      "[ 0.59187994 -0.05946076]\n",
      "Gradient Descent(106/999): loss=10.916986521044917, w0=-0.6182129610325333, w1=0.8637663957146104\n",
      "[ 0.59081302 -0.05935358]\n",
      "10.909915750674093\n",
      "[ 0.59081302 -0.05935358]\n",
      "Gradient Descent(107/999): loss=10.909915750674093, w0=-0.6241210912642648, w1=0.864359931480804\n",
      "[ 0.58974803 -0.05924659]\n",
      "10.902870448810102\n",
      "[ 0.58974803 -0.05924659]\n",
      "Gradient Descent(108/999): loss=10.902870448810102, w0=-0.630018571525129, w1=0.8649523973419058\n",
      "[ 0.58868495 -0.05913979]\n",
      "10.895850523716845\n",
      "[ 0.58868495 -0.05913979]\n",
      "Gradient Descent(109/999): loss=10.895850523716845, w0=-0.6359054210127186, w1=0.865543795226522\n",
      "[ 0.58762379 -0.05903318]\n",
      "10.888855883988667\n",
      "[ 0.58762379 -0.05903318]\n",
      "Gradient Descent(110/999): loss=10.888855883988667, w0=-0.6417816588900213, w1=0.8661341270597828\n",
      "[ 0.58656454 -0.05892677]\n",
      "10.88188643854913\n",
      "[ 0.58656454 -0.05892677]\n",
      "Gradient Descent(111/999): loss=10.88188643854913, w0=-0.6476473042854812, w1=0.8667233947633476\n",
      "[ 0.5855072  -0.05882055]\n",
      "10.874942096649868\n",
      "[ 0.5855072  -0.05882055]\n",
      "Gradient Descent(112/999): loss=10.874942096649868, w0=-0.6535023762930622, w1=0.8673116002554123\n",
      "[ 0.58445177 -0.05871452]\n",
      "10.868022767869373\n",
      "[ 0.58445177 -0.05871452]\n",
      "Gradient Descent(113/999): loss=10.868022767869373, w0=-0.6593468939723088, w1=0.8678987454507151\n",
      "[ 0.58339824 -0.05860868]\n",
      "10.861128362111838\n",
      "[ 0.58339824 -0.05860868]\n",
      "Gradient Descent(114/999): loss=10.861128362111838, w0=-0.6651808763484093, w1=0.8684848322605423\n",
      "[ 0.58234661 -0.05850303]\n",
      "10.85425878960597\n",
      "[ 0.58234661 -0.05850303]\n",
      "Gradient Descent(115/999): loss=10.85425878960597, w0=-0.671004342412257, w1=0.8690698625927351\n",
      "[ 0.58129687 -0.05839758]\n",
      "10.847413960903836\n",
      "[ 0.58129687 -0.05839758]\n",
      "Gradient Descent(116/999): loss=10.847413960903836, w0=-0.6768173111205126, w1=0.8696538383516959\n",
      "[ 0.58024903 -0.05829231]\n",
      "10.840593786879673\n",
      "[ 0.58024903 -0.05829231]\n",
      "Gradient Descent(117/999): loss=10.840593786879673, w0=-0.6826198013956652, w1=0.8702367614383937\n",
      "[ 0.57920307 -0.05818723]\n",
      "10.833798178728763\n",
      "[ 0.57920307 -0.05818723]\n",
      "Gradient Descent(118/999): loss=10.833798178728763, w0=-0.6884118321260947, w1=0.8708186337503714\n",
      "[ 0.578159   -0.05808234]\n",
      "10.827027047966245\n",
      "[ 0.578159   -0.05808234]\n",
      "Gradient Descent(119/999): loss=10.827027047966245, w0=-0.6941934221661327, w1=0.8713994571817509\n",
      "[ 0.57711682 -0.05797764]\n",
      "10.820280306425975\n",
      "[ 0.57711682 -0.05797764]\n",
      "Gradient Descent(120/999): loss=10.820280306425975, w0=-0.6999645903361239, w1=0.8719792336232401\n",
      "[ 0.57607651 -0.05787313]\n",
      "10.813557866259385\n",
      "[ 0.57607651 -0.05787313]\n",
      "Gradient Descent(121/999): loss=10.813557866259385, w0=-0.7057253554224879, w1=0.8725579649621386\n",
      "[ 0.57503808 -0.05776881]\n",
      "10.80685963993433\n",
      "[ 0.57503808 -0.05776881]\n",
      "Gradient Descent(122/999): loss=10.80685963993433, w0=-0.7114757361777797, w1=0.873135653082344\n",
      "[ 0.57400151 -0.05766468]\n",
      "10.800185540233949\n",
      "[ 0.57400151 -0.05766468]\n",
      "Gradient Descent(123/999): loss=10.800185540233949, w0=-0.7172157513207511, w1=0.8737122998643578\n",
      "[ 0.57296682 -0.05756073]\n",
      "10.793535480255535\n",
      "[ 0.57296682 -0.05756073]\n",
      "Gradient Descent(124/999): loss=10.793535480255535, w0=-0.7229454195364116, w1=0.874287907185292\n",
      "[ 0.57193399 -0.05745697]\n",
      "10.786909373409394\n",
      "[ 0.57193399 -0.05745697]\n",
      "Gradient Descent(125/999): loss=10.786909373409394, w0=-0.728664759476089, w1=0.8748624769188748\n",
      "[ 0.57090303 -0.0573534 ]\n",
      "10.780307133417724\n",
      "[ 0.57090303 -0.0573534 ]\n",
      "Gradient Descent(126/999): loss=10.780307133417724, w0=-0.7343737897574906, w1=0.8754360109354568\n",
      "[ 0.56987392 -0.05725002]\n",
      "10.773728674313492\n",
      "[ 0.56987392 -0.05725002]\n",
      "Gradient Descent(127/999): loss=10.773728674313492, w0=-0.7400725289647632, w1=0.876008511102017\n",
      "[ 0.56884667 -0.05714682]\n",
      "10.767173910439322\n",
      "[ 0.56884667 -0.05714682]\n",
      "Gradient Descent(128/999): loss=10.767173910439322, w0=-0.745760995648554, w1=0.8765799792821695\n",
      "[ 0.56782127 -0.05704381]\n",
      "10.760642756446355\n",
      "[ 0.56782127 -0.05704381]\n",
      "Gradient Descent(129/999): loss=10.760642756446355, w0=-0.751439208326071, w1=0.8771504173361683\n",
      "[ 0.56679772 -0.05694098]\n",
      "10.75413512729316\n",
      "[ 0.56679772 -0.05694098]\n",
      "Gradient Descent(130/999): loss=10.75413512729316, w0=-0.7571071854811431, w1=0.8777198271209147\n",
      "[ 0.56577601 -0.05683834]\n",
      "10.747650938244634\n",
      "[ 0.56577601 -0.05683834]\n",
      "Gradient Descent(131/999): loss=10.747650938244634, w0=-0.7627649455642801, w1=0.8782882104899624\n",
      "[ 0.56475614 -0.05673588]\n",
      "10.74119010487086\n",
      "[ 0.56475614 -0.05673588]\n",
      "Gradient Descent(132/999): loss=10.74119010487086, w0=-0.7684125069927333, w1=0.8788555692935243\n",
      "[ 0.56373812 -0.05663361]\n",
      "10.734752543046048\n",
      "[ 0.56373812 -0.05663361]\n",
      "Gradient Descent(133/999): loss=10.734752543046048, w0=-0.7740498881505551, w1=0.8794219053784776\n",
      "[ 0.56272192 -0.05653152]\n",
      "10.728338168947424\n",
      "[ 0.56272192 -0.05653152]\n",
      "Gradient Descent(134/999): loss=10.728338168947424, w0=-0.7796771073886586, w1=0.8799872205883708\n",
      "[ 0.56170756 -0.05642962]\n",
      "10.721946899054135\n",
      "[ 0.56170756 -0.05642962]\n",
      "Gradient Descent(135/999): loss=10.721946899054135, w0=-0.785294183024878, w1=0.8805515167634289\n",
      "[ 0.56069503 -0.0563279 ]\n",
      "10.715578650146169\n",
      "[ 0.56069503 -0.0563279 ]\n",
      "Gradient Descent(136/999): loss=10.715578650146169, w0=-0.7909011333440276, w1=0.8811147957405598\n",
      "[ 0.55968433 -0.05622636]\n",
      "10.709233339303264\n",
      "[ 0.55968433 -0.05622636]\n",
      "Gradient Descent(137/999): loss=10.709233339303264, w0=-0.7964979765979616, w1=0.8816770593533604\n",
      "[ 0.55867544 -0.05612501]\n",
      "10.702910883903833\n",
      "[ 0.55867544 -0.05612501]\n",
      "Gradient Descent(138/999): loss=10.702910883903833, w0=-0.8020847310056336, w1=0.882238309432122\n",
      "[ 0.55766837 -0.05602384]\n",
      "10.696611201623886\n",
      "[ 0.55766837 -0.05602384]\n",
      "Gradient Descent(139/999): loss=10.696611201623886, w0=-0.8076614147531557, w1=0.8827985478038369\n",
      "[ 0.55666312 -0.05592285]\n",
      "10.69033421043596\n",
      "[ 0.55666312 -0.05592285]\n",
      "Gradient Descent(140/999): loss=10.69033421043596, w0=-0.8132280459938577, w1=0.8833577762922041\n",
      "[ 0.55565969 -0.05582204]\n",
      "10.684079828608057\n",
      "[ 0.55565969 -0.05582204]\n",
      "Gradient Descent(141/999): loss=10.684079828608057, w0=-0.8187846428483464, w1=0.883915996717635\n",
      "[ 0.55465806 -0.05572142]\n",
      "10.677847974702567\n",
      "[ 0.55465806 -0.05572142]\n",
      "Gradient Descent(142/999): loss=10.677847974702567, w0=-0.8243312234045646, w1=0.8844732108972596\n",
      "[ 0.55365823 -0.05562097]\n",
      "10.671638567575206\n",
      "[ 0.55365823 -0.05562097]\n",
      "Gradient Descent(143/999): loss=10.671638567575206, w0=-0.8298678057178496, w1=0.8850294206449325\n",
      "[ 0.55266021 -0.05552071]\n",
      "10.665451526373976\n",
      "[ 0.55266021 -0.05552071]\n",
      "Gradient Descent(144/999): loss=10.665451526373976, w0=-0.8353944078109924, w1=0.8855846277712385\n",
      "[ 0.55166399 -0.05542063]\n",
      "10.659286770538106\n",
      "[ 0.55166399 -0.05542063]\n",
      "Gradient Descent(145/999): loss=10.659286770538106, w0=-0.840911047674296, w1=0.8861388340834985\n",
      "[ 0.55066956 -0.05532073]\n",
      "10.653144219796998\n",
      "[ 0.55066956 -0.05532073]\n",
      "Gradient Descent(146/999): loss=10.653144219796998, w0=-0.8464177432656345, w1=0.8866920413857761\n",
      "[ 0.54967692 -0.05522101]\n",
      "10.647023794169174\n",
      "[ 0.54967692 -0.05522101]\n",
      "Gradient Descent(147/999): loss=10.647023794169174, w0=-0.8519145125105108, w1=0.8872442514788821\n",
      "[ 0.54868608 -0.05512147]\n",
      "10.64092541396126\n",
      "[ 0.54868608 -0.05512147]\n",
      "Gradient Descent(148/999): loss=10.64092541396126, w0=-0.8574013733021155, w1=0.8877954661603819\n",
      "[ 0.54769702 -0.05502211]\n",
      "10.634848999766922\n",
      "[ 0.54769702 -0.05502211]\n",
      "Gradient Descent(149/999): loss=10.634848999766922, w0=-0.8628783435013853, w1=0.8883456872246002\n",
      "[ 0.54670974 -0.05492292]\n",
      "10.628794472465849\n",
      "[ 0.54670974 -0.05492292]\n",
      "Gradient Descent(150/999): loss=10.628794472465849, w0=-0.8683454409370605, w1=0.8888949164626272\n",
      "[ 0.54572425 -0.05482392]\n",
      "10.622761753222708\n",
      "[ 0.54572425 -0.05482392]\n",
      "Gradient Descent(151/999): loss=10.622761753222708, w0=-0.8738026834057434, w1=0.8894431556623249\n",
      "[ 0.54474053 -0.05472509]\n",
      "10.616750763486145\n",
      "[ 0.54474053 -0.05472509]\n",
      "Gradient Descent(152/999): loss=10.616750763486145, w0=-0.8792500886719564, w1=0.8899904066083322\n",
      "[ 0.54375858 -0.05462645]\n",
      "10.610761424987722\n",
      "[ 0.54375858 -0.05462645]\n",
      "Gradient Descent(153/999): loss=10.610761424987722, w0=-0.8846876744681996, w1=0.8905366710820709\n",
      "[ 0.5427784  -0.05452798]\n",
      "10.60479365974093\n",
      "[ 0.5427784  -0.05452798]\n",
      "Gradient Descent(154/999): loss=10.60479365974093, w0=-0.8901154584950085, w1=0.891081950861752\n",
      "[ 0.54179999 -0.05442969]\n",
      "10.598847390040163\n",
      "[ 0.54179999 -0.05442969]\n",
      "Gradient Descent(155/999): loss=10.598847390040163, w0=-0.8955334584210117, w1=0.8916262477223807\n",
      "[ 0.54082335 -0.05433157]\n",
      "10.592922538459703\n",
      "[ 0.54082335 -0.05433157]\n",
      "Gradient Descent(156/999): loss=10.592922538459703, w0=-0.9009416918829886, w1=0.892169563435763\n",
      "[ 0.53984846 -0.05423363]\n",
      "10.58701902785272\n",
      "[ 0.53984846 -0.05423363]\n",
      "Gradient Descent(157/999): loss=10.58701902785272, w0=-0.9063401764859261, w1=0.8927118997705107\n",
      "[ 0.53887533 -0.05413587]\n",
      "10.581136781350258\n",
      "[ 0.53887533 -0.05413587]\n",
      "Gradient Descent(158/999): loss=10.581136781350258, w0=-0.9117289298030771, w1=0.8932532584920477\n",
      "[ 0.53790396 -0.05403829]\n",
      "10.575275722360235\n",
      "[ 0.53790396 -0.05403829]\n",
      "Gradient Descent(159/999): loss=10.575275722360235, w0=-0.9171079693760165, w1=0.8937936413626153\n",
      "[ 0.53693433 -0.05394088]\n",
      "10.569435774566463\n",
      "[ 0.53693433 -0.05394088]\n",
      "Gradient Descent(160/999): loss=10.569435774566463, w0=-0.922477312714699, w1=0.8943330501412785\n",
      "[ 0.53596646 -0.05384364]\n",
      "10.563616861927622\n",
      "[ 0.53596646 -0.05384364]\n",
      "Gradient Descent(161/999): loss=10.563616861927622, w0=-0.9278369772975161, w1=0.8948714865839313\n",
      "[ 0.53500033 -0.05374659]\n",
      "10.557818908676307\n",
      "[ 0.53500033 -0.05374659]\n",
      "Gradient Descent(162/999): loss=10.557818908676307, w0=-0.9331869805713526, w1=0.8954089524433027\n",
      "[ 0.53403594 -0.0536497 ]\n",
      "10.552041839318\n",
      "[ 0.53403594 -0.0536497 ]\n",
      "Gradient Descent(163/999): loss=10.552041839318, w0=-0.9385273399516438, w1=0.8959454494689618\n",
      "[ 0.53307329 -0.05355299]\n",
      "10.546285578630139\n",
      "[ 0.53307329 -0.05355299]\n",
      "Gradient Descent(164/999): loss=10.546285578630139, w0=-0.9438580728224318, w1=0.8964809794073243\n",
      "[ 0.53211237 -0.05345646]\n",
      "10.540550051661087\n",
      "[ 0.53211237 -0.05345646]\n",
      "Gradient Descent(165/999): loss=10.540550051661087, w0=-0.9491791965364225, w1=0.8970155440016577\n",
      "[ 0.53115319 -0.0533601 ]\n",
      "10.534835183729184\n",
      "[ 0.53115319 -0.0533601 ]\n",
      "Gradient Descent(166/999): loss=10.534835183729184, w0=-0.9544907284150416, w1=0.8975491449920872\n",
      "[ 0.53019573 -0.05326391]\n",
      "10.529140900421773\n",
      "[ 0.53019573 -0.05326391]\n",
      "Gradient Descent(167/999): loss=10.529140900421773, w0=-0.9597926857484915, w1=0.8980817841156011\n",
      "[ 0.52924   -0.0531679]\n",
      "10.523467127594222\n",
      "[ 0.52924   -0.0531679]\n",
      "Gradient Descent(168/999): loss=10.523467127594222, w0=-0.9650850857958075, w1=0.8986134631060566\n",
      "[ 0.528286   -0.05307206]\n",
      "10.517813791368964\n",
      "[ 0.528286   -0.05307206]\n",
      "Gradient Descent(169/999): loss=10.517813791368964, w0=-0.9703679457849136, w1=0.8991441836941857\n",
      "[ 0.52733371 -0.05297639]\n",
      "10.512180818134548\n",
      "[ 0.52733371 -0.05297639]\n",
      "Gradient Descent(170/999): loss=10.512180818134548, w0=-0.9756412829126787, w1=0.8996739476076003\n",
      "[ 0.52638314 -0.0528809 ]\n",
      "10.506568134544644\n",
      "[ 0.52638314 -0.0528809 ]\n",
      "Gradient Descent(171/999): loss=10.506568134544644, w0=-0.980905114344973, w1=0.9002027565707984\n",
      "[ 0.52543429 -0.05278557]\n",
      "10.50097566751713\n",
      "[ 0.52543429 -0.05278557]\n",
      "Gradient Descent(172/999): loss=10.50097566751713, w0=-0.9861594572167234, w1=0.9007306123051692\n",
      "[ 0.52448714 -0.05269042]\n",
      "10.495403344233111\n",
      "[ 0.52448714 -0.05269042]\n",
      "Gradient Descent(173/999): loss=10.495403344233111, w0=-0.9914043286319694, w1=0.9012575165289988\n",
      "[ 0.5235417  -0.05259544]\n",
      "10.48985109213599\n",
      "[ 0.5235417  -0.05259544]\n",
      "Gradient Descent(174/999): loss=10.48985109213599, w0=-0.996639745663919, w1=0.9017834709574764\n",
      "[ 0.52259797 -0.05250063]\n",
      "10.484318838930506\n",
      "[ 0.52259797 -0.05250063]\n",
      "Gradient Descent(175/999): loss=10.484318838930506, w0=-1.001865725355004, w1=0.902308477302699\n",
      "[ 0.52165594 -0.052406  ]\n",
      "10.478806512581805\n",
      "[ 0.52165594 -0.052406  ]\n",
      "Gradient Descent(176/999): loss=10.478806512581805, w0=-1.0070822847169358, w1=0.9028325372736776\n",
      "[ 0.5207156  -0.05231153]\n",
      "10.473314041314504\n",
      "[ 0.5207156  -0.05231153]\n",
      "Gradient Descent(177/999): loss=10.473314041314504, w0=-1.01228944073076, w1=0.9033556525763423\n",
      "[ 0.51977696 -0.05221723]\n",
      "10.467841353611739\n",
      "[ 0.51977696 -0.05221723]\n",
      "Gradient Descent(178/999): loss=10.467841353611739, w0=-1.017487210346913, w1=0.9038778249135484\n",
      "[ 0.51884001 -0.05212311]\n",
      "10.462388378214257\n",
      "[ 0.51884001 -0.05212311]\n",
      "Gradient Descent(179/999): loss=10.462388378214257, w0=-1.0226756104852757, w1=0.9043990559850815\n",
      "[ 0.51790475 -0.05202915]\n",
      "10.456955044119471\n",
      "[ 0.51790475 -0.05202915]\n",
      "Gradient Descent(180/999): loss=10.456955044119471, w0=-1.0278546580352297, w1=0.9049193474876631\n",
      "[ 0.51697118 -0.05193536]\n",
      "10.451541280580543\n",
      "[ 0.51697118 -0.05193536]\n",
      "Gradient Descent(181/999): loss=10.451541280580543, w0=-1.0330243698557118, w1=0.9054387011149563\n",
      "[ 0.51603929 -0.05184174]\n",
      "10.446147017105456\n",
      "[ 0.51603929 -0.05184174]\n",
      "Gradient Descent(182/999): loss=10.446147017105456, w0=-1.038184762775269, w1=0.9059571185575711\n",
      "[ 0.51510908 -0.05174829]\n",
      "10.44077218345611\n",
      "[ 0.51510908 -0.05174829]\n",
      "Gradient Descent(183/999): loss=10.44077218345611, w0=-1.0433358535921131, w1=0.9064746015030702\n",
      "[ 0.51418055 -0.05165501]\n",
      "10.435416709647392\n",
      "[ 0.51418055 -0.05165501]\n",
      "Gradient Descent(184/999): loss=10.435416709647392, w0=-1.0484776590741756, w1=0.9069911516359741\n",
      "[ 0.51325369 -0.0515619 ]\n",
      "10.430080525946273\n",
      "[ 0.51325369 -0.0515619 ]\n",
      "Gradient Descent(185/999): loss=10.430080525946273, w0=-1.0536101959591622, w1=0.907506770637767\n",
      "[ 0.5123285  -0.05146895]\n",
      "10.424763562870899\n",
      "[ 0.5123285  -0.05146895]\n",
      "Gradient Descent(186/999): loss=10.424763562870899, w0=-1.0587334809546072, w1=0.9080214601869019\n",
      "[ 0.51140498 -0.05137618]\n",
      "10.419465751189687\n",
      "[ 0.51140498 -0.05137618]\n",
      "Gradient Descent(187/999): loss=10.419465751189687, w0=-1.063847530737928, w1=0.9085352219588062\n",
      "[ 0.51048312 -0.05128357]\n",
      "10.414187021920416\n",
      "[ 0.51048312 -0.05128357]\n",
      "Gradient Descent(188/999): loss=10.414187021920416, w0=-1.0689523619564796, w1=0.9090480576258873\n",
      "[ 0.50956293 -0.05119112]\n",
      "10.408927306329344\n",
      "[ 0.50956293 -0.05119112]\n",
      "Gradient Descent(189/999): loss=10.408927306329344, w0=-1.074047991227608, w1=0.9095599688575379\n",
      "[ 0.50864439 -0.05109885]\n",
      "10.403686535930298\n",
      "[ 0.50864439 -0.05109885]\n",
      "Gradient Descent(190/999): loss=10.403686535930298, w0=-1.0791344351387053, w1=0.9100709573201414\n",
      "[ 0.50772751 -0.05100674]\n",
      "10.398464642483791\n",
      "[ 0.50772751 -0.05100674]\n",
      "Gradient Descent(191/999): loss=10.398464642483791, w0=-1.0842117102472633, w1=0.9105810246770776\n",
      "[ 0.50681228 -0.05091479]\n",
      "10.393261557996123\n",
      "[ 0.50681228 -0.05091479]\n",
      "Gradient Descent(192/999): loss=10.393261557996123, w0=-1.0892798330809268, w1=0.9110901725887276\n",
      "[ 0.50589871 -0.05082301]\n",
      "10.388077214718518\n",
      "[ 0.50589871 -0.05082301]\n",
      "Gradient Descent(193/999): loss=10.388077214718518, w0=-1.0943388201375486, w1=0.9115984027124796\n",
      "[ 0.50498677 -0.0507314 ]\n",
      "10.382911545146214\n",
      "[ 0.50498677 -0.0507314 ]\n",
      "Gradient Descent(194/999): loss=10.382911545146214, w0=-1.0993886878852421, w1=0.9121057167027342\n",
      "[ 0.50407649 -0.05063995]\n",
      "10.377764482017604\n",
      "[ 0.50407649 -0.05063995]\n",
      "Gradient Descent(195/999): loss=10.377764482017604, w0=-1.1044294527624354, w1=0.91261211621091\n",
      "[ 0.50316784 -0.05054867]\n",
      "10.372635958313353\n",
      "[ 0.50316784 -0.05054867]\n",
      "Gradient Descent(196/999): loss=10.372635958313353, w0=-1.109461131177925, w1=0.9131176028854483\n",
      "[ 0.50226083 -0.05045755]\n",
      "10.36752590725552\n",
      "[ 0.50226083 -0.05045755]\n",
      "Gradient Descent(197/999): loss=10.36752590725552, w0=-1.1144837395109286, w1=0.9136221783718195\n",
      "[ 0.50135546 -0.05036659]\n",
      "10.362434262306698\n",
      "[ 0.50135546 -0.05036659]\n",
      "Gradient Descent(198/999): loss=10.362434262306698, w0=-1.1194972941111392, w1=0.9141258443125273\n",
      "[ 0.50045172 -0.0502758 ]\n",
      "10.357360957169151\n",
      "[ 0.50045172 -0.0502758 ]\n",
      "Gradient Descent(199/999): loss=10.357360957169151, w0=-1.1245018112987775, w1=0.9146286023471152\n",
      "[ 0.49954961 -0.05018518]\n",
      "10.352305925783932\n",
      "[ 0.49954961 -0.05018518]\n",
      "Gradient Descent(200/999): loss=10.352305925783932, w0=-1.1294973073646457, w1=0.9151304541121706\n",
      "[ 0.49864912 -0.05009471]\n",
      "10.347269102330044\n",
      "[ 0.49864912 -0.05009471]\n",
      "Gradient Descent(201/999): loss=10.347269102330044, w0=-1.1344837985701803, w1=0.9156314012413316\n",
      "[ 0.49775026 -0.05000441]\n",
      "10.342250421223564\n",
      "[ 0.49775026 -0.05000441]\n",
      "Gradient Descent(202/999): loss=10.342250421223564, w0=-1.1394613011475048, w1=0.9161314453652908\n",
      "[ 0.49685302 -0.04991427]\n",
      "10.337249817116808\n",
      "[ 0.49685302 -0.04991427]\n",
      "Gradient Descent(203/999): loss=10.337249817116808, w0=-1.1444298312994827, w1=0.9166305881118018\n",
      "[ 0.49595739 -0.0498243 ]\n",
      "10.332267224897462\n",
      "[ 0.49595739 -0.0498243 ]\n",
      "Gradient Descent(204/999): loss=10.332267224897462, w0=-1.1493894051997708, w1=0.9171288311056836\n",
      "[ 0.49506338 -0.04973449]\n",
      "10.32730257968775\n",
      "[ 0.49506338 -0.04973449]\n",
      "Gradient Descent(205/999): loss=10.32730257968775, w0=-1.1543400389928709, w1=0.9176261759688266\n",
      "[ 0.49417098 -0.04964484]\n",
      "10.322355816843578\n",
      "[ 0.49417098 -0.04964484]\n",
      "Gradient Descent(206/999): loss=10.322355816843578, w0=-1.1592817487941824, w1=0.9181226243201975\n",
      "[ 0.49328019 -0.04955535]\n",
      "10.3174268719537\n",
      "[ 0.49328019 -0.04955535]\n",
      "Gradient Descent(207/999): loss=10.3174268719537, w0=-1.164214550690056, w1=0.9186181777758449\n",
      "[ 0.492391   -0.04946602]\n",
      "10.312515680838867\n",
      "[ 0.492391   -0.04946602]\n",
      "Gradient Descent(208/999): loss=10.312515680838867, w0=-1.169138460737845, w1=0.9191128379489037\n",
      "[ 0.49150342 -0.04937685]\n",
      "10.30762217955101\n",
      "[ 0.49150342 -0.04937685]\n",
      "Gradient Descent(209/999): loss=10.30762217955101, w0=-1.1740534949659573, w1=0.9196066064496018\n",
      "[ 0.49061744 -0.04928784]\n",
      "10.302746304372395\n",
      "[ 0.49061744 -0.04928784]\n",
      "Gradient Descent(210/999): loss=10.302746304372395, w0=-1.1789596693739084, w1=0.9200994848852637\n",
      "[ 0.48973306 -0.049199  ]\n",
      "10.29788799181479\n",
      "[ 0.48973306 -0.049199  ]\n",
      "Gradient Descent(211/999): loss=10.29788799181479, w0=-1.1838569999323731, w1=0.9205914748603171\n",
      "[ 0.48885027 -0.04911031]\n",
      "10.293047178618645\n",
      "[ 0.48885027 -0.04911031]\n",
      "Gradient Descent(212/999): loss=10.293047178618645, w0=-1.1887455025832376, w1=0.9210825779762974\n",
      "[ 0.48796907 -0.04902179]\n",
      "10.288223801752277\n",
      "[ 0.48796907 -0.04902179]\n",
      "Gradient Descent(213/999): loss=10.288223801752277, w0=-1.1936251932396513, w1=0.921572795831853\n",
      "[ 0.48708945 -0.04893342]\n",
      "10.28341779841103\n",
      "[ 0.48708945 -0.04893342]\n",
      "Gradient Descent(214/999): loss=10.28341779841103, w0=-1.1984960877860784, w1=0.9220621300227506\n",
      "[ 0.48621143 -0.04884521]\n",
      "10.278629106016467\n",
      "[ 0.48621143 -0.04884521]\n",
      "Gradient Descent(215/999): loss=10.278629106016467, w0=-1.2033582020783502, w1=0.9225505821418806\n",
      "[ 0.48533499 -0.04875716]\n",
      "10.273857662215557\n",
      "[ 0.48533499 -0.04875716]\n",
      "Gradient Descent(216/999): loss=10.273857662215557, w0=-1.208211551943716, w1=0.9230381537792616\n",
      "[ 0.48446012 -0.04866927]\n",
      "10.269103404879864\n",
      "[ 0.48446012 -0.04866927]\n",
      "Gradient Descent(217/999): loss=10.269103404879864, w0=-1.213056153180895, w1=0.9235248465220465\n",
      "[ 0.48358684 -0.04858154]\n",
      "10.264366272104729\n",
      "[ 0.48358684 -0.04858154]\n",
      "Gradient Descent(218/999): loss=10.264366272104729, w0=-1.217892021560128, w1=0.924010661954527\n",
      "[ 0.48271513 -0.04849397]\n",
      "10.259646202208474\n",
      "[ 0.48271513 -0.04849397]\n",
      "Gradient Descent(219/999): loss=10.259646202208474, w0=-1.2227191728232283, w1=0.9244956016581389\n",
      "[ 0.48184499 -0.04840656]\n",
      "10.254943133731597\n",
      "[ 0.48184499 -0.04840656]\n",
      "Gradient Descent(220/999): loss=10.254943133731597, w0=-1.227537622683633, w1=0.9249796672114676\n",
      "[ 0.48097641 -0.0483193 ]\n",
      "10.25025700543596\n",
      "[ 0.48097641 -0.0483193 ]\n",
      "Gradient Descent(221/999): loss=10.25025700543596, w0=-1.232347386826454, w1=0.9254628601902526\n",
      "[ 0.48010941 -0.0482322 ]\n",
      "10.245587756304014\n",
      "[ 0.48010941 -0.0482322 ]\n",
      "Gradient Descent(222/999): loss=10.245587756304014, w0=-1.2371484809085298, w1=0.9259451821673932\n",
      "[ 0.47924396 -0.04814525]\n",
      "10.240935325537984\n",
      "[ 0.47924396 -0.04814525]\n",
      "Gradient Descent(223/999): loss=10.240935325537984, w0=-1.2419409205584755, w1=0.9264266347129532\n",
      "[ 0.47838008 -0.04805847]\n",
      "10.236299652559083\n",
      "[ 0.47838008 -0.04805847]\n",
      "Gradient Descent(224/999): loss=10.236299652559083, w0=-1.2467247213767343, w1=0.9269072193941664\n",
      "[ 0.47751776 -0.04797184]\n",
      "10.231680677006734\n",
      "[ 0.47751776 -0.04797184]\n",
      "Gradient Descent(225/999): loss=10.231680677006734, w0=-1.2514998989356283, w1=0.9273869377754416\n",
      "[ 0.47665698 -0.04788536]\n",
      "10.227078338737767\n",
      "[ 0.47665698 -0.04788536]\n",
      "Gradient Descent(226/999): loss=10.227078338737767, w0=-1.2562664687794085, w1=0.9278657914183672\n",
      "[ 0.47579776 -0.04779905]\n",
      "10.222492577825648\n",
      "[ 0.47579776 -0.04779905]\n",
      "Gradient Descent(227/999): loss=10.222492577825648, w0=-1.2610244464243063, w1=0.9283437818817171\n",
      "[ 0.47494009 -0.04771288]\n",
      "10.217923334559694\n",
      "[ 0.47494009 -0.04771288]\n",
      "Gradient Descent(228/999): loss=10.217923334559694, w0=-1.2657738473585838, w1=0.928820910721455\n",
      "[ 0.47408397 -0.04762688]\n",
      "10.2133705494443\n",
      "[ 0.47408397 -0.04762688]\n",
      "Gradient Descent(229/999): loss=10.2133705494443, w0=-1.2705146870425834, w1=0.9292971794907403\n",
      "[ 0.47322939 -0.04754102]\n",
      "10.208834163198153\n",
      "[ 0.47322939 -0.04754102]\n",
      "Gradient Descent(230/999): loss=10.208834163198153, w0=-1.2752469809087792, w1=0.9297725897399323\n",
      "[ 0.47237635 -0.04745533]\n",
      "10.204314116753473\n",
      "[ 0.47237635 -0.04745533]\n",
      "Gradient Descent(231/999): loss=10.204314116753473, w0=-1.2799707443618265, w1=0.9302471430165956\n",
      "[ 0.47152484 -0.04736978]\n",
      "10.199810351255238\n",
      "[ 0.47152484 -0.04736978]\n",
      "Gradient Descent(232/999): loss=10.199810351255238, w0=-1.2846859927786125, w1=0.9307208408655054\n",
      "[ 0.47067487 -0.0472844 ]\n",
      "10.195322808060421\n",
      "[ 0.47067487 -0.0472844 ]\n",
      "Gradient Descent(233/999): loss=10.195322808060421, w0=-1.289392741508306, w1=0.9311936848286523\n",
      "[ 0.46982644 -0.04719916]\n",
      "10.190851428737218\n",
      "[ 0.46982644 -0.04719916]\n",
      "Gradient Descent(234/999): loss=10.190851428737218, w0=-1.2940910058724073, w1=0.931665676445247\n",
      "[ 0.46897953 -0.04711408]\n",
      "10.186396155064301\n",
      "[ 0.46897953 -0.04711408]\n",
      "Gradient Descent(235/999): loss=10.186396155064301, w0=-1.2987808011647985, w1=0.932136817251726\n",
      "[ 0.46813415 -0.04702915]\n",
      "10.181956929030042\n",
      "[ 0.46813415 -0.04702915]\n",
      "Gradient Descent(236/999): loss=10.181956929030042, w0=-1.303462142651793, w1=0.932607108781756\n",
      "[ 0.46729029 -0.04694438]\n",
      "10.177533692831775\n",
      "[ 0.46729029 -0.04694438]\n",
      "Gradient Descent(237/999): loss=10.177533692831775, w0=-1.3081350455721847, w1=0.9330765525662391\n",
      "[ 0.46644796 -0.04685976]\n",
      "10.173126388875033\n",
      "[ 0.46644796 -0.04685976]\n",
      "Gradient Descent(238/999): loss=10.173126388875033, w0=-1.312799525137299, w1=0.9335451501333181\n",
      "[ 0.46560714 -0.04677529]\n",
      "10.168734959772802\n",
      "[ 0.46560714 -0.04677529]\n",
      "Gradient Descent(239/999): loss=10.168734959772802, w0=-1.3174555965310406, w1=0.9340129030083807\n",
      "[ 0.46476784 -0.04669097]\n",
      "10.164359348344773\n",
      "[ 0.46476784 -0.04669097]\n",
      "Gradient Descent(240/999): loss=10.164359348344773, w0=-1.3221032749099442, w1=0.9344798127140652\n",
      "[ 0.46393005 -0.04660681]\n",
      "10.159999497616592\n",
      "[ 0.46393005 -0.04660681]\n",
      "Gradient Descent(241/999): loss=10.159999497616592, w0=-1.3267425754032232, w1=0.9349458807702655\n",
      "[ 0.46309377 -0.04652279]\n",
      "10.155655350819135\n",
      "[ 0.46309377 -0.04652279]\n",
      "Gradient Descent(242/999): loss=10.155655350819135, w0=-1.331373513112819, w1=0.9354111086941351\n",
      "[ 0.462259   -0.04643893]\n",
      "10.151326851387743\n",
      "[ 0.462259   -0.04643893]\n",
      "Gradient Descent(243/999): loss=10.151326851387743, w0=-1.335996103113451, w1=0.9358754980000933\n",
      "[ 0.46142573 -0.04635522]\n",
      "10.147013942961513\n",
      "[ 0.46142573 -0.04635522]\n",
      "Gradient Descent(244/999): loss=10.147013942961513, w0=-1.3406103604526642, w1=0.9363390501998291\n",
      "[ 0.46059397 -0.04627166]\n",
      "10.142716569382536\n",
      "[ 0.46059397 -0.04627166]\n",
      "Gradient Descent(245/999): loss=10.142716569382536, w0=-1.3452163001508792, w1=0.9368017668023068\n",
      "[ 0.45976371 -0.04618825]\n",
      "10.138434674695192\n",
      "[ 0.45976371 -0.04618825]\n",
      "Gradient Descent(246/999): loss=10.138434674695192, w0=-1.349813937201441, w1=0.9372636493137702\n",
      "[ 0.45893494 -0.04610499]\n",
      "10.13416820314541\n",
      "[ 0.45893494 -0.04610499]\n",
      "Gradient Descent(247/999): loss=10.13416820314541, w0=-1.3544032865706677, w1=0.9377246992377485\n",
      "[ 0.45810766 -0.04602188]\n",
      "10.129917099179938\n",
      "[ 0.45810766 -0.04602188]\n",
      "Gradient Descent(248/999): loss=10.129917099179938, w0=-1.3589843631978988, w1=0.9381849180750605\n",
      "[ 0.45728188 -0.04593892]\n",
      "10.12568130744562\n",
      "[ 0.45728188 -0.04593892]\n",
      "Gradient Descent(249/999): loss=10.12568130744562, w0=-1.3635571819955448, w1=0.9386443073238195\n",
      "[ 0.45645759 -0.04585612]\n",
      "10.121460772788684\n",
      "[ 0.45645759 -0.04585612]\n",
      "Gradient Descent(250/999): loss=10.121460772788684, w0=-1.3681217578491345, w1=0.9391028684794384\n",
      "[ 0.45563478 -0.04577346]\n",
      "10.11725544025402\n",
      "[ 0.45563478 -0.04577346]\n",
      "Gradient Descent(251/999): loss=10.11725544025402, w0=-1.3726781056173645, w1=0.9395606030346343\n",
      "[ 0.45481345 -0.04569094]\n",
      "10.113065255084463\n",
      "[ 0.45481345 -0.04569094]\n",
      "Gradient Descent(252/999): loss=10.113065255084463, w0=-1.377226240132147, w1=0.9400175124794337\n",
      "[ 0.45399361 -0.04560858]\n",
      "10.108890162720071\n",
      "[ 0.45399361 -0.04560858]\n",
      "Gradient Descent(253/999): loss=10.108890162720071, w0=-1.3817661761986584, w1=0.9404735983011772\n",
      "[ 0.45317524 -0.04552637]\n",
      "10.104730108797437\n",
      "[ 0.45317524 -0.04552637]\n",
      "Gradient Descent(254/999): loss=10.104730108797437, w0=-1.3862979285953874, w1=0.9409288619845243\n",
      "[ 0.45235835 -0.0454443 ]\n",
      "10.100585039148958\n",
      "[ 0.45235835 -0.0454443 ]\n",
      "Gradient Descent(255/999): loss=10.100585039148958, w0=-1.3908215120741827, w1=0.9413833050114583\n",
      "[ 0.45154293 -0.04536238]\n",
      "10.096454899802147\n",
      "[ 0.45154293 -0.04536238]\n",
      "Gradient Descent(256/999): loss=10.096454899802147, w0=-1.3953369413603018, w1=0.9418369288612908\n",
      "[ 0.45072898 -0.04528061]\n",
      "10.092339636978913\n",
      "[ 0.45072898 -0.04528061]\n",
      "Gradient Descent(257/999): loss=10.092339636978913, w0=-1.3998442311524586, w1=0.9422897350106672\n",
      "[ 0.4499165  -0.04519899]\n",
      "10.088239197094882\n",
      "[ 0.4499165  -0.04519899]\n",
      "Gradient Descent(258/999): loss=10.088239197094882, w0=-1.4043433961228704, w1=0.9427417249335708\n",
      "[ 0.44910548 -0.04511752]\n",
      "10.08415352675868\n",
      "[ 0.44910548 -0.04511752]\n",
      "Gradient Descent(259/999): loss=10.08415352675868, w0=-1.4088344509173072, w1=0.9431929001013282\n",
      "[ 0.44829592 -0.04503619]\n",
      "10.080082572771252\n",
      "[ 0.44829592 -0.04503619]\n",
      "Gradient Descent(260/999): loss=10.080082572771252, w0=-1.4133174101551385, w1=0.9436432619826135\n",
      "[ 0.44748783 -0.04495501]\n",
      "10.076026282125152\n",
      "[ 0.44748783 -0.04495501]\n",
      "Gradient Descent(261/999): loss=10.076026282125152, w0=-1.4177922884293805, w1=0.9440928120434534\n",
      "[ 0.44668119 -0.04487397]\n",
      "10.071984602003878\n",
      "[ 0.44668119 -0.04487397]\n",
      "Gradient Descent(262/999): loss=10.071984602003878, w0=-1.4222591003067446, w1=0.9445415517472323\n",
      "[ 0.445876   -0.04479308]\n",
      "10.067957479781157\n",
      "[ 0.445876   -0.04479308]\n",
      "Gradient Descent(263/999): loss=10.067957479781157, w0=-1.426717860327684, w1=0.9449894825546963\n",
      "[ 0.44507227 -0.04471234]\n",
      "10.06394486302028\n",
      "[ 0.44507227 -0.04471234]\n",
      "Gradient Descent(264/999): loss=10.06394486302028, w0=-1.4311685830064413, w1=0.9454366059239585\n",
      "[ 0.44426998 -0.04463174]\n",
      "10.059946699473414\n",
      "[ 0.44426998 -0.04463174]\n",
      "Gradient Descent(265/999): loss=10.059946699473414, w0=-1.435611282831096, w1=0.9458829233105037\n",
      "[ 0.44346914 -0.04455129]\n",
      "10.05596293708091\n",
      "[ 0.44346914 -0.04455129]\n",
      "Gradient Descent(266/999): loss=10.05596293708091, w0=-1.4400459742636116, w1=0.9463284361671929\n",
      "[ 0.44266975 -0.04447098]\n",
      "10.051993523970648\n",
      "[ 0.44266975 -0.04447098]\n",
      "Gradient Descent(267/999): loss=10.051993523970648, w0=-1.4444726717398821, w1=0.9467731459442682\n",
      "[ 0.44187179 -0.04439081]\n",
      "10.048038408457334\n",
      "[ 0.44187179 -0.04439081]\n",
      "Gradient Descent(268/999): loss=10.048038408457334, w0=-1.4488913896697797, w1=0.9472170540893577\n",
      "[ 0.44107528 -0.0443108 ]\n",
      "10.044097539041854\n",
      "[ 0.44107528 -0.0443108 ]\n",
      "Gradient Descent(269/999): loss=10.044097539041854, w0=-1.4533021424372015, w1=0.9476601620474799\n",
      "[ 0.4402802  -0.04423092]\n",
      "10.040170864410586\n",
      "[ 0.4402802  -0.04423092]\n",
      "Gradient Descent(270/999): loss=10.040170864410586, w0=-1.4577049444001158, w1=0.9481024712610484\n",
      "[ 0.43948655 -0.04415119]\n",
      "10.036258333434734\n",
      "[ 0.43948655 -0.04415119]\n",
      "Gradient Descent(271/999): loss=10.036258333434734, w0=-1.4620998098906097, w1=0.9485439831698768\n",
      "[ 0.43869433 -0.0440716 ]\n",
      "10.032359895169668\n",
      "[ 0.43869433 -0.0440716 ]\n",
      "Gradient Descent(272/999): loss=10.032359895169668, w0=-1.4664867532149353, w1=0.9489846992111832\n",
      "[ 0.43790354 -0.04399216]\n",
      "10.028475498854258\n",
      "[ 0.43790354 -0.04399216]\n",
      "Gradient Descent(273/999): loss=10.028475498854258, w0=-1.470865788653556, w1=0.9494246208195952\n",
      "[ 0.43711418 -0.04391286]\n",
      "10.024605093910212\n",
      "[ 0.43711418 -0.04391286]\n",
      "Gradient Descent(274/999): loss=10.024605093910212, w0=-1.4752369304611939, w1=0.9498637494271542\n",
      "[ 0.43632624 -0.0438337 ]\n",
      "10.020748629941417\n",
      "[ 0.43632624 -0.0438337 ]\n",
      "Gradient Descent(275/999): loss=10.020748629941417, w0=-1.479600192866875, w1=0.9503020864633203\n",
      "[ 0.43553972 -0.04375469]\n",
      "10.016906056733285\n",
      "[ 0.43553972 -0.04375469]\n",
      "Gradient Descent(276/999): loss=10.016906056733285, w0=-1.4839555900739763, w1=0.9507396333549765\n",
      "[ 0.43475462 -0.04367582]\n",
      "10.013077324252093\n",
      "[ 0.43475462 -0.04367582]\n",
      "Gradient Descent(277/999): loss=10.013077324252093, w0=-1.4883031362602719, w1=0.9511763915264342\n",
      "[ 0.43397093 -0.04359709]\n",
      "10.00926238264435\n",
      "[ 0.43397093 -0.04359709]\n",
      "Gradient Descent(278/999): loss=10.00926238264435, w0=-1.4926428455779792, w1=0.9516123623994371\n",
      "[ 0.43318866 -0.0435185 ]\n",
      "10.005461182236122\n",
      "[ 0.43318866 -0.0435185 ]\n",
      "Gradient Descent(279/999): loss=10.005461182236122, w0=-1.4969747321538047, w1=0.9520475473931662\n",
      "[ 0.43240779 -0.04344005]\n",
      "10.0016736735324\n",
      "[ 0.43240779 -0.04344005]\n",
      "Gradient Descent(280/999): loss=10.0016736735324, w0=-1.5012988100889904, w1=0.9524819479242441\n",
      "[ 0.43162834 -0.04336175]\n",
      "9.997899807216452\n",
      "[ 0.43162834 -0.04336175]\n",
      "Gradient Descent(281/999): loss=9.997899807216452, w0=-1.505615093459359, w1=0.9529155654067399\n",
      "[ 0.43085029 -0.04328358]\n",
      "9.99413953414919\n",
      "[ 0.43085029 -0.04328358]\n",
      "Gradient Descent(282/999): loss=9.99413953414919, w0=-1.5099235963153608, w1=0.9533484012521737\n",
      "[ 0.43007364 -0.04320556]\n",
      "9.990392805368513\n",
      "[ 0.43007364 -0.04320556]\n",
      "Gradient Descent(283/999): loss=9.990392805368513, w0=-1.514224332682118, w1=0.9537804568695212\n",
      "[ 0.42929839 -0.04312768]\n",
      "9.986659572088685\n",
      "[ 0.42929839 -0.04312768]\n",
      "Gradient Descent(284/999): loss=9.986659572088685, w0=-1.518517316559472, w1=0.9542117336652183\n",
      "[ 0.42852454 -0.04304994]\n",
      "9.98293978569969\n",
      "[ 0.42852454 -0.04304994]\n",
      "Gradient Descent(285/999): loss=9.98293978569969, w0=-1.522802561922028, w1=0.9546422330431656\n",
      "[ 0.42775208 -0.04297234]\n",
      "9.9792333977666\n",
      "[ 0.42775208 -0.04297234]\n",
      "Gradient Descent(286/999): loss=9.9792333977666, w0=-1.5270800827192001, w1=0.9550719564047332\n",
      "[ 0.42698102 -0.04289487]\n",
      "9.975540360028956\n",
      "[ 0.42698102 -0.04289487]\n",
      "Gradient Descent(287/999): loss=9.975540360028956, w0=-1.5313498928752576, w1=0.955500905148765\n",
      "[ 0.42621134 -0.04281755]\n",
      "9.97186062440012\n",
      "[ 0.42621134 -0.04281755]\n",
      "Gradient Descent(288/999): loss=9.97186062440012, w0=-1.5356120062893701, w1=0.9559290806715831\n",
      "[ 0.42544305 -0.04274037]\n",
      "9.968194142966665\n",
      "[ 0.42544305 -0.04274037]\n",
      "Gradient Descent(289/999): loss=9.968194142966665, w0=-1.5398664368356523, w1=0.956356484366993\n",
      "[ 0.42467615 -0.04266333]\n",
      "9.964540867987743\n",
      "[ 0.42467615 -0.04266333]\n",
      "Gradient Descent(290/999): loss=9.964540867987743, w0=-1.5441131983632097, w1=0.9567831176262876\n",
      "[ 0.42391063 -0.04258642]\n",
      "9.960900751894469\n",
      "[ 0.42391063 -0.04258642]\n",
      "Gradient Descent(291/999): loss=9.960900751894469, w0=-1.5483523046961836, w1=0.9572089818382518\n",
      "[ 0.42314649 -0.04250966]\n",
      "9.957273747289296\n",
      "[ 0.42314649 -0.04250966]\n",
      "Gradient Descent(292/999): loss=9.957273747289296, w0=-1.5525837696337954, w1=0.957634078389167\n",
      "[ 0.42238373 -0.04243303]\n",
      "9.953659806945398\n",
      "[ 0.42238373 -0.04243303]\n",
      "Gradient Descent(293/999): loss=9.953659806945398, w0=-1.5568076069503929, w1=0.9580584086628159\n",
      "[ 0.42162234 -0.04235654]\n",
      "9.950058883806062\n",
      "[ 0.42162234 -0.04235654]\n",
      "Gradient Descent(294/999): loss=9.950058883806062, w0=-1.5610238303954935, w1=0.9584819740404867\n",
      "[ 0.42086233 -0.04228019]\n",
      "9.946470930984068\n",
      "[ 0.42086233 -0.04228019]\n",
      "Gradient Descent(295/999): loss=9.946470930984068, w0=-1.5652324536938302, w1=0.9589047759009777\n",
      "[ 0.42010369 -0.04220397]\n",
      "9.94289590176108\n",
      "[ 0.42010369 -0.04220397]\n",
      "Gradient Descent(296/999): loss=9.94289590176108, w0=-1.569433490545396, w1=0.9593268156206016\n",
      "[ 0.41934641 -0.0421279 ]\n",
      "9.939333749587043\n",
      "[ 0.41934641 -0.0421279 ]\n",
      "Gradient Descent(297/999): loss=9.939333749587043, w0=-1.573626954625488, w1=0.9597480945731905\n",
      "[ 0.4185905  -0.04205196]\n",
      "9.93578442807957\n",
      "[ 0.4185905  -0.04205196]\n",
      "Gradient Descent(298/999): loss=9.93578442807957, w0=-1.5778128595847523, w1=0.9601686141300999\n",
      "[ 0.41783595 -0.04197615]\n",
      "9.932247891023335\n",
      "[ 0.41783595 -0.04197615]\n",
      "Gradient Descent(299/999): loss=9.932247891023335, w0=-1.5819912190492287, w1=0.9605883756602133\n",
      "[ 0.41708276 -0.04190049]\n",
      "9.928724092369489\n",
      "[ 0.41708276 -0.04190049]\n",
      "Gradient Descent(300/999): loss=9.928724092369489, w0=-1.5861620466203945, w1=0.9610073805299465\n",
      "[ 0.41633093 -0.04182496]\n",
      "9.925212986235039\n",
      "[ 0.41633093 -0.04182496]\n",
      "Gradient Descent(301/999): loss=9.925212986235039, w0=-1.5903253558752093, w1=0.9614256301032524\n",
      "[ 0.41558045 -0.04174956]\n",
      "9.92171452690226\n",
      "[ 0.41558045 -0.04174956]\n",
      "Gradient Descent(302/999): loss=9.92171452690226, w0=-1.5944811603661584, w1=0.9618431257416252\n",
      "[ 0.41483133 -0.04167431]\n",
      "9.918228668818106\n",
      "[ 0.41483133 -0.04167431]\n",
      "Gradient Descent(303/999): loss=9.918228668818106, w0=-1.598629473621298, w1=0.9622598688041049\n",
      "[ 0.41408355 -0.04159918]\n",
      "9.914755366593608\n",
      "[ 0.41408355 -0.04159918]\n",
      "Gradient Descent(304/999): loss=9.914755366593608, w0=-1.6027703091442984, w1=0.9626758606472816\n",
      "[ 0.41333713 -0.0415242 ]\n",
      "9.91129457500328\n",
      "[ 0.41333713 -0.0415242 ]\n",
      "Gradient Descent(305/999): loss=9.91129457500328, w0=-1.6069036804144885, w1=0.9630911026253001\n",
      "[ 0.41259205 -0.04144935]\n",
      "9.907846248984539\n",
      "[ 0.41259205 -0.04144935]\n",
      "Gradient Descent(306/999): loss=9.907846248984539, w0=-1.611029600886899, w1=0.9635055960898642\n",
      "[ 0.41184831 -0.04137463]\n",
      "9.90441034363712\n",
      "[ 0.41184831 -0.04137463]\n",
      "Gradient Descent(307/999): loss=9.90441034363712, w0=-1.6151480839923067, w1=0.963919342390241\n",
      "[ 0.41110591 -0.04130005]\n",
      "9.90098681422248\n",
      "[ 0.41110591 -0.04130005]\n",
      "Gradient Descent(308/999): loss=9.90098681422248, w0=-1.6192591431372787, w1=0.9643323428732659\n",
      "[ 0.41036486 -0.0412256 ]\n",
      "9.897575616163222\n",
      "[ 0.41036486 -0.0412256 ]\n",
      "Gradient Descent(309/999): loss=9.897575616163222, w0=-1.6233627917042148, w1=0.9647445988833456\n",
      "[ 0.40962513 -0.04115129]\n",
      "9.894176705042515\n",
      "[ 0.40962513 -0.04115129]\n",
      "Gradient Descent(310/999): loss=9.894176705042515, w0=-1.627459043051392, w1=0.9651561117624644\n",
      "[ 0.40888675 -0.04107711]\n",
      "9.890790036603518\n",
      "[ 0.40888675 -0.04107711]\n",
      "Gradient Descent(311/999): loss=9.890790036603518, w0=-1.6315479105130077, w1=0.965566882850187\n",
      "[ 0.40814969 -0.04100306]\n",
      "9.887415566748798\n",
      "[ 0.40814969 -0.04100306]\n",
      "Gradient Descent(312/999): loss=9.887415566748798, w0=-1.6356294073992232, w1=0.9659769134836632\n",
      "[ 0.40741396 -0.04092915]\n",
      "9.884053251539754\n",
      "[ 0.40741396 -0.04092915]\n",
      "Gradient Descent(313/999): loss=9.884053251539754, w0=-1.6397035469962071, w1=0.966386204997633\n",
      "[ 0.40667956 -0.04085537]\n",
      "9.880703047196057\n",
      "[ 0.40667956 -0.04085537]\n",
      "Gradient Descent(314/999): loss=9.880703047196057, w0=-1.643770342566178, w1=0.9667947587244299\n",
      "[ 0.40594648 -0.04078173]\n",
      "9.877364910095073\n",
      "[ 0.40594648 -0.04078173]\n",
      "Gradient Descent(315/999): loss=9.877364910095073, w0=-1.6478298073474484, w1=0.9672025759939861\n",
      "[ 0.40521472 -0.04070821]\n",
      "9.874038796771282\n",
      "[ 0.40521472 -0.04070821]\n",
      "Gradient Descent(316/999): loss=9.874038796771282, w0=-1.6518819545544674, w1=0.9676096581338364\n",
      "[ 0.40448428 -0.04063483]\n",
      "9.870724663915738\n",
      "[ 0.40448428 -0.04063483]\n",
      "Gradient Descent(317/999): loss=9.870724663915738, w0=-1.6559267973778635, w1=0.9680160064691224\n",
      "[ 0.40375516 -0.04056159]\n",
      "9.867422468375485\n",
      "[ 0.40375516 -0.04056159]\n",
      "Gradient Descent(318/999): loss=9.867422468375485, w0=-1.6599643489844884, w1=0.9684216223225973\n",
      "[ 0.40302735 -0.04048847]\n",
      "9.864132167152997\n",
      "[ 0.40302735 -0.04048847]\n",
      "Gradient Descent(319/999): loss=9.864132167152997, w0=-1.6639946225174589, w1=0.9688265070146297\n",
      "[ 0.40230086 -0.04041548]\n",
      "9.860853717405638\n",
      "[ 0.40230086 -0.04041548]\n",
      "Gradient Descent(320/999): loss=9.860853717405638, w0=-1.6680176310962, w1=0.969230661863208\n",
      "[ 0.40157567 -0.04034263]\n",
      "9.857587076445068\n",
      "[ 0.40157567 -0.04034263]\n",
      "Gradient Descent(321/999): loss=9.857587076445068, w0=-1.6720333878164881, w1=0.9696340881839453\n",
      "[ 0.40085179 -0.04026991]\n",
      "9.854332201736725\n",
      "[ 0.40085179 -0.04026991]\n",
      "Gradient Descent(322/999): loss=9.854332201736725, w0=-1.676041905750493, w1=0.9700367872900825\n",
      "[ 0.40012922 -0.04019732]\n",
      "9.851089050899246\n",
      "[ 0.40012922 -0.04019732]\n",
      "Gradient Descent(323/999): loss=9.851089050899246, w0=-1.6800431979468202, w1=0.9704387604924937\n",
      "[ 0.39940795 -0.04012486]\n",
      "9.847857581703924\n",
      "[ 0.39940795 -0.04012486]\n",
      "Gradient Descent(324/999): loss=9.847857581703924, w0=-1.6840372774305545, w1=0.9708400090996899\n",
      "[ 0.39868798 -0.04005253]\n",
      "9.844637752074158\n",
      "[ 0.39868798 -0.04005253]\n",
      "Gradient Descent(325/999): loss=9.844637752074158, w0=-1.6880241572033017, w1=0.9712405344178237\n",
      "[ 0.3979693  -0.03998033]\n",
      "9.841429520084906\n",
      "[ 0.3979693  -0.03998033]\n",
      "Gradient Descent(326/999): loss=9.841429520084906, w0=-1.6920038502432304, w1=0.9716403377506925\n",
      "[ 0.39725193 -0.03990826]\n",
      "9.838232843962137\n",
      "[ 0.39725193 -0.03990826]\n",
      "Gradient Descent(327/999): loss=9.838232843962137, w0=-1.695976369505115, w1=0.9720394203997442\n",
      "[ 0.39653584 -0.03983633]\n",
      "9.83504768208228\n",
      "[ 0.39653584 -0.03983633]\n",
      "Gradient Descent(328/999): loss=9.83504768208228, w0=-1.6999417279203783, w1=0.9724377836640803\n",
      "[ 0.39582105 -0.03976452]\n",
      "9.831873992971705\n",
      "[ 0.39582105 -0.03976452]\n",
      "Gradient Descent(329/999): loss=9.831873992971705, w0=-1.7038999383971323, w1=0.9728354288404608\n",
      "[ 0.39510754 -0.03969284]\n",
      "9.82871173530615\n",
      "[ 0.39510754 -0.03969284]\n",
      "Gradient Descent(330/999): loss=9.82871173530615, w0=-1.707851013820221, w1=0.973232357223308\n",
      "[ 0.39439532 -0.03962129]\n",
      "9.825560867910221\n",
      "[ 0.39439532 -0.03962129]\n",
      "Gradient Descent(331/999): loss=9.825560867910221, w0=-1.7117949670512624, w1=0.973628570104711\n",
      "[ 0.39368439 -0.03954987]\n",
      "9.822421349756818\n",
      "[ 0.39368439 -0.03954987]\n",
      "Gradient Descent(332/999): loss=9.822421349756818, w0=-1.71573181092869, w1=0.9740240687744296\n",
      "[ 0.39297473 -0.03947857]\n",
      "9.819293139966625\n",
      "[ 0.39297473 -0.03947857]\n",
      "Gradient Descent(333/999): loss=9.819293139966625, w0=-1.7196615582677952, w1=0.9744188545198987\n",
      "[ 0.39226636 -0.03940741]\n",
      "9.816176197807568\n",
      "[ 0.39226636 -0.03940741]\n",
      "Gradient Descent(334/999): loss=9.816176197807568, w0=-1.723584221860768, w1=0.9748129286262328\n",
      "[ 0.39155926 -0.03933637]\n",
      "9.813070482694297\n",
      "[ 0.39155926 -0.03933637]\n",
      "Gradient Descent(335/999): loss=9.813070482694297, w0=-1.7274998144767397, w1=0.9752062923762296\n",
      "[ 0.39085344 -0.03926547]\n",
      "9.80997595418763\n",
      "[ 0.39085344 -0.03926547]\n",
      "Gradient Descent(336/999): loss=9.80997595418763, w0=-1.731408348861824, w1=0.9755989470503743\n",
      "[ 0.39014889 -0.03919469]\n",
      "9.806892571994066\n",
      "[ 0.39014889 -0.03919469]\n",
      "Gradient Descent(337/999): loss=9.806892571994066, w0=-1.7353098377391583, w1=0.9759908939268442\n",
      "[ 0.38944561 -0.03912404]\n",
      "9.80382029596522\n",
      "[ 0.38944561 -0.03912404]\n",
      "Gradient Descent(338/999): loss=9.80382029596522, w0=-1.7392042938089454, w1=0.9763821342815124\n",
      "[ 0.38874359 -0.03905351]\n",
      "9.800759086097333\n",
      "[ 0.38874359 -0.03905351]\n",
      "Gradient Descent(339/999): loss=9.800759086097333, w0=-1.743091729748495, w1=0.9767726693879522\n",
      "[ 0.38804285 -0.03898311]\n",
      "9.797708902530731\n",
      "[ 0.38804285 -0.03898311]\n",
      "Gradient Descent(340/999): loss=9.797708902530731, w0=-1.7469721582122641, w1=0.977162500517441\n",
      "[ 0.38734336 -0.03891284]\n",
      "9.794669705549312\n",
      "[ 0.38734336 -0.03891284]\n",
      "Gradient Descent(341/999): loss=9.794669705549312, w0=-1.7508455918318997, w1=0.9775516289389647\n",
      "[ 0.38664514 -0.0388427 ]\n",
      "9.791641455580033\n",
      "[ 0.38664514 -0.0388427 ]\n",
      "Gradient Descent(342/999): loss=9.791641455580033, w0=-1.7547120432162784, w1=0.977940055919222\n",
      "[ 0.38594817 -0.03877268]\n",
      "9.788624113192384\n",
      "[ 0.38594817 -0.03877268]\n",
      "Gradient Descent(343/999): loss=9.788624113192384, w0=-1.7585715249515483, w1=0.9783277827226277\n",
      "[ 0.38525246 -0.03870279]\n",
      "9.785617639097888\n",
      "[ 0.38525246 -0.03870279]\n",
      "Gradient Descent(344/999): loss=9.785617639097888, w0=-1.7624240496011698, w1=0.9787148106113178\n",
      "[ 0.38455801 -0.03863302]\n",
      "9.782621994149578\n",
      "[ 0.38455801 -0.03863302]\n",
      "Gradient Descent(345/999): loss=9.782621994149578, w0=-1.7662696297059566, w1=0.979101140845153\n",
      "[ 0.38386481 -0.03856338]\n",
      "9.779637139341498\n",
      "[ 0.38386481 -0.03856338]\n",
      "Gradient Descent(346/999): loss=9.779637139341498, w0=-1.770108277784116, w1=0.979486774681723\n",
      "[ 0.38317285 -0.03849387]\n",
      "9.77666303580818\n",
      "[ 0.38317285 -0.03849387]\n",
      "Gradient Descent(347/999): loss=9.77666303580818, w0=-1.77394000633129, w1=0.9798717133763506\n",
      "[ 0.38248215 -0.03842448]\n",
      "9.77369964482416\n",
      "[ 0.38248215 -0.03842448]\n",
      "Gradient Descent(348/999): loss=9.77369964482416, w0=-1.7777648278205966, w1=0.980255958182096\n",
      "[ 0.38179269 -0.03835522]\n",
      "9.77074692780345\n",
      "[ 0.38179269 -0.03835522]\n",
      "Gradient Descent(349/999): loss=9.77074692780345, w0=-1.7815827547026695, w1=0.98063951034976\n",
      "[ 0.38110447 -0.03828608]\n",
      "9.767804846299047\n",
      "[ 0.38110447 -0.03828608]\n",
      "Gradient Descent(350/999): loss=9.767804846299047, w0=-1.7853937994056985, w1=0.9810223711278895\n",
      "[ 0.38041749 -0.03821706]\n",
      "9.76487336200244\n",
      "[ 0.38041749 -0.03821706]\n",
      "Gradient Descent(351/999): loss=9.76487336200244, w0=-1.7891979743354711, w1=0.9814045417627801\n",
      "[ 0.37973175 -0.03814817]\n",
      "9.761952436743094\n",
      "[ 0.37973175 -0.03814817]\n",
      "Gradient Descent(352/999): loss=9.761952436743094, w0=-1.7929952918754117, w1=0.9817860234984812\n",
      "[ 0.37904725 -0.03807941]\n",
      "9.75904203248797\n",
      "[ 0.37904725 -0.03807941]\n",
      "Gradient Descent(353/999): loss=9.75904203248797, w0=-1.7967857643866227, w1=0.9821668175767997\n",
      "[ 0.37836398 -0.03801077]\n",
      "9.756142111341019\n",
      "[ 0.37836398 -0.03801077]\n",
      "Gradient Descent(354/999): loss=9.756142111341019, w0=-1.8005694042079243, w1=0.9825469252373038\n",
      "[ 0.37768194 -0.03794225]\n",
      "9.753252635542681\n",
      "[ 0.37768194 -0.03794225]\n",
      "Gradient Descent(355/999): loss=9.753252635542681, w0=-1.8043462236558947, w1=0.9829263477173273\n",
      "[ 0.37700114 -0.03787385]\n",
      "9.750373567469426\n",
      "[ 0.37700114 -0.03787385]\n",
      "Gradient Descent(356/999): loss=9.750373567469426, w0=-1.8081162350249103, w1=0.983305086251974\n",
      "[ 0.37632156 -0.03780558]\n",
      "9.74750486963322\n",
      "[ 0.37632156 -0.03780558]\n",
      "Gradient Descent(357/999): loss=9.74750486963322, w0=-1.8118794505871858, w1=0.9836831420741207\n",
      "[ 0.3756432  -0.03773743]\n",
      "9.74464650468107\n",
      "[ 0.3756432  -0.03773743]\n",
      "Gradient Descent(358/999): loss=9.74464650468107, w0=-1.815635882592814, w1=0.9840605164144222\n",
      "[ 0.37496607 -0.03766941]\n",
      "9.741798435394525\n",
      "[ 0.37496607 -0.03766941]\n",
      "Gradient Descent(359/999): loss=9.741798435394525, w0=-1.819385543269806, w1=0.9844372105013148\n",
      "[ 0.37429016 -0.03760151]\n",
      "9.738960624689186\n",
      "[ 0.37429016 -0.03760151]\n",
      "Gradient Descent(360/999): loss=9.738960624689186, w0=-1.8231284448241303, w1=0.9848132255610205\n",
      "[ 0.37361546 -0.03753373]\n",
      "9.736133035614245\n",
      "[ 0.37361546 -0.03753373]\n",
      "Gradient Descent(361/999): loss=9.736133035614245, w0=-1.8268645994397532, w1=0.9851885628175507\n",
      "[ 0.37294198 -0.03746607]\n",
      "9.733315631351974\n",
      "[ 0.37294198 -0.03746607]\n",
      "Gradient Descent(362/999): loss=9.733315631351974, w0=-1.8305940192786783, w1=0.9855632234927107\n",
      "[ 0.37226972 -0.03739853]\n",
      "9.730508375217266\n",
      "[ 0.37226972 -0.03739853]\n",
      "Gradient Descent(363/999): loss=9.730508375217266, w0=-1.8343167164809857, w1=0.9859372088061034\n",
      "[ 0.37159867 -0.03733112]\n",
      "9.727711230657148\n",
      "[ 0.37159867 -0.03733112]\n",
      "Gradient Descent(364/999): loss=9.727711230657148, w0=-1.8380327031648724, w1=0.9863105199751329\n",
      "[ 0.37092883 -0.03726382]\n",
      "9.724924161250318\n",
      "[ 0.37092883 -0.03726382]\n",
      "Gradient Descent(365/999): loss=9.724924161250318, w0=-1.8417419914266906, w1=0.986683158215009\n",
      "[ 0.37026019 -0.03719665]\n",
      "9.722147130706649\n",
      "[ 0.37026019 -0.03719665]\n",
      "Gradient Descent(366/999): loss=9.722147130706649, w0=-1.845444593340988, w1=0.9870551247387511\n",
      "[ 0.36959276 -0.0371296 ]\n",
      "9.719380102866744\n",
      "[ 0.36959276 -0.0371296 ]\n",
      "Gradient Descent(367/999): loss=9.719380102866744, w0=-1.8491405209605467, w1=0.9874264207571918\n",
      "[ 0.36892654 -0.03706267]\n",
      "9.716623041701428\n",
      "[ 0.36892654 -0.03706267]\n",
      "Gradient Descent(368/999): loss=9.716623041701428, w0=-1.8528297863164227, w1=0.9877970474789811\n",
      "[ 0.36826151 -0.03699586]\n",
      "9.713875911311328\n",
      "[ 0.36826151 -0.03699586]\n",
      "Gradient Descent(369/999): loss=9.713875911311328, w0=-1.8565124014179843, w1=0.9881670061105906\n",
      "[ 0.36759768 -0.03692917]\n",
      "9.711138675926357\n",
      "[ 0.36759768 -0.03692917]\n",
      "Gradient Descent(370/999): loss=9.711138675926357, w0=-1.8601883782529525, w1=0.9885362978563167\n",
      "[ 0.36693505 -0.03686261]\n",
      "9.70841129990528\n",
      "[ 0.36693505 -0.03686261]\n",
      "Gradient Descent(371/999): loss=9.70841129990528, w0=-1.8638577287874387, w1=0.988904923918285\n",
      "[ 0.36627362 -0.03679616]\n",
      "9.705693747735234\n",
      "[ 0.36627362 -0.03679616]\n",
      "Gradient Descent(372/999): loss=9.705693747735234, w0=-1.8675204649659847, w1=0.9892728854964544\n",
      "[ 0.36561337 -0.03672983]\n",
      "9.70298598403128\n",
      "[ 0.36561337 -0.03672983]\n",
      "Gradient Descent(373/999): loss=9.70298598403128, w0=-1.8711765987116007, w1=0.9896401837886206\n",
      "[ 0.36495432 -0.03666362]\n",
      "9.700287973535923\n",
      "[ 0.36495432 -0.03666362]\n",
      "Gradient Descent(374/999): loss=9.700287973535923, w0=-1.8748261419258045, w1=0.9900068199904202\n",
      "[ 0.36429646 -0.03659753]\n",
      "9.697599681118671\n",
      "[ 0.36429646 -0.03659753]\n",
      "Gradient Descent(375/999): loss=9.697599681118671, w0=-1.878469106488661, w1=0.9903727952953345\n",
      "[ 0.36363978 -0.03653156]\n",
      "9.69492107177557\n",
      "[ 0.36363978 -0.03653156]\n",
      "Gradient Descent(376/999): loss=9.69492107177557, w0=-1.882105504258819, w1=0.9907381108946934\n",
      "[ 0.36298428 -0.03646571]\n",
      "9.69225211062874\n",
      "[ 0.36298428 -0.03646571]\n",
      "Gradient Descent(377/999): loss=9.69225211062874, w0=-1.885735347073552, w1=0.9911027679776796\n",
      "[ 0.36232997 -0.03639998]\n",
      "9.689592762925939\n",
      "[ 0.36232997 -0.03639998]\n",
      "Gradient Descent(378/999): loss=9.689592762925939, w0=-1.8893586467487953, w1=0.9914667677313317\n",
      "[ 0.36167683 -0.03633436]\n",
      "9.686942994040093\n",
      "[ 0.36167683 -0.03633436]\n",
      "Gradient Descent(379/999): loss=9.686942994040093, w0=-1.8929754150791847, w1=0.9918301113405489\n",
      "[ 0.36102488 -0.03626886]\n",
      "9.684302769468855\n",
      "[ 0.36102488 -0.03626886]\n",
      "Gradient Descent(380/999): loss=9.684302769468855, w0=-1.896585663838095, w1=0.9921927999880944\n",
      "[ 0.36037409 -0.03620349]\n",
      "9.681672054834163\n",
      "[ 0.36037409 -0.03620349]\n",
      "Gradient Descent(381/999): loss=9.681672054834163, w0=-1.9001894047776786, w1=0.9925548348545994\n",
      "[ 0.35972449 -0.03613823]\n",
      "9.679050815881766\n",
      "[ 0.35972449 -0.03613823]\n",
      "Gradient Descent(382/999): loss=9.679050815881766, w0=-1.9037866496289035, w1=0.9929162171185668\n",
      "[ 0.35907605 -0.03607308]\n",
      "9.676439018480808\n",
      "[ 0.35907605 -0.03607308]\n",
      "Gradient Descent(383/999): loss=9.676439018480808, w0=-1.9073774101015915, w1=0.9932769479563754\n",
      "[ 0.35842878 -0.03600806]\n",
      "9.673836628623365\n",
      "[ 0.35842878 -0.03600806]\n",
      "Gradient Descent(384/999): loss=9.673836628623365, w0=-1.910961697884456, w1=0.9936370285422832\n",
      "[ 0.35778268 -0.03594315]\n",
      "9.671243612424005\n",
      "[ 0.35778268 -0.03594315]\n",
      "Gradient Descent(385/999): loss=9.671243612424005, w0=-1.9145395246451407, w1=0.9939964600484313\n",
      "[ 0.35713774 -0.03587836]\n",
      "9.668659936119354\n",
      "[ 0.35713774 -0.03587836]\n",
      "Gradient Descent(386/999): loss=9.668659936119354, w0=-1.9181109020302571, w1=0.9943552436448485\n",
      "[ 0.35649396 -0.03581369]\n",
      "9.666085566067652\n",
      "[ 0.35649396 -0.03581369]\n",
      "Gradient Descent(387/999): loss=9.666085566067652, w0=-1.921675841665423, w1=0.9947133804994541\n",
      "[ 0.35585135 -0.03574913]\n",
      "9.663520468748313\n",
      "[ 0.35585135 -0.03574913]\n",
      "Gradient Descent(388/999): loss=9.663520468748313, w0=-1.9252343551552993, w1=0.9950708717780621\n",
      "[ 0.35520989 -0.03568469]\n",
      "9.66096461076149\n",
      "[ 0.35520989 -0.03568469]\n",
      "Gradient Descent(389/999): loss=9.66096461076149, w0=-1.9287864540836286, w1=0.9954277186443852\n",
      "[ 0.35456959 -0.03562036]\n",
      "9.658417958827634\n",
      "[ 0.35456959 -0.03562036]\n",
      "Gradient Descent(390/999): loss=9.658417958827634, w0=-1.9323321500132729, w1=0.995783922260038\n",
      "[ 0.35393045 -0.03555615]\n",
      "9.655880479787083\n",
      "[ 0.35393045 -0.03555615]\n",
      "Gradient Descent(391/999): loss=9.655880479787083, w0=-1.9358714544862508, w1=0.9961394837845419\n",
      "[ 0.35329245 -0.03549206]\n",
      "9.653352140599598\n",
      "[ 0.35329245 -0.03549206]\n",
      "Gradient Descent(392/999): loss=9.653352140599598, w0=-1.9394043790237754, w1=0.9964944043753273\n",
      "[ 0.35265561 -0.03542808]\n",
      "9.650832908343958\n",
      "[ 0.35265561 -0.03542808]\n",
      "Gradient Descent(393/999): loss=9.650832908343958, w0=-1.9429309351262916, w1=0.9968486851877391\n",
      "[ 0.35201991 -0.03536422]\n",
      "9.648322750217522\n",
      "[ 0.35201991 -0.03536422]\n",
      "Gradient Descent(394/999): loss=9.648322750217522, w0=-1.9464511342735138, w1=0.9972023273750387\n",
      "[ 0.35138537 -0.03530047]\n",
      "9.645821633535796\n",
      "[ 0.35138537 -0.03530047]\n",
      "Gradient Descent(395/999): loss=9.645821633535796, w0=-1.9499649879244632, w1=0.9975553320884093\n",
      "[ 0.35075196 -0.03523684]\n",
      "9.643329525732023\n",
      "[ 0.35075196 -0.03523684]\n",
      "Gradient Descent(396/999): loss=9.643329525732023, w0=-1.9534725075175046, w1=0.9979077004769588\n",
      "[ 0.3501197  -0.03517332]\n",
      "9.640846394356743\n",
      "[ 0.3501197  -0.03517332]\n",
      "Gradient Descent(397/999): loss=9.640846394356743, w0=-1.9569737044703845, w1=0.9982594336877233\n",
      "[ 0.34948857 -0.03510992]\n",
      "9.638372207077376\n",
      "[ 0.34948857 -0.03510992]\n",
      "Gradient Descent(398/999): loss=9.638372207077376, w0=-1.9604685901802676, w1=0.998610532865672\n",
      "[ 0.34885858 -0.03504663]\n",
      "9.635906931677804\n",
      "[ 0.34885858 -0.03504663]\n",
      "Gradient Descent(399/999): loss=9.635906931677804, w0=-1.9639571760237742, w1=0.9989609991537096\n",
      "[ 0.34822973 -0.03498345]\n",
      "9.633450536057955\n",
      "[ 0.34822973 -0.03498345]\n",
      "Gradient Descent(400/999): loss=9.633450536057955, w0=-1.9674394733570169, w1=0.9993108336926809\n",
      "[ 0.34760202 -0.03492039]\n",
      "9.631002988233373\n",
      "[ 0.34760202 -0.03492039]\n",
      "Gradient Descent(401/999): loss=9.631002988233373, w0=-1.970915493515638, w1=0.9996600376213741\n",
      "[ 0.34697543 -0.03485745]\n",
      "9.628564256334807\n",
      "[ 0.34697543 -0.03485745]\n",
      "Gradient Descent(402/999): loss=9.628564256334807, w0=-1.9743852478148467, w1=1.0000086120765248\n",
      "[ 0.34634997 -0.03479461]\n",
      "9.626134308607803\n",
      "[ 0.34634997 -0.03479461]\n",
      "Gradient Descent(403/999): loss=9.626134308607803, w0=-1.9778487475494546, w1=1.000356558192819\n",
      "[ 0.34572564 -0.03473189]\n",
      "9.623713113412283\n",
      "[ 0.34572564 -0.03473189]\n",
      "Gradient Descent(404/999): loss=9.623713113412283, w0=-1.9813060039939139, w1=1.0007038771028982\n",
      "[ 0.34510244 -0.03466928]\n",
      "9.621300639222131\n",
      "[ 0.34510244 -0.03466928]\n",
      "Gradient Descent(405/999): loss=9.621300639222131, w0=-1.984757028402353, w1=1.0010505699373613\n",
      "[ 0.34448036 -0.03460679]\n",
      "9.61889685462479\n",
      "[ 0.34448036 -0.03460679]\n",
      "Gradient Descent(406/999): loss=9.61889685462479, w0=-1.9882018320086143, w1=1.0013966378247696\n",
      "[ 0.3438594  -0.03454441]\n",
      "9.616501728320848\n",
      "[ 0.3438594  -0.03454441]\n",
      "Gradient Descent(407/999): loss=9.616501728320848, w0=-1.9916404260262899, w1=1.0017420818916503\n",
      "[ 0.34323956 -0.03448214]\n",
      "9.614115229123634\n",
      "[ 0.34323956 -0.03448214]\n",
      "Gradient Descent(408/999): loss=9.614115229123634, w0=-1.9950728216487579, w1=1.0020869032624995\n",
      "[ 0.34262084 -0.03441998]\n",
      "9.611737325958806\n",
      "[ 0.34262084 -0.03441998]\n",
      "Gradient Descent(409/999): loss=9.611737325958806, w0=-1.9984990300492198, w1=1.0024311030597866\n",
      "[ 0.34200323 -0.03435793]\n",
      "9.60936798786395\n",
      "[ 0.34200323 -0.03435793]\n",
      "Gradient Descent(410/999): loss=9.60936798786395, w0=-2.001919062380736, w1=1.0027746824039572\n",
      "[ 0.34138674 -0.034296  ]\n",
      "9.60700718398818\n",
      "[ 0.34138674 -0.034296  ]\n",
      "Gradient Descent(411/999): loss=9.60700718398818, w0=-2.0053329297762628, w1=1.0031176424134378\n",
      "[ 0.34077136 -0.03423418]\n",
      "9.604654883591731\n",
      "[ 0.34077136 -0.03423418]\n",
      "Gradient Descent(412/999): loss=9.604654883591731, w0=-2.008740643348688, w1=1.0034599842046386\n",
      "[ 0.34015708 -0.03417247]\n",
      "9.60231105604556\n",
      "[ 0.34015708 -0.03417247]\n",
      "Gradient Descent(413/999): loss=9.60231105604556, w0=-2.0121422141908676, w1=1.0038017088919569\n",
      "[ 0.33954392 -0.03411087]\n",
      "9.599975670830952\n",
      "[ 0.33954392 -0.03411087]\n",
      "Gradient Descent(414/999): loss=9.599975670830952, w0=-2.015537653375661, w1=1.004142817587782\n",
      "[ 0.33893186 -0.03404938]\n",
      "9.59764869753911\n",
      "[ 0.33893186 -0.03404938]\n",
      "Gradient Descent(415/999): loss=9.59764869753911, w0=-2.018926971955968, w1=1.0044833114024971\n",
      "[ 0.3383209 -0.033988 ]\n",
      "9.595330105870783\n",
      "[ 0.3383209 -0.033988 ]\n",
      "Gradient Descent(416/999): loss=9.595330105870783, w0=-2.0223101809647654, w1=1.004823191444485\n",
      "[ 0.33771105 -0.03392674]\n",
      "9.593019865635839\n",
      "[ 0.33771105 -0.03392674]\n",
      "Gradient Descent(417/999): loss=9.593019865635839, w0=-2.025687291415141, w1=1.0051624588201293\n",
      "[ 0.33710229 -0.03386558]\n",
      "9.590717946752898\n",
      "[ 0.33710229 -0.03386558]\n",
      "Gradient Descent(418/999): loss=9.590717946752898, w0=-2.0290583143003307, w1=1.00550111463382\n",
      "[ 0.33649463 -0.03380454]\n",
      "9.588424319248933\n",
      "[ 0.33649463 -0.03380454]\n",
      "Gradient Descent(419/999): loss=9.588424319248933, w0=-2.0324232605937538, w1=1.005839159987956\n",
      "[ 0.33588807 -0.0337436 ]\n",
      "9.586138953258873\n",
      "[ 0.33588807 -0.0337436 ]\n",
      "Gradient Descent(420/999): loss=9.586138953258873, w0=-2.0357821412490495, w1=1.0061765959829492\n",
      "[ 0.3352826  -0.03368277]\n",
      "9.583861819025223\n",
      "[ 0.3352826  -0.03368277]\n",
      "Gradient Descent(421/999): loss=9.583861819025223, w0=-2.039134967200112, w1=1.006513423717228\n",
      "[ 0.33467822 -0.03362206]\n",
      "9.581592886897674\n",
      "[ 0.33467822 -0.03362206]\n",
      "Gradient Descent(422/999): loss=9.581592886897674, w0=-2.042481749361125, w1=1.0068496442872403\n",
      "[ 0.33407493 -0.03356145]\n",
      "9.579332127332709\n",
      "[ 0.33407493 -0.03356145]\n",
      "Gradient Descent(423/999): loss=9.579332127332709, w0=-2.0458224986266003, w1=1.0071852587874581\n",
      "[ 0.33347272 -0.03350095]\n",
      "9.577079510893228\n",
      "[ 0.33347272 -0.03350095]\n",
      "Gradient Descent(424/999): loss=9.577079510893228, w0=-2.0491572258714092, w1=1.0075202683103803\n",
      "[ 0.33287161 -0.03344056]\n",
      "9.574835008248169\n",
      "[ 0.33287161 -0.03344056]\n",
      "Gradient Descent(425/999): loss=9.574835008248169, w0=-2.0524859419508217, w1=1.0078546739465364\n",
      "[ 0.33227157 -0.03338028]\n",
      "9.572598590172108\n",
      "[ 0.33227157 -0.03338028]\n",
      "Gradient Descent(426/999): loss=9.572598590172108, w0=-2.055808657700539, w1=1.0081884767844902\n",
      "[ 0.33167262 -0.03332011]\n",
      "9.570370227544895\n",
      "[ 0.33167262 -0.03332011]\n",
      "Gradient Descent(427/999): loss=9.570370227544895, w0=-2.0591253839367307, w1=1.008521677910843\n",
      "[ 0.33107475 -0.03326005]\n",
      "9.56814989135127\n",
      "[ 0.33107475 -0.03326005]\n",
      "Gradient Descent(428/999): loss=9.56814989135127, w0=-2.0624361314560686, w1=1.008854278410238\n",
      "[ 0.33047796 -0.0332001 ]\n",
      "9.565937552680477\n",
      "[ 0.33047796 -0.0332001 ]\n",
      "Gradient Descent(429/999): loss=9.565937552680477, w0=-2.0657409110357627, w1=1.0091862793653623\n",
      "[ 0.32988224 -0.03314025]\n",
      "9.563733182725908\n",
      "[ 0.32988224 -0.03314025]\n",
      "Gradient Descent(430/999): loss=9.563733182725908, w0=-2.069039733433596, w1=1.0095176818569525\n",
      "[ 0.3292876  -0.03308051]\n",
      "9.561536752784702\n",
      "[ 0.3292876  -0.03308051]\n",
      "Gradient Descent(431/999): loss=9.561536752784702, w0=-2.07233260938796, w1=1.0098484869637963\n",
      "[ 0.32869402 -0.03302088]\n",
      "9.55934823425739\n",
      "[ 0.32869402 -0.03302088]\n",
      "Gradient Descent(432/999): loss=9.55934823425739, w0=-2.075619549617888, w1=1.0101786957627366\n",
      "[ 0.32810152 -0.03296136]\n",
      "9.557167598647519\n",
      "[ 0.32810152 -0.03296136]\n",
      "Gradient Descent(433/999): loss=9.557167598647519, w0=-2.078900564823093, w1=1.0105083093286757\n",
      "[ 0.32751009 -0.03290194]\n",
      "9.55499481756127\n",
      "[ 0.32751009 -0.03290194]\n",
      "Gradient Descent(434/999): loss=9.55499481756127, w0=-2.082175665684, w1=1.0108373287345784\n",
      "[ 0.32691972 -0.03284263]\n",
      "9.552829862707103\n",
      "[ 0.32691972 -0.03284263]\n",
      "Gradient Descent(435/999): loss=9.552829862707103, w0=-2.08544486286178, w1=1.0111657550514748\n",
      "[ 0.32633041 -0.03278343]\n",
      "9.550672705895384\n",
      "[ 0.32633041 -0.03278343]\n",
      "Gradient Descent(436/999): loss=9.550672705895384, w0=-2.0887081669983885, w1=1.0114935893484647\n",
      "[ 0.32574217 -0.03272433]\n",
      "9.54852331903801\n",
      "[ 0.32574217 -0.03272433]\n",
      "Gradient Descent(437/999): loss=9.54852331903801, w0=-2.091965588716597, w1=1.011820832692721\n",
      "[ 0.32515499 -0.03266535]\n",
      "9.546381674148062\n",
      "[ 0.32515499 -0.03266535]\n",
      "Gradient Descent(438/999): loss=9.546381674148062, w0=-2.095217138620028, w1=1.0121474861494921\n",
      "[ 0.32456887 -0.03260646]\n",
      "9.544247743339415\n",
      "[ 0.32456887 -0.03260646]\n",
      "Gradient Descent(439/999): loss=9.544247743339415, w0=-2.09846282729319, w1=1.0124735507821072\n",
      "[ 0.3239838  -0.03254769]\n",
      "9.542121498826393\n",
      "[ 0.3239838  -0.03254769]\n",
      "Gradient Descent(440/999): loss=9.542121498826393, w0=-2.1017026653015125, w1=1.012799027651978\n",
      "[ 0.32339979 -0.03248902]\n",
      "9.540002912923402\n",
      "[ 0.32339979 -0.03248902]\n",
      "Gradient Descent(441/999): loss=9.540002912923402, w0=-2.1049366631913795, w1=1.0131239178186033\n",
      "[ 0.32281683 -0.03243045]\n",
      "9.537891958044574\n",
      "[ 0.32281683 -0.03243045]\n",
      "Gradient Descent(442/999): loss=9.537891958044574, w0=-2.108164831490164, w1=1.0134482223395718\n",
      "[ 0.32223492 -0.03237199]\n",
      "9.535788606703395\n",
      "[ 0.32223492 -0.03237199]\n",
      "Gradient Descent(443/999): loss=9.535788606703395, w0=-2.111387180706263, w1=1.013771942270566\n",
      "[ 0.32165406 -0.03231364]\n",
      "9.533692831512365\n",
      "[ 0.32165406 -0.03231364]\n",
      "Gradient Descent(444/999): loss=9.533692831512365, w0=-2.11460372132913, w1=1.0140950786653653\n",
      "[ 0.32107425 -0.03225539]\n",
      "9.53160460518263\n",
      "[ 0.32107425 -0.03225539]\n",
      "Gradient Descent(445/999): loss=9.53160460518263, w0=-2.117814463829311, w1=1.0144176325758494\n",
      "[ 0.32049548 -0.03219725]\n",
      "9.529523900523625\n",
      "[ 0.32049548 -0.03219725]\n",
      "Gradient Descent(446/999): loss=9.529523900523625, w0=-2.121019418658478, w1=1.0147396050520026\n",
      "[ 0.31991776 -0.03213921]\n",
      "9.527450690442725\n",
      "[ 0.31991776 -0.03213921]\n",
      "Gradient Descent(447/999): loss=9.527450690442725, w0=-2.1242185962494626, w1=1.0150609971419156\n",
      "[ 0.31934108 -0.03208127]\n",
      "9.525384947944893\n",
      "[ 0.31934108 -0.03208127]\n",
      "Gradient Descent(448/999): loss=9.525384947944893, w0=-2.1274120070162903, w1=1.0153818098917904\n",
      "[ 0.31876543 -0.03202345]\n",
      "9.523326646132327\n",
      "[ 0.31876543 -0.03202345]\n",
      "Gradient Descent(449/999): loss=9.523326646132327, w0=-2.1305996613542137, w1=1.0157020443459428\n",
      "[ 0.31819083 -0.03196572]\n",
      "9.521275758204107\n",
      "[ 0.31819083 -0.03196572]\n",
      "Gradient Descent(450/999): loss=9.521275758204107, w0=-2.1337815696397477, w1=1.0160217015468063\n",
      "[ 0.31761726 -0.0319081 ]\n",
      "9.51923225745585\n",
      "[ 0.31761726 -0.0319081 ]\n",
      "Gradient Descent(451/999): loss=9.51923225745585, w0=-2.1369577422307025, w1=1.0163407825349353\n",
      "[ 0.31704472 -0.03185058]\n",
      "9.51719611727936\n",
      "[ 0.31704472 -0.03185058]\n",
      "Gradient Descent(452/999): loss=9.51719611727936, w0=-2.140128189466217, w1=1.0166592883490084\n",
      "[ 0.31647322 -0.03179317]\n",
      "9.51516731116228\n",
      "[ 0.31647322 -0.03179317]\n",
      "Gradient Descent(453/999): loss=9.51516731116228, w0=-2.1432929216667933, w1=1.016977220025832\n",
      "[ 0.31590275 -0.03173586]\n",
      "9.513145812687755\n",
      "[ 0.31590275 -0.03173586]\n",
      "Gradient Descent(454/999): loss=9.513145812687755, w0=-2.1464519491343292, w1=1.0172945786003436\n",
      "[ 0.3153333  -0.03167865]\n",
      "9.511131595534076\n",
      "[ 0.3153333  -0.03167865]\n",
      "Gradient Descent(455/999): loss=9.511131595534076, w0=-2.149605282152153, w1=1.017611365105615\n",
      "[ 0.31476488 -0.03162155]\n",
      "9.509124633474345\n",
      "[ 0.31476488 -0.03162155]\n",
      "Gradient Descent(456/999): loss=9.509124633474345, w0=-2.1527529309850553, w1=1.0179275805728554\n",
      "[ 0.31419749 -0.03156455]\n",
      "9.507124900376134\n",
      "[ 0.31419749 -0.03156455]\n",
      "Gradient Descent(457/999): loss=9.507124900376134, w0=-2.155894905879325, w1=1.018243226031416\n",
      "[ 0.31363112 -0.03150765]\n",
      "9.505132370201139\n",
      "[ 0.31363112 -0.03150765]\n",
      "Gradient Descent(458/999): loss=9.505132370201139, w0=-2.159031217062779, w1=1.0185583025087923\n",
      "[ 0.31306577 -0.03145085]\n",
      "9.50314701700485\n",
      "[ 0.31306577 -0.03145085]\n",
      "Gradient Descent(459/999): loss=9.50314701700485, w0=-2.1621618747448, w1=1.0188728110306269\n",
      "[ 0.31250144 -0.03139416]\n",
      "9.501168814936197\n",
      "[ 0.31250144 -0.03139416]\n",
      "Gradient Descent(460/999): loss=9.501168814936197, w0=-2.165286889116365, w1=1.0191867526207143\n",
      "[ 0.31193812 -0.03133757]\n",
      "9.499197738237239\n",
      "[ 0.31193812 -0.03133757]\n",
      "Gradient Descent(461/999): loss=9.499197738237239, w0=-2.1684062703500824, w1=1.0195001283010032\n",
      "[ 0.31137583 -0.03128108]\n",
      "9.4972337612428\n",
      "[ 0.31137583 -0.03128108]\n",
      "Gradient Descent(462/999): loss=9.4972337612428, w0=-2.1715200286002228, w1=1.0198129390916004\n",
      "[ 0.31081454 -0.03122469]\n",
      "9.495276858380155\n",
      "[ 0.31081454 -0.03122469]\n",
      "Gradient Descent(463/999): loss=9.495276858380155, w0=-2.174628174002753, w1=1.0201251860107734\n",
      "[ 0.31025427 -0.03116841]\n",
      "9.493327004168691\n",
      "[ 0.31025427 -0.03116841]\n",
      "Gradient Descent(464/999): loss=9.493327004168691, w0=-2.1777307166753683, w1=1.0204368700749549\n",
      "[ 0.309695   -0.03111222]\n",
      "9.491384173219572\n",
      "[ 0.309695   -0.03111222]\n",
      "Gradient Descent(465/999): loss=9.491384173219572, w0=-2.180827666717527, w1=1.0207479922987448\n",
      "[ 0.30913675 -0.03105614]\n",
      "9.489448340235413\n",
      "[ 0.30913675 -0.03105614]\n",
      "Gradient Descent(466/999): loss=9.489448340235413, w0=-2.1839190342104806, w1=1.021058553694914\n",
      "[ 0.3085795  -0.03100016]\n",
      "9.487519480009945\n",
      "[ 0.3085795  -0.03100016]\n",
      "Gradient Descent(467/999): loss=9.487519480009945, w0=-2.1870048292173094, w1=1.0213685552744083\n",
      "[ 0.30802326 -0.03094428]\n",
      "9.485597567427702\n",
      "[ 0.30802326 -0.03094428]\n",
      "Gradient Descent(468/999): loss=9.485597567427702, w0=-2.1900850617829537, w1=1.0216779980463508\n",
      "[ 0.30746802 -0.0308885 ]\n",
      "9.483682577463663\n",
      "[ 0.30746802 -0.0308885 ]\n",
      "Gradient Descent(469/999): loss=9.483682577463663, w0=-2.1931597419342466, w1=1.0219868830180456\n",
      "[ 0.30691377 -0.03083282]\n",
      "9.481774485182966\n",
      "[ 0.30691377 -0.03083282]\n",
      "Gradient Descent(470/999): loss=9.481774485182966, w0=-2.1962288796799467, w1=1.0222952111949812\n",
      "[ 0.30636053 -0.03077724]\n",
      "9.479873265740547\n",
      "[ 0.30636053 -0.03077724]\n",
      "Gradient Descent(471/999): loss=9.479873265740547, w0=-2.1992924850107713, w1=1.0226029835808335\n",
      "[ 0.30580829 -0.03072176]\n",
      "9.477978894380845\n",
      "[ 0.30580829 -0.03072176]\n",
      "Gradient Descent(472/999): loss=9.477978894380845, w0=-2.2023505678994284, w1=1.0229102011774691\n",
      "[ 0.30525704 -0.03066638]\n",
      "9.476091346437457\n",
      "[ 0.30525704 -0.03066638]\n",
      "Gradient Descent(473/999): loss=9.476091346437457, w0=-2.205403138300649, w1=1.023216864984949\n",
      "[ 0.30470679 -0.0306111 ]\n",
      "9.474210597332831\n",
      "[ 0.30470679 -0.0306111 ]\n",
      "Gradient Descent(474/999): loss=9.474210597332831, w0=-2.2084502061512206, w1=1.0235229760015305\n",
      "[ 0.30415752 -0.03055592]\n",
      "9.472336622577943\n",
      "[ 0.30415752 -0.03055592]\n",
      "Gradient Descent(475/999): loss=9.472336622577943, w0=-2.2114917813700172, w1=1.0238285352236731\n",
      "[ 0.30360925 -0.03050084]\n",
      "9.470469397771978\n",
      "[ 0.30360925 -0.03050084]\n",
      "Gradient Descent(476/999): loss=9.470469397771978, w0=-2.2145278738580343, w1=1.0241335436460386\n",
      "[ 0.30306196 -0.03044586]\n",
      "9.46860889860201\n",
      "[ 0.30306196 -0.03044586]\n",
      "Gradient Descent(477/999): loss=9.46860889860201, w0=-2.2175584934984194, w1=1.0244380022614963\n",
      "[ 0.30251567 -0.03039098]\n",
      "9.466755100842684\n",
      "[ 0.30251567 -0.03039098]\n",
      "Gradient Descent(478/999): loss=9.466755100842684, w0=-2.220583650156505, w1=1.024741912061126\n",
      "[ 0.30197035 -0.0303362 ]\n",
      "9.464907980355903\n",
      "[ 0.30197035 -0.0303362 ]\n",
      "Gradient Descent(479/999): loss=9.464907980355903, w0=-2.22360335367984, w1=1.025045274034221\n",
      "[ 0.30142602 -0.03028151]\n",
      "9.463067513090522\n",
      "[ 0.30142602 -0.03028151]\n",
      "Gradient Descent(480/999): loss=9.463067513090522, w0=-2.226617613898222, w1=1.0253480891682911\n",
      "[ 0.30088267 -0.03022693]\n",
      "9.461233675082022\n",
      "[ 0.30088267 -0.03022693]\n",
      "Gradient Descent(481/999): loss=9.461233675082022, w0=-2.22962644062373, w1=1.0256503584490657\n",
      "[ 0.3003403  -0.03017244]\n",
      "9.459406442452195\n",
      "[ 0.3003403  -0.03017244]\n",
      "Gradient Descent(482/999): loss=9.459406442452195, w0=-2.2326298436507557, w1=1.025952082860498\n",
      "[ 0.29979891 -0.03011805]\n",
      "9.457585791408853\n",
      "[ 0.29979891 -0.03011805]\n",
      "Gradient Descent(483/999): loss=9.457585791408853, w0=-2.2356278327560353, w1=1.026253263384767\n",
      "[ 0.29925849 -0.03006376]\n",
      "9.455771698245501\n",
      "[ 0.29925849 -0.03006376]\n",
      "Gradient Descent(484/999): loss=9.455771698245501, w0=-2.238620417698681, w1=1.0265539010022813\n",
      "[ 0.29871905 -0.03000957]\n",
      "9.453964139341036\n",
      "[ 0.29871905 -0.03000957]\n",
      "Gradient Descent(485/999): loss=9.453964139341036, w0=-2.2416076082202143, w1=1.0268539966916828\n",
      "[ 0.29818058 -0.02995547]\n",
      "9.452163091159434\n",
      "[ 0.29818058 -0.02995547]\n",
      "Gradient Descent(486/999): loss=9.452163091159434, w0=-2.2445894140445963, w1=1.0271535514298484\n",
      "[ 0.29764308 -0.02990148]\n",
      "9.450368530249442\n",
      "[ 0.29764308 -0.02990148]\n",
      "Gradient Descent(487/999): loss=9.450368530249442, w0=-2.247565844878259, w1=1.027452566191895\n",
      "[ 0.29710655 -0.02984758]\n",
      "9.448580433244285\n",
      "[ 0.29710655 -0.02984758]\n",
      "Gradient Descent(488/999): loss=9.448580433244285, w0=-2.2505369104101387, w1=1.0277510419511808\n",
      "[ 0.29657099 -0.02979377]\n",
      "9.446798776861353\n",
      "[ 0.29657099 -0.02979377]\n",
      "Gradient Descent(489/999): loss=9.446798776861353, w0=-2.2535026203117057, w1=1.0280489796793102\n",
      "[ 0.29603639 -0.02974007]\n",
      "9.445023537901893\n",
      "[ 0.29603639 -0.02974007]\n",
      "Gradient Descent(490/999): loss=9.445023537901893, w0=-2.256462984236997, w1=1.028346380346136\n",
      "[ 0.29550276 -0.02968646]\n",
      "9.443254693250719\n",
      "[ 0.29550276 -0.02968646]\n",
      "Gradient Descent(491/999): loss=9.443254693250719, w0=-2.2594180118226475, w1=1.0286432449197622\n",
      "[ 0.29497009 -0.02963294]\n",
      "9.441492219875899\n",
      "[ 0.29497009 -0.02963294]\n",
      "Gradient Descent(492/999): loss=9.441492219875899, w0=-2.26236771268792, w1=1.0289395743665486\n",
      "[ 0.29443837 -0.02957953]\n",
      "9.439736094828463\n",
      "[ 0.29443837 -0.02957953]\n",
      "Gradient Descent(493/999): loss=9.439736094828463, w0=-2.2653120964347386, w1=1.0292353696511127\n",
      "[ 0.29390762 -0.02952621]\n",
      "9.437986295242107\n",
      "[ 0.29390762 -0.02952621]\n",
      "Gradient Descent(494/999): loss=9.437986295242107, w0=-2.268251172647719, w1=1.0295306317363326\n",
      "[ 0.29337782 -0.02947298]\n",
      "9.436242798332877\n",
      "[ 0.29337782 -0.02947298]\n",
      "Gradient Descent(495/999): loss=9.436242798332877, w0=-2.2711849508941993, w1=1.0298253615833517\n",
      "[ 0.29284898 -0.02941986]\n",
      "9.434505581398906\n",
      "[ 0.29284898 -0.02941986]\n",
      "Gradient Descent(496/999): loss=9.434505581398906, w0=-2.2741134407242716, w1=1.0301195601515802\n",
      "[ 0.29232109 -0.02936682]\n",
      "9.43277462182007\n",
      "[ 0.29232109 -0.02936682]\n",
      "Gradient Descent(497/999): loss=9.43277462182007, w0=-2.2770366516708136, w1=1.0304132283986989\n",
      "[ 0.29179416 -0.02931389]\n",
      "9.43104989705775\n",
      "[ 0.29179416 -0.02931389]\n",
      "Gradient Descent(498/999): loss=9.43104989705775, w0=-2.2799545932495184, w1=1.0307063672806624\n",
      "[ 0.29126817 -0.02926105]\n",
      "9.429331384654489\n",
      "[ 0.29126817 -0.02926105]\n",
      "Gradient Descent(499/999): loss=9.429331384654489, w0=-2.2828672749589267, w1=1.0309989777517024\n",
      "[ 0.29074313 -0.0292083 ]\n",
      "9.427619062233733\n",
      "[ 0.29074313 -0.0292083 ]\n",
      "Gradient Descent(500/999): loss=9.427619062233733, w0=-2.2857747062804568, w1=1.0312910607643297\n",
      "[ 0.29021904 -0.02915565]\n",
      "9.425912907499518\n",
      "[ 0.29021904 -0.02915565]\n",
      "Gradient Descent(501/999): loss=9.425912907499518, w0=-2.288676896678436, w1=1.0315826172693392\n",
      "[ 0.28969589 -0.02910309]\n",
      "9.424212898236197\n",
      "[ 0.28969589 -0.02910309]\n",
      "Gradient Descent(502/999): loss=9.424212898236197, w0=-2.2915738556001313, w1=1.031873648215811\n",
      "[ 0.28917369 -0.02905063]\n",
      "9.422519012308133\n",
      "[ 0.28917369 -0.02905063]\n",
      "Gradient Descent(503/999): loss=9.422519012308133, w0=-2.2944655924757797, w1=1.032164154551115\n",
      "[ 0.28865242 -0.02899827]\n",
      "9.420831227659432\n",
      "[ 0.28865242 -0.02899827]\n",
      "Gradient Descent(504/999): loss=9.420831227659432, w0=-2.2973521167186197, w1=1.0324541372209128\n",
      "[ 0.2881321  -0.02894599]\n",
      "9.419149522313633\n",
      "[ 0.2881321  -0.02894599]\n",
      "Gradient Descent(505/999): loss=9.419149522313633, w0=-2.3002334377249216, w1=1.0327435971691619\n",
      "[ 0.28761271 -0.02889382]\n",
      "9.417473874373442\n",
      "[ 0.28761271 -0.02889382]\n",
      "Gradient Descent(506/999): loss=9.417473874373442, w0=-2.303109564874018, w1=1.0330325353381176\n",
      "[ 0.28709427 -0.02884173]\n",
      "9.415804262020432\n",
      "[ 0.28709427 -0.02884173]\n",
      "Gradient Descent(507/999): loss=9.415804262020432, w0=-2.3059805075283335, w1=1.0333209526683376\n",
      "[ 0.28657675 -0.02878974]\n",
      "9.414140663514763\n",
      "[ 0.28657675 -0.02878974]\n",
      "Gradient Descent(508/999): loss=9.414140663514763, w0=-2.3088462750334173, w1=1.033608850098683\n",
      "[ 0.28606017 -0.02873785]\n",
      "9.412483057194908\n",
      "[ 0.28606017 -0.02873785]\n",
      "Gradient Descent(509/999): loss=9.412483057194908, w0=-2.3117068767179716, w1=1.0338962285663238\n",
      "[ 0.28554452 -0.02868604]\n",
      "9.41083142147736\n",
      "[ 0.28554452 -0.02868604]\n",
      "Gradient Descent(510/999): loss=9.41083142147736, w0=-2.3145623218938827, w1=1.0341830890067396\n",
      "[ 0.2850298  -0.02863433]\n",
      "9.409185734856349\n",
      "[ 0.2850298  -0.02863433]\n",
      "Gradient Descent(511/999): loss=9.409185734856349, w0=-2.3174126198562517, w1=1.0344694323537242\n",
      "[ 0.284516   -0.02858272]\n",
      "9.407545975903579\n",
      "[ 0.284516   -0.02858272]\n",
      "Gradient Descent(512/999): loss=9.407545975903579, w0=-2.3202577798834243, w1=1.034755259539388\n",
      "[ 0.28400314 -0.0285312 ]\n",
      "9.405912123267926\n",
      "[ 0.28400314 -0.0285312 ]\n",
      "Gradient Descent(513/999): loss=9.405912123267926, w0=-2.3230978112370213, w1=1.0350405714941615\n",
      "[ 0.28349119 -0.02847977]\n",
      "9.40428415567518\n",
      "[ 0.28349119 -0.02847977]\n",
      "Gradient Descent(514/999): loss=9.40428415567518, w0=-2.325932723161968, w1=1.0353253691467976\n",
      "[ 0.28298017 -0.02842843]\n",
      "9.402662051927754\n",
      "[ 0.28298017 -0.02842843]\n",
      "Gradient Descent(515/999): loss=9.402662051927754, w0=-2.3287625248865247, w1=1.0356096534243748\n",
      "[ 0.28247007 -0.02837718]\n",
      "9.401045790904412\n",
      "[ 0.28247007 -0.02837718]\n",
      "Gradient Descent(516/999): loss=9.401045790904412, w0=-2.3315872256223176, w1=1.0358934252523015\n",
      "[ 0.28196089 -0.02832603]\n",
      "9.399435351560008\n",
      "[ 0.28196089 -0.02832603]\n",
      "Gradient Descent(517/999): loss=9.399435351560008, w0=-2.334406834564368, w1=1.0361766855543169\n",
      "[ 0.28145263 -0.02827497]\n",
      "9.397830712925185\n",
      "[ 0.28145263 -0.02827497]\n",
      "Gradient Descent(518/999): loss=9.397830712925185, w0=-2.3372213608911214, w1=1.0364594352524954\n",
      "[ 0.28094529 -0.028224  ]\n",
      "9.396231854106123\n",
      "[ 0.28094529 -0.028224  ]\n",
      "Gradient Descent(519/999): loss=9.396231854106123, w0=-2.3400308137644794, w1=1.0367416752672496\n",
      "[ 0.28043886 -0.02817313]\n",
      "9.394638754284255\n",
      "[ 0.28043886 -0.02817313]\n",
      "Gradient Descent(520/999): loss=9.394638754284255, w0=-2.3428352023298276, w1=1.0370234065173325\n",
      "[ 0.27993334 -0.02812234]\n",
      "9.393051392716014\n",
      "[ 0.27993334 -0.02812234]\n",
      "Gradient Descent(521/999): loss=9.393051392716014, w0=-2.3456345357160666, w1=1.0373046299198414\n",
      "[ 0.27942873 -0.02807165]\n",
      "9.391469748732533\n",
      "[ 0.27942873 -0.02807165]\n",
      "Gradient Descent(522/999): loss=9.391469748732533, w0=-2.348428823035641, w1=1.03758534639022\n",
      "[ 0.27892503 -0.02802105]\n",
      "9.389893801739404\n",
      "[ 0.27892503 -0.02802105]\n",
      "Gradient Descent(523/999): loss=9.389893801739404, w0=-2.35121807338457, w1=1.0378655568422621\n",
      "[ 0.27842225 -0.02797053]\n",
      "9.388323531216402\n",
      "[ 0.27842225 -0.02797053]\n",
      "Gradient Descent(524/999): loss=9.388323531216402, w0=-2.354002295842475, w1=1.0381452621881142\n",
      "[ 0.27792036 -0.02792012]\n",
      "9.3867589167172\n",
      "[ 0.27792036 -0.02792012]\n",
      "Gradient Descent(525/999): loss=9.3867589167172, w0=-2.356781499472612, w1=1.0384244633382789\n",
      "[ 0.27741938 -0.02786979]\n",
      "9.385199937869135\n",
      "[ 0.27741938 -0.02786979]\n",
      "Gradient Descent(526/999): loss=9.385199937869135, w0=-2.359555693321899, w1=1.0387031612016169\n",
      "[ 0.27691931 -0.02781955]\n",
      "9.383646574372914\n",
      "[ 0.27691931 -0.02781955]\n",
      "Gradient Descent(527/999): loss=9.383646574372914, w0=-2.3623248864209456, w1=1.038981356685351\n",
      "[ 0.27642014 -0.0277694 ]\n",
      "9.382098806002364\n",
      "[ 0.27642014 -0.0277694 ]\n",
      "Gradient Descent(528/999): loss=9.382098806002364, w0=-2.3650890877840833, w1=1.0392590506950687\n",
      "[ 0.27592186 -0.02771934]\n",
      "9.38055661260416\n",
      "[ 0.27592186 -0.02771934]\n",
      "Gradient Descent(529/999): loss=9.38055661260416, w0=-2.3678483064093947, w1=1.039536244134725\n",
      "[ 0.27542449 -0.02766938]\n",
      "9.379019974097583\n",
      "[ 0.27542449 -0.02766938]\n",
      "Gradient Descent(530/999): loss=9.379019974097583, w0=-2.370602551278742, w1=1.039812937906645\n",
      "[ 0.27492801 -0.0276195 ]\n",
      "9.377488870474226\n",
      "[ 0.27492801 -0.0276195 ]\n",
      "Gradient Descent(531/999): loss=9.377488870474226, w0=-2.3733518313577973, w1=1.040089132911528\n",
      "[ 0.27443242 -0.02756971]\n",
      "9.37596328179776\n",
      "[ 0.27443242 -0.02756971]\n",
      "Gradient Descent(532/999): loss=9.37596328179776, w0=-2.3760961555960702, w1=1.0403648300484494\n",
      "[ 0.27393773 -0.02752002]\n",
      "9.374443188203669\n",
      "[ 0.27393773 -0.02752002]\n",
      "Gradient Descent(533/999): loss=9.374443188203669, w0=-2.378835532926939, w1=1.0406400302148637\n",
      "[ 0.27344393 -0.02747041]\n",
      "9.372928569898981\n",
      "[ 0.27344393 -0.02747041]\n",
      "Gradient Descent(534/999): loss=9.372928569898981, w0=-2.381569972267678, w1=1.0409147343066083\n",
      "[ 0.27295103 -0.02742089]\n",
      "9.371419407162016\n",
      "[ 0.27295103 -0.02742089]\n",
      "Gradient Descent(535/999): loss=9.371419407162016, w0=-2.3842994825194883, w1=1.041188943217905\n",
      "[ 0.272459   -0.02737146]\n",
      "9.369915680342139\n",
      "[ 0.272459   -0.02737146]\n",
      "Gradient Descent(536/999): loss=9.369915680342139, w0=-2.3870240725675242, w1=1.0414626578413637\n",
      "[ 0.27196787 -0.02732212]\n",
      "9.36841736985949\n",
      "[ 0.27196787 -0.02732212]\n",
      "Gradient Descent(537/999): loss=9.36841736985949, w0=-2.3897437512809248, w1=1.041735879067986\n",
      "[ 0.27147762 -0.02727287]\n",
      "9.366924456204734\n",
      "[ 0.27147762 -0.02727287]\n",
      "Gradient Descent(538/999): loss=9.366924456204734, w0=-2.392458527512841, w1=1.042008607787167\n",
      "[ 0.27098826 -0.02722371]\n",
      "9.365436919938803\n",
      "[ 0.27098826 -0.02722371]\n",
      "Gradient Descent(539/999): loss=9.365436919938803, w0=-2.395168410100466, w1=1.0422808448866987\n",
      "[ 0.27049978 -0.02717464]\n",
      "9.363954741692663\n",
      "[ 0.27049978 -0.02717464]\n",
      "Gradient Descent(540/999): loss=9.363954741692663, w0=-2.3978734078650623, w1=1.042552591252772\n",
      "[ 0.27001217 -0.02712565]\n",
      "9.362477902167027\n",
      "[ 0.27001217 -0.02712565]\n",
      "Gradient Descent(541/999): loss=9.362477902167027, w0=-2.4005735296119917, w1=1.0428238477699816\n",
      "[ 0.26952545 -0.02707676]\n",
      "9.361006382132134\n",
      "[ 0.26952545 -0.02707676]\n",
      "Gradient Descent(542/999): loss=9.361006382132134, w0=-2.4032687841307427, w1=1.0430946153213267\n",
      "[ 0.26903961 -0.02702795]\n",
      "9.35954016242749\n",
      "[ 0.26903961 -0.02702795]\n",
      "Gradient Descent(543/999): loss=9.35954016242749, w0=-2.405959180194961, w1=1.0433648947882153\n",
      "[ 0.26855464 -0.02697923]\n",
      "9.358079223961607\n",
      "[ 0.26855464 -0.02697923]\n",
      "Gradient Descent(544/999): loss=9.358079223961607, w0=-2.408644726562476, w1=1.043634687050466\n",
      "[ 0.26807054 -0.02693059]\n",
      "9.356623547711767\n",
      "[ 0.26807054 -0.02693059]\n",
      "Gradient Descent(545/999): loss=9.356623547711767, w0=-2.4113254319753312, w1=1.0439039929863123\n",
      "[ 0.26758732 -0.02688205]\n",
      "9.355173114723776\n",
      "[ 0.26758732 -0.02688205]\n",
      "Gradient Descent(546/999): loss=9.355173114723776, w0=-2.414001305159811, w1=1.0441728134724042\n",
      "[ 0.26710497 -0.02683359]\n",
      "9.353727906111704\n",
      "[ 0.26710497 -0.02683359]\n",
      "Gradient Descent(547/999): loss=9.353727906111704, w0=-2.41667235482647, w1=1.044441149383811\n",
      "[ 0.26662348 -0.02678522]\n",
      "9.352287903057649\n",
      "[ 0.26662348 -0.02678522]\n",
      "Gradient Descent(548/999): loss=9.352287903057649, w0=-2.419338589670162, w1=1.0447090015940252\n",
      "[ 0.26614287 -0.02673694]\n",
      "9.350853086811496\n",
      "[ 0.26614287 -0.02673694]\n",
      "Gradient Descent(549/999): loss=9.350853086811496, w0=-2.4220000183700656, w1=1.0449763709749644\n",
      "[ 0.26566312 -0.02668874]\n",
      "9.349423438690655\n",
      "[ 0.26566312 -0.02668874]\n",
      "Gradient Descent(550/999): loss=9.349423438690655, w0=-2.4246566495897164, w1=1.045243258396975\n",
      "[ 0.26518424 -0.02664063]\n",
      "9.347998940079847\n",
      "[ 0.26518424 -0.02664063]\n",
      "Gradient Descent(551/999): loss=9.347998940079847, w0=-2.4273084919770316, w1=1.045509664728834\n",
      "[ 0.26470622 -0.02659261]\n",
      "9.346579572430825\n",
      "[ 0.26470622 -0.02659261]\n",
      "Gradient Descent(552/999): loss=9.346579572430825, w0=-2.4299555541643407, w1=1.0457755908377522\n",
      "[ 0.26422906 -0.02654468]\n",
      "9.345165317262168\n",
      "[ 0.26422906 -0.02654468]\n",
      "Gradient Descent(553/999): loss=9.345165317262168, w0=-2.4325978447684125, w1=1.0460410375893778\n",
      "[ 0.26375276 -0.02649683]\n",
      "9.34375615615901\n",
      "[ 0.26375276 -0.02649683]\n",
      "Gradient Descent(554/999): loss=9.34375615615901, w0=-2.4352353723904825, w1=1.046306005847798\n",
      "[ 0.26327732 -0.02644906]\n",
      "9.342352070772826\n",
      "[ 0.26327732 -0.02644906]\n",
      "Gradient Descent(555/999): loss=9.342352070772826, w0=-2.4378681456162825, w1=1.046570496475543\n",
      "[ 0.26280274 -0.02640139]\n",
      "9.340953042821175\n",
      "[ 0.26280274 -0.02640139]\n",
      "Gradient Descent(556/999): loss=9.340953042821175, w0=-2.4404961730160672, w1=1.0468345103335874\n",
      "[ 0.26232901 -0.02635379]\n",
      "9.339559054087466\n",
      "[ 0.26232901 -0.02635379]\n",
      "Gradient Descent(557/999): loss=9.339559054087466, w0=-2.443119463144643, w1=1.0470980482813548\n",
      "[ 0.26185614 -0.02630629]\n",
      "9.33817008642073\n",
      "[ 0.26185614 -0.02630629]\n",
      "Gradient Descent(558/999): loss=9.33817008642073, w0=-2.4457380245413947, w1=1.0473611111767187\n",
      "[ 0.26138412 -0.02625887]\n",
      "9.336786121735377\n",
      "[ 0.26138412 -0.02625887]\n",
      "Gradient Descent(559/999): loss=9.336786121735377, w0=-2.4483518657303147, w1=1.0476236998760067\n",
      "[ 0.26091295 -0.02621154]\n",
      "9.335407142010954\n",
      "[ 0.26091295 -0.02621154]\n",
      "Gradient Descent(560/999): loss=9.335407142010954, w0=-2.45096099522003, w1=1.0478858152340025\n",
      "[ 0.26044263 -0.02616429]\n",
      "9.334033129291914\n",
      "[ 0.26044263 -0.02616429]\n",
      "Gradient Descent(561/999): loss=9.334033129291914, w0=-2.4535654215038303, w1=1.0481474581039494\n",
      "[ 0.25997316 -0.02611712]\n",
      "9.332664065687396\n",
      "[ 0.25997316 -0.02611712]\n",
      "Gradient Descent(562/999): loss=9.332664065687396, w0=-2.456165153059694, w1=1.0484086293375523\n",
      "[ 0.25950453 -0.02607004]\n",
      "9.331299933370978\n",
      "[ 0.25950453 -0.02607004]\n",
      "Gradient Descent(563/999): loss=9.331299933370978, w0=-2.458760198350319, w1=1.0486693297849807\n",
      "[ 0.25903675 -0.02602305]\n",
      "9.329940714580438\n",
      "[ 0.25903675 -0.02602305]\n",
      "Gradient Descent(564/999): loss=9.329940714580438, w0=-2.461350565823147, w1=1.048929560294872\n",
      "[ 0.25856981 -0.02597614]\n",
      "9.328586391617542\n",
      "[ 0.25856981 -0.02597614]\n",
      "Gradient Descent(565/999): loss=9.328586391617542, w0=-2.4639362639103926, w1=1.0491893217143338\n",
      "[ 0.25810371 -0.02592932]\n",
      "9.327236946847803\n",
      "[ 0.25810371 -0.02592932]\n",
      "Gradient Descent(566/999): loss=9.327236946847803, w0=-2.466517301029071, w1=1.0494486148889461\n",
      "[ 0.25763846 -0.02588258]\n",
      "9.325892362700248\n",
      "[ 0.25763846 -0.02588258]\n",
      "Gradient Descent(567/999): loss=9.325892362700248, w0=-2.469093685581025, w1=1.0497074406627656\n",
      "[ 0.25717404 -0.02583592]\n",
      "9.324552621667198\n",
      "[ 0.25717404 -0.02583592]\n",
      "Gradient Descent(568/999): loss=9.324552621667198, w0=-2.471665425952951, w1=1.0499657998783265\n",
      "[ 0.25671046 -0.02578935]\n",
      "9.323217706304035\n",
      "[ 0.25671046 -0.02578935]\n",
      "Gradient Descent(569/999): loss=9.323217706304035, w0=-2.474232530516429, w1=1.0502236933766451\n",
      "[ 0.25624771 -0.02574286]\n",
      "9.321887599228972\n",
      "[ 0.25624771 -0.02574286]\n",
      "Gradient Descent(570/999): loss=9.321887599228972, w0=-2.4767950076279486, w1=1.0504811219972212\n",
      "[ 0.2557858  -0.02569646]\n",
      "9.320562283122836\n",
      "[ 0.2557858  -0.02569646]\n",
      "Gradient Descent(571/999): loss=9.320562283122836, w0=-2.4793528656289343, w1=1.0507380865780418\n",
      "[ 0.25532472 -0.02565014]\n",
      "9.31924174072883\n",
      "[ 0.25532472 -0.02565014]\n",
      "Gradient Descent(572/999): loss=9.31924174072883, w0=-2.4819061128457762, w1=1.0509945879555826\n",
      "[ 0.25486447 -0.0256039 ]\n",
      "9.31792595485232\n",
      "[ 0.25486447 -0.0256039 ]\n",
      "Gradient Descent(573/999): loss=9.31792595485232, w0=-2.4844547575898543, w1=1.0512506269648123\n",
      "[ 0.25440506 -0.02555775]\n",
      "9.316614908360604\n",
      "[ 0.25440506 -0.02555775]\n",
      "Gradient Descent(574/999): loss=9.316614908360604, w0=-2.4869988081575665, w1=1.0515062044391936\n",
      "[ 0.25394647 -0.02551168]\n",
      "9.315308584182691\n",
      "[ 0.25394647 -0.02551168]\n",
      "Gradient Descent(575/999): loss=9.315308584182691, w0=-2.489538272830356, w1=1.0517613212106878\n",
      "[ 0.2534887  -0.02546569]\n",
      "9.314006965309085\n",
      "[ 0.2534887  -0.02546569]\n",
      "Gradient Descent(576/999): loss=9.314006965309085, w0=-2.4920731598747383, w1=1.0520159781097556\n",
      "[ 0.25303177 -0.02541979]\n",
      "9.312710034791543\n",
      "[ 0.25303177 -0.02541979]\n",
      "Gradient Descent(577/999): loss=9.312710034791543, w0=-2.4946034775423267, w1=1.0522701759653614\n",
      "[ 0.25257565 -0.02537396]\n",
      "9.31141777574288\n",
      "[ 0.25257565 -0.02537396]\n",
      "Gradient Descent(578/999): loss=9.31141777574288, w0=-2.497129234069861, w1=1.0525239156049748\n",
      "[ 0.25212036 -0.02532822]\n",
      "9.310130171336736\n",
      "[ 0.25212036 -0.02532822]\n",
      "Gradient Descent(579/999): loss=9.310130171336736, w0=-2.4996504376792332, w1=1.0527771978545741\n",
      "[ 0.25166589 -0.02528257]\n",
      "9.308847204807357\n",
      "[ 0.25166589 -0.02528257]\n",
      "Gradient Descent(580/999): loss=9.308847204807357, w0=-2.5021670965775145, w1=1.0530300235386487\n",
      "[ 0.25121224 -0.02523699]\n",
      "9.307568859449376\n",
      "[ 0.25121224 -0.02523699]\n",
      "Gradient Descent(581/999): loss=9.307568859449376, w0=-2.504679218956982, w1=1.0532823934802016\n",
      "[ 0.2507594 -0.0251915]\n",
      "9.306295118617603\n",
      "[ 0.2507594 -0.0251915]\n",
      "Gradient Descent(582/999): loss=9.306295118617603, w0=-2.507186812995146, w1=1.0535343085007525\n",
      "[ 0.25030739 -0.02514609]\n",
      "9.3050259657268\n",
      "[ 0.25030739 -0.02514609]\n",
      "Gradient Descent(583/999): loss=9.3050259657268, w0=-2.509689886854775, w1=1.05378576942034\n",
      "[ 0.24985618 -0.02510076]\n",
      "9.30376138425147\n",
      "[ 0.24985618 -0.02510076]\n",
      "Gradient Descent(584/999): loss=9.30376138425147, w0=-2.512188448683924, w1=1.0540367770575247\n",
      "[ 0.24940579 -0.02505552]\n",
      "9.302501357725633\n",
      "[ 0.24940579 -0.02505552]\n",
      "Gradient Descent(585/999): loss=9.302501357725633, w0=-2.514682506615961, w1=1.0542873322293915\n",
      "[ 0.24895622 -0.02501035]\n",
      "9.301245869742628\n",
      "[ 0.24895622 -0.02501035]\n",
      "Gradient Descent(586/999): loss=9.301245869742628, w0=-2.5171720687695913, w1=1.0545374357515525\n",
      "[ 0.24850745 -0.02496527]\n",
      "9.299994903954886\n",
      "[ 0.24850745 -0.02496527]\n",
      "Gradient Descent(587/999): loss=9.299994903954886, w0=-2.5196571432488866, w1=1.0547870884381496\n",
      "[ 0.24805949 -0.02492027]\n",
      "9.298748444073716\n",
      "[ 0.24805949 -0.02492027]\n",
      "Gradient Descent(588/999): loss=9.298748444073716, w0=-2.52213773814331, w1=1.055036291101857\n",
      "[ 0.24761234 -0.02487535]\n",
      "9.297506473869111\n",
      "[ 0.24761234 -0.02487535]\n",
      "Gradient Descent(589/999): loss=9.297506473869111, w0=-2.524613861527742, w1=1.055285044553884\n",
      "[ 0.24716599 -0.02483051]\n",
      "9.296268977169508\n",
      "[ 0.24716599 -0.02483051]\n",
      "Gradient Descent(590/999): loss=9.296268977169508, w0=-2.5270855214625083, w1=1.055533349603978\n",
      "[ 0.24672045 -0.02478575]\n",
      "9.295035937861602\n",
      "[ 0.24672045 -0.02478575]\n",
      "Gradient Descent(591/999): loss=9.295035937861602, w0=-2.5295527259934047, w1=1.0557812070604262\n",
      "[ 0.24627572 -0.02474107]\n",
      "9.293807339890124\n",
      "[ 0.24627572 -0.02474107]\n",
      "Gradient Descent(592/999): loss=9.293807339890124, w0=-2.5320154831517234, w1=1.0560286177300593\n",
      "[ 0.24583178 -0.02469647]\n",
      "9.29258316725764\n",
      "[ 0.24583178 -0.02469647]\n",
      "Gradient Descent(593/999): loss=9.29258316725764, w0=-2.5344738009542795, w1=1.056275582418253\n",
      "[ 0.24538864 -0.02465195]\n",
      "9.291363404024331\n",
      "[ 0.24538864 -0.02465195]\n",
      "Gradient Descent(594/999): loss=9.291363404024331, w0=-2.5369276874034377, w1=1.056522101928932\n",
      "[ 0.24494631 -0.02460751]\n",
      "9.290148034307798\n",
      "[ 0.24494631 -0.02460751]\n",
      "Gradient Descent(595/999): loss=9.290148034307798, w0=-2.5393771504871365, w1=1.056768177064571\n",
      "[ 0.24450477 -0.02456316]\n",
      "9.288937042282845\n",
      "[ 0.24450477 -0.02456316]\n",
      "Gradient Descent(596/999): loss=9.288937042282845, w0=-2.541822198178916, w1=1.0570138086261986\n",
      "[ 0.24406403 -0.02451888]\n",
      "9.287730412181281\n",
      "[ 0.24406403 -0.02451888]\n",
      "Gradient Descent(597/999): loss=9.287730412181281, w0=-2.5442628384379438, w1=1.0572589974134\n",
      "[ 0.24362408 -0.02447468]\n",
      "9.286528128291708\n",
      "[ 0.24362408 -0.02447468]\n",
      "Gradient Descent(598/999): loss=9.286528128291708, w0=-2.546699079209039, w1=1.0575037442243178\n",
      "[ 0.24318492 -0.02443056]\n",
      "9.285330174959324\n",
      "[ 0.24318492 -0.02443056]\n",
      "Gradient Descent(599/999): loss=9.285330174959324, w0=-2.549130928422701, w1=1.057748049855657\n",
      "[ 0.24274656 -0.02438652]\n",
      "9.284136536585713\n",
      "[ 0.24274656 -0.02438652]\n",
      "Gradient Descent(600/999): loss=9.284136536585713, w0=-2.5515583939951316, w1=1.0579919151026862\n",
      "[ 0.24230898 -0.02434257]\n",
      "9.282947197628637\n",
      "[ 0.24230898 -0.02434257]\n",
      "Gradient Descent(601/999): loss=9.282947197628637, w0=-2.5539814838282653, w1=1.0582353407592402\n",
      "[ 0.2418722  -0.02429869]\n",
      "9.281762142601849\n",
      "[ 0.2418722  -0.02429869]\n",
      "Gradient Descent(602/999): loss=9.281762142601849, w0=-2.556400205809791, w1=1.0584783276177228\n",
      "[ 0.2414362  -0.02425489]\n",
      "9.280581356074881\n",
      "[ 0.2414362  -0.02425489]\n",
      "Gradient Descent(603/999): loss=9.280581356074881, w0=-2.55881456781318, w1=1.0587208764691094\n",
      "[ 0.24100099 -0.02421116]\n",
      "9.27940482267284\n",
      "[ 0.24100099 -0.02421116]\n",
      "Gradient Descent(604/999): loss=9.27940482267284, w0=-2.5612245776977107, w1=1.0589629881029503\n",
      "[ 0.24056656 -0.02416752]\n",
      "9.278232527076213\n",
      "[ 0.24056656 -0.02416752]\n",
      "Gradient Descent(605/999): loss=9.278232527076213, w0=-2.563630243308494, w1=1.0592046633073717\n",
      "[ 0.24013292 -0.02412396]\n",
      "9.277064454020678\n",
      "[ 0.24013292 -0.02412396]\n",
      "Gradient Descent(606/999): loss=9.277064454020678, w0=-2.5660315724765, w1=1.0594459028690797\n",
      "[ 0.23970005 -0.02408047]\n",
      "9.275900588296878\n",
      "[ 0.23970005 -0.02408047]\n",
      "Gradient Descent(607/999): loss=9.275900588296878, w0=-2.5684285730185823, w1=1.0596867075733616\n",
      "[ 0.23926797 -0.02403706]\n",
      "9.274740914750248\n",
      "[ 0.23926797 -0.02403706]\n",
      "Gradient Descent(608/999): loss=9.274740914750248, w0=-2.570821252737504, w1=1.0599270782040897\n",
      "[ 0.23883667 -0.02399373]\n",
      "9.273585418280813\n",
      "[ 0.23883667 -0.02399373]\n",
      "Gradient Descent(609/999): loss=9.273585418280813, w0=-2.573209619421962, w1=1.0601670155437235\n",
      "[ 0.23840614 -0.02395048]\n",
      "9.272434083842976\n",
      "[ 0.23840614 -0.02395048]\n",
      "Gradient Descent(610/999): loss=9.272434083842976, w0=-2.575593680846615, w1=1.0604065203733115\n",
      "[ 0.23797639 -0.02390731]\n",
      "9.271286896445341\n",
      "[ 0.23797639 -0.02390731]\n",
      "Gradient Descent(611/999): loss=9.271286896445341, w0=-2.5779734447721063, w1=1.0606455934724945\n",
      "[ 0.23754742 -0.02386421]\n",
      "9.270143841150512\n",
      "[ 0.23754742 -0.02386421]\n",
      "Gradient Descent(612/999): loss=9.270143841150512, w0=-2.58034891894509, w1=1.0608842356195078\n",
      "[ 0.23711922 -0.0238212 ]\n",
      "9.269004903074887\n",
      "[ 0.23711922 -0.0238212 ]\n",
      "Gradient Descent(613/999): loss=9.269004903074887, w0=-2.582720111098256, w1=1.0611224475911842\n",
      "[ 0.23669179 -0.02377826]\n",
      "9.267870067388483\n",
      "[ 0.23669179 -0.02377826]\n",
      "Gradient Descent(614/999): loss=9.267870067388483, w0=-2.585087028950355, w1=1.0613602301629559\n",
      "[ 0.23626513 -0.02373539]\n",
      "9.266739319314736\n",
      "[ 0.23626513 -0.02373539]\n",
      "Gradient Descent(615/999): loss=9.266739319314736, w0=-2.5874496802062246, w1=1.0615975841088572\n",
      "[ 0.23583924 -0.02369261]\n",
      "9.265612644130291\n",
      "[ 0.23583924 -0.02369261]\n",
      "Gradient Descent(616/999): loss=9.265612644130291, w0=-2.589808072556813, w1=1.0618345102015276\n",
      "[ 0.23541411 -0.0236499 ]\n",
      "9.264490027164843\n",
      "[ 0.23541411 -0.0236499 ]\n",
      "Gradient Descent(617/999): loss=9.264490027164843, w0=-2.5921622136792055, w1=1.0620710092122134\n",
      "[ 0.23498976 -0.02360727]\n",
      "9.26337145380092\n",
      "[ 0.23498976 -0.02360727]\n",
      "Gradient Descent(618/999): loss=9.26337145380092, w0=-2.594512111236648, w1=1.0623070819107707\n",
      "[ 0.23456616 -0.02356472]\n",
      "9.2622569094737\n",
      "[ 0.23456616 -0.02356472]\n",
      "Gradient Descent(619/999): loss=9.2622569094737, w0=-2.5968577728785727, w1=1.0625427290656682\n",
      "[ 0.23414334 -0.02352224]\n",
      "9.26114637967083\n",
      "[ 0.23414334 -0.02352224]\n",
      "Gradient Descent(620/999): loss=9.26114637967083, w0=-2.5991992062406233, w1=1.0627779514439888\n",
      "[ 0.23372127 -0.02347984]\n",
      "9.260039849932221\n",
      "[ 0.23372127 -0.02347984]\n",
      "Gradient Descent(621/999): loss=9.260039849932221, w0=-2.6015364189446797, w1=1.0630127498114332\n",
      "[ 0.23329997 -0.02343751]\n",
      "9.258937305849873\n",
      "[ 0.23329997 -0.02343751]\n",
      "Gradient Descent(622/999): loss=9.258937305849873, w0=-2.603869418598882, w1=1.0632471249323217\n",
      "[ 0.23287942 -0.02339526]\n",
      "9.257838733067684\n",
      "[ 0.23287942 -0.02339526]\n",
      "Gradient Descent(623/999): loss=9.257838733067684, w0=-2.606198212797657, w1=1.0634810775695966\n",
      "[ 0.23245963 -0.02335309]\n",
      "9.25674411728126\n",
      "[ 0.23245963 -0.02335309]\n",
      "Gradient Descent(624/999): loss=9.25674411728126, w0=-2.6085228091217405, w1=1.063714608484825\n",
      "[ 0.2320406 -0.023311 ]\n",
      "9.255653444237728\n",
      "[ 0.2320406 -0.023311 ]\n",
      "Gradient Descent(625/999): loss=9.255653444237728, w0=-2.6108432151382037, w1=1.0639477184382018\n",
      "[ 0.23162233 -0.02326898]\n",
      "9.254566699735557\n",
      "[ 0.23162233 -0.02326898]\n",
      "Gradient Descent(626/999): loss=9.254566699735557, w0=-2.613159438400478, w1=1.0641804081885504\n",
      "[ 0.2312048  -0.02322703]\n",
      "9.253483869624368\n",
      "[ 0.2312048  -0.02322703]\n",
      "Gradient Descent(627/999): loss=9.253483869624368, w0=-2.615471486448379, w1=1.0644126784933277\n",
      "[ 0.23078804 -0.02318516]\n",
      "9.252404939804746\n",
      "[ 0.23078804 -0.02318516]\n",
      "Gradient Descent(628/999): loss=9.252404939804746, w0=-2.6177793668081297, w1=1.0646445301086238\n",
      "[ 0.23037202 -0.02314337]\n",
      "9.251329896228077\n",
      "[ 0.23037202 -0.02314337]\n",
      "Gradient Descent(629/999): loss=9.251329896228077, w0=-2.620083086992388, w1=1.0648759637891674\n",
      "[ 0.22995675 -0.02310165]\n",
      "9.250258724896327\n",
      "[ 0.22995675 -0.02310165]\n",
      "Gradient Descent(630/999): loss=9.250258724896327, w0=-2.6223826545002686, w1=1.0651069802883253\n",
      "[ 0.22954223 -0.02306001]\n",
      "9.249191411861899\n",
      "[ 0.22954223 -0.02306001]\n",
      "Gradient Descent(631/999): loss=9.249191411861899, w0=-2.6246780768173688, w1=1.0653375803581073\n",
      "[ 0.22912846 -0.02301844]\n",
      "9.248127943227429\n",
      "[ 0.22912846 -0.02301844]\n",
      "Gradient Descent(632/999): loss=9.248127943227429, w0=-2.626969361415792, w1=1.065567764749167\n",
      "[ 0.22871543 -0.02297695]\n",
      "9.247068305145605\n",
      "[ 0.22871543 -0.02297695]\n",
      "Gradient Descent(633/999): loss=9.247068305145605, w0=-2.6292565157541725, w1=1.0657975342108053\n",
      "[ 0.22830315 -0.02293553]\n",
      "9.246012483819005\n",
      "[ 0.22830315 -0.02293553]\n",
      "Gradient Descent(634/999): loss=9.246012483819005, w0=-2.6315395472777, w1=1.0660268894909721\n",
      "[ 0.22789161 -0.02289418]\n",
      "9.24496046549989\n",
      "[ 0.22789161 -0.02289418]\n",
      "Gradient Descent(635/999): loss=9.24496046549989, w0=-2.6338184634181436, w1=1.0662558313362696\n",
      "[ 0.22748082 -0.02285292]\n",
      "9.24391223649005\n",
      "[ 0.22748082 -0.02285292]\n",
      "Gradient Descent(636/999): loss=9.24391223649005, w0=-2.636093271593875, w1=1.0664843604919532\n",
      "[ 0.22707076 -0.02281172]\n",
      "9.242867783140612\n",
      "[ 0.22707076 -0.02281172]\n",
      "Gradient Descent(637/999): loss=9.242867783140612, w0=-2.638363979209895, w1=1.0667124777019357\n",
      "[ 0.22666144 -0.0227706 ]\n",
      "9.241827091851865\n",
      "[ 0.22666144 -0.0227706 ]\n",
      "Gradient Descent(638/999): loss=9.241827091851865, w0=-2.6406305936578547, w1=1.0669401837087886\n",
      "[ 0.22625287 -0.02272955]\n",
      "9.24079014907308\n",
      "[ 0.22625287 -0.02272955]\n",
      "Gradient Descent(639/999): loss=9.24079014907308, w0=-2.642893122316082, w1=1.0671674792537453\n",
      "[ 0.22584502 -0.02268858]\n",
      "9.239756941302346\n",
      "[ 0.22584502 -0.02268858]\n",
      "Gradient Descent(640/999): loss=9.239756941302346, w0=-2.6451515725496044, w1=1.0673943650767024\n",
      "[ 0.22543792 -0.02264768]\n",
      "9.238727455086375\n",
      "[ 0.22543792 -0.02264768]\n",
      "Gradient Descent(641/999): loss=9.238727455086375, w0=-2.6474059517101733, w1=1.067620841916223\n",
      "[ 0.22503154 -0.02260686]\n",
      "9.237701677020345\n",
      "[ 0.22503154 -0.02260686]\n",
      "Gradient Descent(642/999): loss=9.237701677020345, w0=-2.6496562671362875, w1=1.0678469105095392\n",
      "[ 0.2246259  -0.02256611]\n",
      "9.23667959374771\n",
      "[ 0.2246259  -0.02256611]\n",
      "Gradient Descent(643/999): loss=9.23667959374771, w0=-2.651902526153218, w1=1.0680725715925536\n",
      "[ 0.22422099 -0.02252543]\n",
      "9.235661191960041\n",
      "[ 0.22422099 -0.02252543]\n",
      "Gradient Descent(644/999): loss=9.235661191960041, w0=-2.654144736073031, w1=1.068297825899843\n",
      "[ 0.22381681 -0.02248483]\n",
      "9.234646458396838\n",
      "[ 0.22381681 -0.02248483]\n",
      "Gradient Descent(645/999): loss=9.234646458396838, w0=-2.6563829041946123, w1=1.0685226741646592\n",
      "[ 0.22341336 -0.0224443 ]\n",
      "9.233635379845369\n",
      "[ 0.22341336 -0.0224443 ]\n",
      "Gradient Descent(646/999): loss=9.233635379845369, w0=-2.65861703780369, w1=1.0687471171189327\n",
      "[ 0.22301064 -0.02240384]\n",
      "9.2326279431405\n",
      "[ 0.22301064 -0.02240384]\n",
      "Gradient Descent(647/999): loss=9.2326279431405, w0=-2.66084714417286, w1=1.0689711554932748\n",
      "[ 0.22260864 -0.02236345]\n",
      "9.2316241351645\n",
      "[ 0.22260864 -0.02236345]\n",
      "Gradient Descent(648/999): loss=9.2316241351645, w0=-2.6630732305616074, w1=1.0691947900169796\n",
      "[ 0.22220737 -0.02232314]\n",
      "9.230623942846902\n",
      "[ 0.22220737 -0.02232314]\n",
      "Gradient Descent(649/999): loss=9.230623942846902, w0=-2.665295304216333, w1=1.0694180214180262\n",
      "[ 0.22180682 -0.0222829 ]\n",
      "9.229627353164311\n",
      "[ 0.22180682 -0.0222829 ]\n",
      "Gradient Descent(650/999): loss=9.229627353164311, w0=-2.6675133723703737, w1=1.0696408504230823\n",
      "[ 0.22140699 -0.02224273]\n",
      "9.228634353140244\n",
      "[ 0.22140699 -0.02224273]\n",
      "Gradient Descent(651/999): loss=9.228634353140244, w0=-2.6697274422440285, w1=1.069863277757505\n",
      "[ 0.22100788 -0.02220264]\n",
      "9.227644929844958\n",
      "[ 0.22100788 -0.02220264]\n",
      "Gradient Descent(652/999): loss=9.227644929844958, w0=-2.671937521044581, w1=1.070085304145344\n",
      "[ 0.22060949 -0.02216262]\n",
      "9.226659070395284\n",
      "[ 0.22060949 -0.02216262]\n",
      "Gradient Descent(653/999): loss=9.226659070395284, w0=-2.674143615966323, w1=1.0703069303093442\n",
      "[ 0.22021182 -0.02212267]\n",
      "9.225676761954453\n",
      "[ 0.22021182 -0.02212267]\n",
      "Gradient Descent(654/999): loss=9.225676761954453, w0=-2.676345734190578, w1=1.0705281569709473\n",
      "[ 0.21981487 -0.02208279]\n",
      "9.224697991731942\n",
      "[ 0.21981487 -0.02208279]\n",
      "Gradient Descent(655/999): loss=9.224697991731942, w0=-2.6785438828857235, w1=1.0707489848502945\n",
      "[ 0.21941863 -0.02204298]\n",
      "9.223722746983285\n",
      "[ 0.21941863 -0.02204298]\n",
      "Gradient Descent(656/999): loss=9.223722746983285, w0=-2.6807380692072167, w1=1.0709694146662292\n",
      "[ 0.21902311 -0.02200325]\n",
      "9.222751015009933\n",
      "[ 0.21902311 -0.02200325]\n",
      "Gradient Descent(657/999): loss=9.222751015009933, w0=-2.6829283002976156, w1=1.0711894471362988\n",
      "[ 0.2186283  -0.02196358]\n",
      "9.221782783159071\n",
      "[ 0.2186283  -0.02196358]\n",
      "Gradient Descent(658/999): loss=9.221782783159071, w0=-2.6851145832866035, w1=1.0714090829767575\n",
      "[ 0.2182342  -0.02192399]\n",
      "9.220818038823461\n",
      "[ 0.2182342  -0.02192399]\n",
      "Gradient Descent(659/999): loss=9.220818038823461, w0=-2.687296925291011, w1=1.0716283229025678\n",
      "[ 0.21784081 -0.02188447]\n",
      "9.219856769441272\n",
      "[ 0.21784081 -0.02188447]\n",
      "Gradient Descent(660/999): loss=9.219856769441272, w0=-2.6894753334148405, w1=1.071847167627404\n",
      "[ 0.21744813 -0.02184502]\n",
      "9.218898962495926\n",
      "[ 0.21744813 -0.02184502]\n",
      "Gradient Descent(661/999): loss=9.218898962495926, w0=-2.691649814749289, w1=1.0720656178636536\n",
      "[ 0.21705616 -0.02180565]\n",
      "9.21794460551592\n",
      "[ 0.21705616 -0.02180565]\n",
      "Gradient Descent(662/999): loss=9.21794460551592, w0=-2.6938203763727704, w1=1.0722836743224204\n",
      "[ 0.2166649  -0.02176634]\n",
      "9.216993686074682\n",
      "[ 0.2166649  -0.02176634]\n",
      "Gradient Descent(663/999): loss=9.216993686074682, w0=-2.6959870253509397, w1=1.0725013377135257\n",
      "[ 0.21627434 -0.0217271 ]\n",
      "9.216046191790394\n",
      "[ 0.21627434 -0.0217271 ]\n",
      "Gradient Descent(664/999): loss=9.216046191790394, w0=-2.6981497687367146, w1=1.0727186087455116\n",
      "[ 0.21588448 -0.02168794]\n",
      "9.21510211032584\n",
      "[ 0.21588448 -0.02168794]\n",
      "Gradient Descent(665/999): loss=9.21510211032584, w0=-2.7003086135702996, w1=1.0729354881256434\n",
      "[ 0.21549533 -0.02164884]\n",
      "9.21416142938824\n",
      "[ 0.21549533 -0.02164884]\n",
      "Gradient Descent(666/999): loss=9.21416142938824, w0=-2.7024635668792087, w1=1.0731519765599107\n",
      "[ 0.21510688 -0.02160982]\n",
      "9.213224136729092\n",
      "[ 0.21510688 -0.02160982]\n",
      "Gradient Descent(667/999): loss=9.213224136729092, w0=-2.7046146356782885, w1=1.073368074753031\n",
      "[ 0.21471913 -0.02157087]\n",
      "9.212290220144007\n",
      "[ 0.21471913 -0.02157087]\n",
      "Gradient Descent(668/999): loss=9.212290220144007, w0=-2.7067618269697395, w1=1.0735837834084514\n",
      "[ 0.21433208 -0.02153198]\n",
      "9.211359667472571\n",
      "[ 0.21433208 -0.02153198]\n",
      "Gradient Descent(669/999): loss=9.211359667472571, w0=-2.708905147743141, w1=1.0737991032283511\n",
      "[ 0.21394572 -0.02149317]\n",
      "9.210432466598155\n",
      "[ 0.21394572 -0.02149317]\n",
      "Gradient Descent(670/999): loss=9.210432466598155, w0=-2.7110446049754726, w1=1.0740140349136431\n",
      "[ 0.21356007 -0.02145443]\n",
      "9.209508605447779\n",
      "[ 0.21356007 -0.02145443]\n",
      "Gradient Descent(671/999): loss=9.209508605447779, w0=-2.7131802056311374, w1=1.0742285791639772\n",
      "[ 0.2131751  -0.02141575]\n",
      "9.208588071991954\n",
      "[ 0.2131751  -0.02141575]\n",
      "Gradient Descent(672/999): loss=9.208588071991954, w0=-2.715311956661984, w1=1.0744427366777423\n",
      "[ 0.21279083 -0.02137715]\n",
      "9.207670854244512\n",
      "[ 0.21279083 -0.02137715]\n",
      "Gradient Descent(673/999): loss=9.207670854244512, w0=-2.717439865007331, w1=1.0746565081520678\n",
      "[ 0.21240726 -0.02133861]\n",
      "9.206756940262466\n",
      "[ 0.21240726 -0.02133861]\n",
      "Gradient Descent(674/999): loss=9.206756940262466, w0=-2.719563937593986, w1=1.0748698942828272\n",
      "[ 0.21202437 -0.02130015]\n",
      "9.20584631814584\n",
      "[ 0.21202437 -0.02130015]\n",
      "Gradient Descent(675/999): loss=9.20584631814584, w0=-2.7216841813362724, w1=1.0750828957646388\n",
      "[ 0.21164218 -0.02126175]\n",
      "9.204938976037528\n",
      "[ 0.21164218 -0.02126175]\n",
      "Gradient Descent(676/999): loss=9.204938976037528, w0=-2.723800603136049, w1=1.0752955132908695\n",
      "[ 0.21126067 -0.02122343]\n",
      "9.204034902123126\n",
      "[ 0.21126067 -0.02122343]\n",
      "Gradient Descent(677/999): loss=9.204034902123126, w0=-2.725913209882733, w1=1.0755077475536359\n",
      "[ 0.21087986 -0.02118517]\n",
      "9.203134084630797\n",
      "[ 0.21087986 -0.02118517]\n",
      "Gradient Descent(678/999): loss=9.203134084630797, w0=-2.728022008453323, w1=1.0757195992438071\n",
      "[ 0.21049973 -0.02114698]\n",
      "9.202236511831083\n",
      "[ 0.21049973 -0.02114698]\n",
      "Gradient Descent(679/999): loss=9.202236511831083, w0=-2.730127005712422, w1=1.0759310690510067\n",
      "[ 0.21012028 -0.02110886]\n",
      "9.2013421720368\n",
      "[ 0.21012028 -0.02110886]\n",
      "Gradient Descent(680/999): loss=9.2013421720368, w0=-2.732228208512258, w1=1.0761421576636157\n",
      "[ 0.20974152 -0.02107081]\n",
      "9.20045105360284\n",
      "[ 0.20974152 -0.02107081]\n",
      "Gradient Descent(681/999): loss=9.20045105360284, w0=-2.734325623692707, w1=1.0763528657687735\n",
      "[ 0.20936344 -0.02103283]\n",
      "9.19956314492606\n",
      "[ 0.20936344 -0.02103283]\n",
      "Gradient Descent(682/999): loss=9.19956314492606, w0=-2.7364192580813165, w1=1.0765631940523814\n",
      "[ 0.20898604 -0.02099491]\n",
      "9.198678434445087\n",
      "[ 0.20898604 -0.02099491]\n",
      "Gradient Descent(683/999): loss=9.198678434445087, w0=-2.7385091184933255, w1=1.0767731431991043\n",
      "[ 0.20860932 -0.02095707]\n",
      "9.197796910640216\n",
      "[ 0.20860932 -0.02095707]\n",
      "Gradient Descent(684/999): loss=9.197796910640216, w0=-2.740595211731689, w1=1.0769827138923727\n",
      "[ 0.20823329 -0.02091929]\n",
      "9.196918562033215\n",
      "[ 0.20823329 -0.02091929]\n",
      "Gradient Descent(685/999): loss=9.196918562033215, w0=-2.742677544587098, w1=1.077191906814385\n",
      "[ 0.20785793 -0.02088158]\n",
      "9.196043377187207\n",
      "[ 0.20785793 -0.02088158]\n",
      "Gradient Descent(686/999): loss=9.196043377187207, w0=-2.7447561238380036, w1=1.0774007226461102\n",
      "[ 0.20748324 -0.02084394]\n",
      "9.195171344706509\n",
      "[ 0.20748324 -0.02084394]\n",
      "Gradient Descent(687/999): loss=9.195171344706509, w0=-2.746830956250637, w1=1.0776091620672898\n",
      "[ 0.20710923 -0.02080637]\n",
      "9.194302453236482\n",
      "[ 0.20710923 -0.02080637]\n",
      "Gradient Descent(688/999): loss=9.194302453236482, w0=-2.7489020485790334, w1=1.0778172257564398\n",
      "[ 0.2067359  -0.02076886]\n",
      "9.193436691463388\n",
      "[ 0.2067359  -0.02076886]\n",
      "Gradient Descent(689/999): loss=9.193436691463388, w0=-2.7509694075650533, w1=1.0780249143908531\n",
      "[ 0.20636324 -0.02073143]\n",
      "9.192574048114237\n",
      "[ 0.20636324 -0.02073143]\n",
      "Gradient Descent(690/999): loss=9.192574048114237, w0=-2.7530330399384035, w1=1.078232228646602\n",
      "[ 0.20599125 -0.02069406]\n",
      "9.191714511956652\n",
      "[ 0.20599125 -0.02069406]\n",
      "Gradient Descent(691/999): loss=9.191714511956652, w0=-2.755092952416661, w1=1.0784391691985398\n",
      "[ 0.20561993 -0.02065675]\n",
      "9.190858071798697\n",
      "[ 0.20561993 -0.02065675]\n",
      "Gradient Descent(692/999): loss=9.190858071798697, w0=-2.757149151705293, w1=1.0786457367203033\n",
      "[ 0.20524928 -0.02061952]\n",
      "9.19000471648877\n",
      "[ 0.20524928 -0.02061952]\n",
      "Gradient Descent(693/999): loss=9.19000471648877, w0=-2.7592016444976792, w1=1.0788519318843153\n",
      "[ 0.2048793  -0.02058235]\n",
      "9.189154434915423\n",
      "[ 0.2048793  -0.02058235]\n",
      "Gradient Descent(694/999): loss=9.189154434915423, w0=-2.7612504374751348, w1=1.0790577553617862\n",
      "[ 0.20450998 -0.02054525]\n",
      "9.188307216007233\n",
      "[ 0.20450998 -0.02054525]\n",
      "Gradient Descent(695/999): loss=9.188307216007233, w0=-2.7632955373069303, w1=1.0792632078227167\n",
      "[ 0.20414133 -0.02050821]\n",
      "9.18746304873266\n",
      "[ 0.20414133 -0.02050821]\n",
      "Gradient Descent(696/999): loss=9.18746304873266, w0=-2.7653369506503154, w1=1.0794682899358998\n",
      "[ 0.20377335 -0.02047124]\n",
      "9.186621922099892\n",
      "[ 0.20377335 -0.02047124]\n",
      "Gradient Descent(697/999): loss=9.186621922099892, w0=-2.767374684150538, w1=1.0796730023689225\n",
      "[ 0.20340603 -0.02043434]\n",
      "9.185783825156715\n",
      "[ 0.20340603 -0.02043434]\n",
      "Gradient Descent(698/999): loss=9.185783825156715, w0=-2.769408744440868, w1=1.079877345788169\n",
      "[ 0.20303937 -0.02039751]\n",
      "9.184948746990363\n",
      "[ 0.20303937 -0.02039751]\n",
      "Gradient Descent(699/999): loss=9.184948746990363, w0=-2.771439138142618, w1=1.0800813208588216\n",
      "[ 0.20267337 -0.02036074]\n",
      "9.184116676727372\n",
      "[ 0.20267337 -0.02036074]\n",
      "Gradient Descent(700/999): loss=9.184116676727372, w0=-2.773465871865166, w1=1.0802849282448643\n",
      "[ 0.20230803 -0.02032404]\n",
      "9.183287603533453\n",
      "[ 0.20230803 -0.02032404]\n",
      "Gradient Descent(701/999): loss=9.183287603533453, w0=-2.775488952205975, w1=1.080488168609084\n",
      "[ 0.20194335 -0.0202874 ]\n",
      "9.182461516613332\n",
      "[ 0.20194335 -0.0202874 ]\n",
      "Gradient Descent(702/999): loss=9.182461516613332, w0=-2.7775083857506155, w1=1.0806910426130723\n",
      "[ 0.20157933 -0.02025083]\n",
      "9.181638405210624\n",
      "[ 0.20157933 -0.02025083]\n",
      "Gradient Descent(703/999): loss=9.181638405210624, w0=-2.779524179072787, w1=1.0808935509172286\n",
      "[ 0.20121597 -0.02021433]\n",
      "9.180818258607689\n",
      "[ 0.20121597 -0.02021433]\n",
      "Gradient Descent(704/999): loss=9.180818258607689, w0=-2.7815363387343393, w1=1.0810956941807621\n",
      "[ 0.20085326 -0.02017789]\n",
      "9.180001066125488\n",
      "[ 0.20085326 -0.02017789]\n",
      "Gradient Descent(705/999): loss=9.180001066125488, w0=-2.7835448712852937, w1=1.0812974730616935\n",
      "[ 0.2004912  -0.02014152]\n",
      "9.179186817123444\n",
      "[ 0.2004912  -0.02014152]\n",
      "Gradient Descent(706/999): loss=9.179186817123444, w0=-2.785549783263865, w1=1.0814988882168572\n",
      "[ 0.20012979 -0.02010521]\n",
      "9.178375500999323\n",
      "[ 0.20012979 -0.02010521]\n",
      "Gradient Descent(707/999): loss=9.178375500999323, w0=-2.7875510811964817, w1=1.0816999403019039\n",
      "[ 0.19976904 -0.02006897]\n",
      "9.177567107189056\n",
      "[ 0.19976904 -0.02006897]\n",
      "Gradient Descent(708/999): loss=9.177567107189056, w0=-2.789548771597808, w1=1.0819006299713023\n",
      "[ 0.19940894 -0.02003279]\n",
      "9.176761625166641\n",
      "[ 0.19940894 -0.02003279]\n",
      "Gradient Descent(709/999): loss=9.176761625166641, w0=-2.791542860970764, w1=1.0821009578783414\n",
      "[ 0.19904948 -0.01999668]\n",
      "9.175959044443992\n",
      "[ 0.19904948 -0.01999668]\n",
      "Gradient Descent(710/999): loss=9.175959044443992, w0=-2.7935333558065496, w1=1.082300924675132\n",
      "[ 0.19869068 -0.01996063]\n",
      "9.175159354570788\n",
      "[ 0.19869068 -0.01996063]\n",
      "Gradient Descent(711/999): loss=9.175159354570788, w0=-2.795520262584662, w1=1.0825005310126106\n",
      "[ 0.19833252 -0.01992465]\n",
      "9.174362545134356\n",
      "[ 0.19833252 -0.01992465]\n",
      "Gradient Descent(712/999): loss=9.174362545134356, w0=-2.797503587772918, w1=1.0826997775405394\n",
      "[ 0.19797501 -0.01988874]\n",
      "9.17356860575953\n",
      "[ 0.19797501 -0.01988874]\n",
      "Gradient Descent(713/999): loss=9.17356860575953, w0=-2.799483337827478, w1=1.0828986649075094\n",
      "[ 0.19761814 -0.01985289]\n",
      "9.172777526108518\n",
      "[ 0.19761814 -0.01985289]\n",
      "Gradient Descent(714/999): loss=9.172777526108518, w0=-2.8014595191928624, w1=1.0830971937609428\n",
      "[ 0.19726191 -0.0198171 ]\n",
      "9.171989295880756\n",
      "[ 0.19726191 -0.0198171 ]\n",
      "Gradient Descent(715/999): loss=9.171989295880756, w0=-2.803432138301975, w1=1.0832953647470946\n",
      "[ 0.19690633 -0.01978138]\n",
      "9.171203904812783\n",
      "[ 0.19690633 -0.01978138]\n",
      "Gradient Descent(716/999): loss=9.171203904812783, w0=-2.805401201576125, w1=1.0834931785110549\n",
      "[ 0.19655138 -0.01974572]\n",
      "9.170421342678114\n",
      "[ 0.19655138 -0.01974572]\n",
      "Gradient Descent(717/999): loss=9.170421342678114, w0=-2.807366715425045, w1=1.0836906356967508\n",
      "[ 0.19619708 -0.01971013]\n",
      "9.169641599287095\n",
      "[ 0.19619708 -0.01971013]\n",
      "Gradient Descent(718/999): loss=9.169641599287095, w0=-2.8093286862469142, w1=1.083887736946949\n",
      "[ 0.19584342 -0.0196746 ]\n",
      "9.168864664486769\n",
      "[ 0.19584342 -0.0196746 ]\n",
      "Gradient Descent(719/999): loss=9.168864664486769, w0=-2.8112871204283785, w1=1.0840844829032572\n",
      "[ 0.19549039 -0.01963913]\n",
      "9.168090528160763\n",
      "[ 0.19549039 -0.01963913]\n",
      "Gradient Descent(720/999): loss=9.168090528160763, w0=-2.8132420243445706, w1=1.0842808742061265\n",
      "[ 0.195138   -0.01960373]\n",
      "9.167319180229123\n",
      "[ 0.195138   -0.01960373]\n",
      "Gradient Descent(721/999): loss=9.167319180229123, w0=-2.8151934043591327, w1=1.084476911494854\n",
      "[ 0.19478625 -0.01956839]\n",
      "9.16655061064822\n",
      "[ 0.19478625 -0.01956839]\n",
      "Gradient Descent(722/999): loss=9.16655061064822, w0=-2.8171412668242346, w1=1.0846725954075842\n",
      "[ 0.19443513 -0.01953312]\n",
      "9.165784809410596\n",
      "[ 0.19443513 -0.01953312]\n",
      "Gradient Descent(723/999): loss=9.165784809410596, w0=-2.8190856180805963, w1=1.084867926581311\n",
      "[ 0.19408464 -0.01949791]\n",
      "9.165021766544838\n",
      "[ 0.19408464 -0.01949791]\n",
      "Gradient Descent(724/999): loss=9.165021766544838, w0=-2.8210264644575083, w1=1.0850629056518801\n",
      "[ 0.19373478 -0.01946276]\n",
      "9.164261472115443\n",
      "[ 0.19373478 -0.01946276]\n",
      "Gradient Descent(725/999): loss=9.164261472115443, w0=-2.8229638122728513, w1=1.0852575332539913\n",
      "[ 0.19338556 -0.01942768]\n",
      "9.163503916222709\n",
      "[ 0.19338556 -0.01942768]\n",
      "Gradient Descent(726/999): loss=9.163503916222709, w0=-2.824897667833118, w1=1.0854518100212005\n",
      "[ 0.19303696 -0.01939266]\n",
      "9.162749089002585\n",
      "[ 0.19303696 -0.01939266]\n",
      "Gradient Descent(727/999): loss=9.162749089002585, w0=-2.826828037433433, w1=1.0856457365859207\n",
      "[ 0.19268899 -0.0193577 ]\n",
      "9.161996980626544\n",
      "[ 0.19268899 -0.0193577 ]\n",
      "Gradient Descent(728/999): loss=9.161996980626544, w0=-2.828754927357573, w1=1.0858393135794258\n",
      "[ 0.19234165 -0.01932281]\n",
      "9.161247581301472\n",
      "[ 0.19234165 -0.01932281]\n",
      "Gradient Descent(729/999): loss=9.161247581301472, w0=-2.8306783438779872, w1=1.0860325416318513\n",
      "[ 0.19199494 -0.01928797]\n",
      "9.160500881269522\n",
      "[ 0.19199494 -0.01928797]\n",
      "Gradient Descent(730/999): loss=9.160500881269522, w0=-2.832598293255819, w1=1.086225421372197\n",
      "[ 0.19164885 -0.01925321]\n",
      "9.159756870807994\n",
      "[ 0.19164885 -0.01925321]\n",
      "Gradient Descent(731/999): loss=9.159756870807994, w0=-2.8345147817409257, w1=1.0864179534283287\n",
      "[ 0.19130338 -0.0192185 ]\n",
      "9.159015540229214\n",
      "[ 0.19130338 -0.0192185 ]\n",
      "Gradient Descent(732/999): loss=9.159015540229214, w0=-2.836427815571897, w1=1.0866101384269804\n",
      "[ 0.19095854 -0.01918386]\n",
      "9.158276879880395\n",
      "[ 0.19095854 -0.01918386]\n",
      "Gradient Descent(733/999): loss=9.158276879880395, w0=-2.838337400976079, w1=1.0868019769937567\n",
      "[ 0.19061432 -0.01914928]\n",
      "9.157540880143529\n",
      "[ 0.19061432 -0.01914928]\n",
      "Gradient Descent(734/999): loss=9.157540880143529, w0=-2.840243544169591, w1=1.0869934697531343\n",
      "[ 0.19027072 -0.01911476]\n",
      "9.156807531435236\n",
      "[ 0.19027072 -0.01911476]\n",
      "Gradient Descent(735/999): loss=9.156807531435236, w0=-2.8421462513573474, w1=1.0871846173284638\n",
      "[ 0.18992774 -0.0190803 ]\n",
      "9.156076824206673\n",
      "[ 0.18992774 -0.0190803 ]\n",
      "Gradient Descent(736/999): loss=9.156076824206673, w0=-2.844045528733078, w1=1.087375420341973\n",
      "[ 0.18958537 -0.01904591]\n",
      "9.15534874894338\n",
      "[ 0.18958537 -0.01904591]\n",
      "Gradient Descent(737/999): loss=9.15534874894338, w0=-2.8459413824793476, w1=1.0875658794147673\n",
      "[ 0.18924363 -0.01901158]\n",
      "9.154623296165166\n",
      "[ 0.18924363 -0.01901158]\n",
      "Gradient Descent(738/999): loss=9.154623296165166, w0=-2.8478338187675765, w1=1.0877559951668327\n",
      "[ 0.1889025  -0.01897731]\n",
      "9.153900456425994\n",
      "[ 0.1889025  -0.01897731]\n",
      "Gradient Descent(739/999): loss=9.153900456425994, w0=-2.84972284375806, w1=1.087945768217038\n",
      "[ 0.18856198 -0.0189431 ]\n",
      "9.153180220313848\n",
      "[ 0.18856198 -0.0189431 ]\n",
      "Gradient Descent(740/999): loss=9.153180220313848, w0=-2.8516084635999897, w1=1.088135199183136\n",
      "[ 0.18822208 -0.01890895]\n",
      "9.152462578450615\n",
      "[ 0.18822208 -0.01890895]\n",
      "Gradient Descent(741/999): loss=9.152462578450615, w0=-2.8534906844314714, w1=1.0883242886817661\n",
      "[ 0.18788279 -0.01887486]\n",
      "9.15174752149196\n",
      "[ 0.18788279 -0.01887486]\n",
      "Gradient Descent(742/999): loss=9.15174752149196, w0=-2.8553695123795473, w1=1.088513037328456\n",
      "[ 0.18754412 -0.01884084]\n",
      "9.151035040127201\n",
      "[ 0.18754412 -0.01884084]\n",
      "Gradient Descent(743/999): loss=9.151035040127201, w0=-2.8572449535602154, w1=1.088701445737624\n",
      "[ 0.18720605 -0.01880688]\n",
      "9.150325125079204\n",
      "[ 0.18720605 -0.01880688]\n",
      "Gradient Descent(744/999): loss=9.150325125079204, w0=-2.859117014078448, w1=1.0888895145225812\n",
      "[ 0.18686859 -0.01877298]\n",
      "9.149617767104242\n",
      "[ 0.18686859 -0.01877298]\n",
      "Gradient Descent(745/999): loss=9.149617767104242, w0=-2.860985700028213, w1=1.0890772442955325\n",
      "[ 0.18653175 -0.01873914]\n",
      "9.148912956991884\n",
      "[ 0.18653175 -0.01873914]\n",
      "Gradient Descent(746/999): loss=9.148912956991884, w0=-2.8628510174924937, w1=1.0892646356675795\n",
      "[ 0.18619551 -0.01870536]\n",
      "9.148210685564877\n",
      "[ 0.18619551 -0.01870536]\n",
      "Gradient Descent(747/999): loss=9.148210685564877, w0=-2.864712972543308, w1=1.0894516892487223\n",
      "[ 0.18585987 -0.01867164]\n",
      "9.147510943679022\n",
      "[ 0.18585987 -0.01867164]\n",
      "Gradient Descent(748/999): loss=9.147510943679022, w0=-2.866571571241728, w1=1.0896384056478612\n",
      "[ 0.18552484 -0.01863798]\n",
      "9.146813722223058\n",
      "[ 0.18552484 -0.01863798]\n",
      "Gradient Descent(749/999): loss=9.146813722223058, w0=-2.868426819637901, w1=1.0898247854727994\n",
      "[ 0.18519041 -0.01860439]\n",
      "9.146119012118543\n",
      "[ 0.18519041 -0.01860439]\n",
      "Gradient Descent(750/999): loss=9.146119012118543, w0=-2.8702787237710674, w1=1.090010829330244\n",
      "[ 0.18485659 -0.01857085]\n",
      "9.145426804319733\n",
      "[ 0.18485659 -0.01857085]\n",
      "Gradient Descent(751/999): loss=9.145426804319733, w0=-2.872127289669582, w1=1.0901965378258085\n",
      "[ 0.18452337 -0.01853737]\n",
      "9.14473708981347\n",
      "[ 0.18452337 -0.01853737]\n",
      "Gradient Descent(752/999): loss=9.14473708981347, w0=-2.8739725233509326, w1=1.090381911564015\n",
      "[ 0.18419075 -0.01850396]\n",
      "9.14404985961905\n",
      "[ 0.18419075 -0.01850396]\n",
      "Gradient Descent(753/999): loss=9.14404985961905, w0=-2.8758144308217597, w1=1.0905669511482956\n",
      "[ 0.18385873 -0.0184706 ]\n",
      "9.143365104788137\n",
      "[ 0.18385873 -0.0184706 ]\n",
      "Gradient Descent(754/999): loss=9.143365104788137, w0=-2.877653018077877, w1=1.0907516571809948\n",
      "[ 0.1835273  -0.01843731]\n",
      "9.142682816404605\n",
      "[ 0.1835273  -0.01843731]\n",
      "Gradient Descent(755/999): loss=9.142682816404605, w0=-2.879488291104289, w1=1.0909360302633713\n",
      "[ 0.18319648 -0.01840407]\n",
      "9.142002985584458\n",
      "[ 0.18319648 -0.01840407]\n",
      "Gradient Descent(756/999): loss=9.142002985584458, w0=-2.8813202558752127, w1=1.0911200709956\n",
      "[ 0.18286625 -0.0183709 ]\n",
      "9.141325603475691\n",
      "[ 0.18286625 -0.0183709 ]\n",
      "Gradient Descent(757/999): loss=9.141325603475691, w0=-2.8831489183540957, w1=1.0913037799767737\n",
      "[ 0.18253661 -0.01833778]\n",
      "9.140650661258189\n",
      "[ 0.18253661 -0.01833778]\n",
      "Gradient Descent(758/999): loss=9.140650661258189, w0=-2.8849742844936355, w1=1.0914871578049057\n",
      "[ 0.18220757 -0.01830473]\n",
      "9.139978150143605\n",
      "[ 0.18220757 -0.01830473]\n",
      "Gradient Descent(759/999): loss=9.139978150143605, w0=-2.8867963602357998, w1=1.091670205076931\n",
      "[ 0.18187913 -0.01827173]\n",
      "9.139308061375246\n",
      "[ 0.18187913 -0.01827173]\n",
      "Gradient Descent(760/999): loss=9.139308061375246, w0=-2.8886151515118454, w1=1.0918529223887083\n",
      "[ 0.18155127 -0.01823879]\n",
      "9.13864038622796\n",
      "[ 0.18155127 -0.01823879]\n",
      "Gradient Descent(761/999): loss=9.13864038622796, w0=-2.890430664242337, w1=1.0920353103350233\n",
      "[ 0.18122401 -0.01820592]\n",
      "9.137975116008024\n",
      "[ 0.18122401 -0.01820592]\n",
      "Gradient Descent(762/999): loss=9.137975116008024, w0=-2.892242904337167, w1=1.0922173695095885\n",
      "[ 0.18089734 -0.0181731 ]\n",
      "9.137312242053028\n",
      "[ 0.18089734 -0.0181731 ]\n",
      "Gradient Descent(763/999): loss=9.137312242053028, w0=-2.8940518776955746, w1=1.0923991005050462\n",
      "[ 0.18057125 -0.01814034]\n",
      "9.136651755731764\n",
      "[ 0.18057125 -0.01814034]\n",
      "Gradient Descent(764/999): loss=9.136651755731764, w0=-2.8958575902061656, w1=1.0925805039129708\n",
      "[ 0.18024575 -0.01810764]\n",
      "9.135993648444112\n",
      "[ 0.18024575 -0.01810764]\n",
      "Gradient Descent(765/999): loss=9.135993648444112, w0=-2.8976600477469305, w1=1.0927615803238706\n",
      "[ 0.17992084 -0.018075  ]\n",
      "9.135337911620931\n",
      "[ 0.17992084 -0.018075  ]\n",
      "Gradient Descent(766/999): loss=9.135337911620931, w0=-2.8994592561852643, w1=1.0929423303271886\n",
      "[ 0.17959652 -0.01804242]\n",
      "9.134684536723945\n",
      "[ 0.17959652 -0.01804242]\n",
      "Gradient Descent(767/999): loss=9.134684536723945, w0=-2.9012552213779856, w1=1.0931227545113058\n",
      "[ 0.17927278 -0.0180099 ]\n",
      "9.134033515245635\n",
      "[ 0.17927278 -0.0180099 ]\n",
      "Gradient Descent(768/999): loss=9.134033515245635, w0=-2.9030479491713552, w1=1.0933028534635427\n",
      "[ 0.17894962 -0.01797743]\n",
      "9.133384838709118\n",
      "[ 0.17894962 -0.01797743]\n",
      "Gradient Descent(769/999): loss=9.133384838709118, w0=-2.904837445401096, w1=1.093482627770161\n",
      "[ 0.17862705 -0.01794502]\n",
      "9.132738498668052\n",
      "[ 0.17862705 -0.01794502]\n",
      "Gradient Descent(770/999): loss=9.132738498668052, w0=-2.906623715892411, w1=1.0936620780163653\n",
      "[ 0.17830506 -0.01791268]\n",
      "9.132094486706514\n",
      "[ 0.17830506 -0.01791268]\n",
      "Gradient Descent(771/999): loss=9.132094486706514, w0=-2.9084067664600024, w1=1.0938412047863058\n",
      "[ 0.17798364 -0.01788039]\n",
      "9.131452794438896\n",
      "[ 0.17798364 -0.01788039]\n",
      "Gradient Descent(772/999): loss=9.131452794438896, w0=-2.9101866029080914, w1=1.0940200086630791\n",
      "[ 0.17766281 -0.01784816]\n",
      "9.130813413509793\n",
      "[ 0.17766281 -0.01784816]\n",
      "Gradient Descent(773/999): loss=9.130813413509793, w0=-2.9119632310304366, w1=1.0941984902287314\n",
      "[ 0.17734256 -0.01781598]\n",
      "9.130176335593902\n",
      "[ 0.17734256 -0.01781598]\n",
      "Gradient Descent(774/999): loss=9.130176335593902, w0=-2.9137366566103524, w1=1.0943766500642593\n",
      "[ 0.17702288 -0.01778387]\n",
      "9.129541552395898\n",
      "[ 0.17702288 -0.01778387]\n",
      "Gradient Descent(775/999): loss=9.129541552395898, w0=-2.9155068854207284, w1=1.0945544887496121\n",
      "[ 0.17670378 -0.01775181]\n",
      "9.128909055650336\n",
      "[ 0.17670378 -0.01775181]\n",
      "Gradient Descent(776/999): loss=9.128909055650336, w0=-2.917273923224048, w1=1.094732006863694\n",
      "[ 0.17638525 -0.01771981]\n",
      "9.128278837121554\n",
      "[ 0.17638525 -0.01771981]\n",
      "Gradient Descent(777/999): loss=9.128278837121554, w0=-2.9190377757724075, w1=1.0949092049843652\n",
      "[ 0.1760673  -0.01768787]\n",
      "9.127650888603544\n",
      "[ 0.1760673  -0.01768787]\n",
      "Gradient Descent(778/999): loss=9.127650888603544, w0=-2.920798448807534, w1=1.0950860836884444\n",
      "[ 0.17574993 -0.01765599]\n",
      "9.127025201919858\n",
      "[ 0.17574993 -0.01765599]\n",
      "Gradient Descent(779/999): loss=9.127025201919858, w0=-2.922555948060804, w1=1.0952626435517108\n",
      "[ 0.17543312 -0.01762416]\n",
      "9.126401768923495\n",
      "[ 0.17543312 -0.01762416]\n",
      "Gradient Descent(780/999): loss=9.126401768923495, w0=-2.9243102792532647, w1=1.0954388851489054\n",
      "[ 0.17511688 -0.01759239]\n",
      "9.125780581496812\n",
      "[ 0.17511688 -0.01759239]\n",
      "Gradient Descent(781/999): loss=9.125780581496812, w0=-2.9260614480956484, w1=1.0956148090537334\n",
      "[ 0.17480122 -0.01756068]\n",
      "9.12516163155139\n",
      "[ 0.17480122 -0.01756068]\n",
      "Gradient Descent(782/999): loss=9.12516163155139, w0=-2.9278094602883944, w1=1.0957904158388658\n",
      "[ 0.17448612 -0.01752902]\n",
      "9.124544911027952\n",
      "[ 0.17448612 -0.01752902]\n",
      "Gradient Descent(783/999): loss=9.124544911027952, w0=-2.929554321521666, w1=1.0959657060759411\n",
      "[ 0.1741716  -0.01749743]\n",
      "9.123930411896248\n",
      "[ 0.1741716  -0.01749743]\n",
      "Gradient Descent(784/999): loss=9.123930411896248, w0=-2.93129603747537, w1=1.0961406803355676\n",
      "[ 0.17385763 -0.01746589]\n",
      "9.123318126154956\n",
      "[ 0.17385763 -0.01746589]\n",
      "Gradient Descent(785/999): loss=9.123318126154956, w0=-2.933034613819174, w1=1.0963153391873246\n",
      "[ 0.17354424 -0.0174344 ]\n",
      "9.122708045831567\n",
      "[ 0.17354424 -0.0174344 ]\n",
      "Gradient Descent(786/999): loss=9.122708045831567, w0=-2.9347700562125256, w1=1.0964896831997655\n",
      "[ 0.17323141 -0.01740297]\n",
      "9.122100162982298\n",
      "[ 0.17323141 -0.01740297]\n",
      "Gradient Descent(787/999): loss=9.122100162982298, w0=-2.936502370304671, w1=1.0966637129404178\n",
      "[ 0.17291914 -0.0173716 ]\n",
      "9.12149446969197\n",
      "[ 0.17291914 -0.0173716 ]\n",
      "Gradient Descent(788/999): loss=9.12149446969197, w0=-2.9382315617346726, w1=1.0968374289757872\n",
      "[ 0.17260744 -0.01734029]\n",
      "9.12089095807392\n",
      "[ 0.17260744 -0.01734029]\n",
      "Gradient Descent(789/999): loss=9.12089095807392, w0=-2.939957636131428, w1=1.097010831871357\n",
      "[ 0.1722963  -0.01730903]\n",
      "9.120289620269892\n",
      "[ 0.1722963  -0.01730903]\n",
      "Gradient Descent(790/999): loss=9.120289620269892, w0=-2.9416805991136887, w1=1.0971839221915918\n",
      "[ 0.17198572 -0.01727783]\n",
      "9.119690448449933\n",
      "[ 0.17198572 -0.01727783]\n",
      "Gradient Descent(791/999): loss=9.119690448449933, w0=-2.9434004562900773, w1=1.0973567004999385\n",
      "[ 0.1716757  -0.01724669]\n",
      "9.119093434812289\n",
      "[ 0.1716757  -0.01724669]\n",
      "Gradient Descent(792/999): loss=9.119093434812289, w0=-2.9451172132591066, w1=1.0975291673588288\n",
      "[ 0.17136624 -0.0172156 ]\n",
      "9.118498571583315\n",
      "[ 0.17136624 -0.0172156 ]\n",
      "Gradient Descent(793/999): loss=9.118498571583315, w0=-2.9468308756091974, w1=1.0977013233296797\n",
      "[ 0.17105733 -0.01718456]\n",
      "9.117905851017362\n",
      "[ 0.17105733 -0.01718456]\n",
      "Gradient Descent(794/999): loss=9.117905851017362, w0=-2.948541448918697, w1=1.0978731689728973\n",
      "[ 0.17074898 -0.01715359]\n",
      "9.117315265396682\n",
      "[ 0.17074898 -0.01715359]\n",
      "Gradient Descent(795/999): loss=9.117315265396682, w0=-2.9502489387558963, w1=1.0980447048478763\n",
      "[ 0.17044119 -0.01712267]\n",
      "9.116726807031329\n",
      "[ 0.17044119 -0.01712267]\n",
      "Gradient Descent(796/999): loss=9.116726807031329, w0=-2.9519533506790503, w1=1.0982159315130038\n",
      "[ 0.17013396 -0.0170918 ]\n",
      "9.116140468259045\n",
      "[ 0.17013396 -0.0170918 ]\n",
      "Gradient Descent(797/999): loss=9.116140468259045, w0=-2.953654690236394, w1=1.0983868495256603\n",
      "[ 0.16982727 -0.01706099]\n",
      "9.115556241445184\n",
      "[ 0.16982727 -0.01706099]\n",
      "Gradient Descent(798/999): loss=9.115556241445184, w0=-2.955352962966161, w1=1.0985574594422216\n",
      "[ 0.16952114 -0.01703024]\n",
      "9.11497411898259\n",
      "[ 0.16952114 -0.01703024]\n",
      "Gradient Descent(799/999): loss=9.11497411898259, w0=-2.957048174396602, w1=1.09872776181806\n",
      "[ 0.16921556 -0.01699954]\n",
      "9.11439409329151\n",
      "[ 0.16921556 -0.01699954]\n",
      "Gradient Descent(800/999): loss=9.11439409329151, w0=-2.9587403300460022, w1=1.0988977572075473\n",
      "[ 0.16891054 -0.0169689 ]\n",
      "9.113816156819501\n",
      "[ 0.16891054 -0.0169689 ]\n",
      "Gradient Descent(801/999): loss=9.113816156819501, w0=-2.9604294354226997, w1=1.0990674461640562\n",
      "[ 0.16860606 -0.01693831]\n",
      "9.113240302041309\n",
      "[ 0.16860606 -0.01693831]\n",
      "Gradient Descent(802/999): loss=9.113240302041309, w0=-2.9621154960251035, w1=1.0992368292399608\n",
      "[ 0.16830213 -0.01690777]\n",
      "9.112666521458799\n",
      "[ 0.16830213 -0.01690777]\n",
      "Gradient Descent(803/999): loss=9.112666521458799, w0=-2.963798517341711, w1=1.0994059069866409\n",
      "[ 0.16799875 -0.0168773 ]\n",
      "9.112094807600833\n",
      "[ 0.16799875 -0.0168773 ]\n",
      "Gradient Descent(804/999): loss=9.112094807600833, w0=-2.965478504851126, w1=1.0995746799544812\n",
      "[ 0.16769592 -0.01684687]\n",
      "9.111525153023189\n",
      "[ 0.16769592 -0.01684687]\n",
      "Gradient Descent(805/999): loss=9.111525153023189, w0=-2.9671554640220763, w1=1.0997431486928748\n",
      "[ 0.16739363 -0.01681651]\n",
      "9.110957550308463\n",
      "[ 0.16739363 -0.01681651]\n",
      "Gradient Descent(806/999): loss=9.110957550308463, w0=-2.968829400313433, w1=1.0999113137502248\n",
      "[ 0.16709189 -0.01678619]\n",
      "9.110391992065955\n",
      "[ 0.16709189 -0.01678619]\n",
      "Gradient Descent(807/999): loss=9.110391992065955, w0=-2.9705003191742256, w1=1.100079175673945\n",
      "[ 0.16679069 -0.01675593]\n",
      "9.109828470931603\n",
      "[ 0.16679069 -0.01675593]\n",
      "Gradient Descent(808/999): loss=9.109828470931603, w0=-2.972168226043662, w1=1.1002467350104628\n",
      "[ 0.16649003 -0.01672573]\n",
      "9.109266979567852\n",
      "[ 0.16649003 -0.01672573]\n",
      "Gradient Descent(809/999): loss=9.109266979567852, w0=-2.973833126351145, w1=1.100413992305221\n",
      "[ 0.16618992 -0.01669558]\n",
      "9.108707510663594\n",
      "[ 0.16618992 -0.01669558]\n",
      "Gradient Descent(810/999): loss=9.108707510663594, w0=-2.9754950255162913, w1=1.1005809481026785\n",
      "[ 0.16589034 -0.01666548]\n",
      "9.108150056934042\n",
      "[ 0.16589034 -0.01666548]\n",
      "Gradient Descent(811/999): loss=9.108150056934042, w0=-2.977153928948947, w1=1.1007476029463132\n",
      "[ 0.16559131 -0.01663544]\n",
      "9.107594611120653\n",
      "[ 0.16559131 -0.01663544]\n",
      "Gradient Descent(812/999): loss=9.107594611120653, w0=-2.978809842049207, w1=1.100913957378623\n",
      "[ 0.16529282 -0.01660546]\n",
      "9.10704116599103\n",
      "[ 0.16529282 -0.01660546]\n",
      "Gradient Descent(813/999): loss=9.10704116599103, w0=-2.9804627702074318, w1=1.1010800119411286\n",
      "[ 0.16499486 -0.01657552]\n",
      "9.106489714338824\n",
      "[ 0.16499486 -0.01657552]\n",
      "Gradient Descent(814/999): loss=9.106489714338824, w0=-2.9821127188042658, w1=1.1012457671743734\n",
      "[ 0.16469744 -0.01654564]\n",
      "9.105940248983648\n",
      "[ 0.16469744 -0.01654564]\n",
      "Gradient Descent(815/999): loss=9.105940248983648, w0=-2.983759693210654, w1=1.1014112236179276\n",
      "[ 0.16440056 -0.01651582]\n",
      "9.10539276277097\n",
      "[ 0.16440056 -0.01651582]\n",
      "Gradient Descent(816/999): loss=9.10539276277097, w0=-2.985403698787859, w1=1.1015763818103879\n",
      "[ 0.16410421 -0.01648605]\n",
      "9.104847248572035\n",
      "[ 0.16410421 -0.01648605]\n",
      "Gradient Descent(817/999): loss=9.104847248572035, w0=-2.9870447408874803, w1=1.1017412422893806\n",
      "[ 0.1638084  -0.01645633]\n",
      "9.104303699283765\n",
      "[ 0.1638084  -0.01645633]\n",
      "Gradient Descent(818/999): loss=9.104303699283765, w0=-2.9886828248514705, w1=1.101905805591563\n",
      "[ 0.16351312 -0.01642667]\n",
      "9.103762107828661\n",
      "[ 0.16351312 -0.01642667]\n",
      "Gradient Descent(819/999): loss=9.103762107828661, w0=-2.9903179560121522, w1=1.1020700722526242\n",
      "[ 0.16321837 -0.01639706]\n",
      "9.103222467154726\n",
      "[ 0.16321837 -0.01639706]\n",
      "Gradient Descent(820/999): loss=9.103222467154726, w0=-2.9919501396922366, w1=1.1022340428072888\n",
      "[ 0.16292415 -0.0163675 ]\n",
      "9.102684770235355\n",
      "[ 0.16292415 -0.0163675 ]\n",
      "Gradient Descent(821/999): loss=9.102684770235355, w0=-2.9935793812048392, w1=1.1023977177893167\n",
      "[ 0.16263046 -0.01633799]\n",
      "9.102149010069258\n",
      "[ 0.16263046 -0.01633799]\n",
      "Gradient Descent(822/999): loss=9.102149010069258, w0=-2.9952056858534997, w1=1.102561097731506\n",
      "[ 0.16233731 -0.01630854]\n",
      "9.101615179680362\n",
      "[ 0.16233731 -0.01630854]\n",
      "Gradient Descent(823/999): loss=9.101615179680362, w0=-2.996829058932196, w1=1.1027241831656942\n",
      "[ 0.16204468 -0.01627915]\n",
      "9.101083272117718\n",
      "[ 0.16204468 -0.01627915]\n",
      "Gradient Descent(824/999): loss=9.101083272117718, w0=-2.9984495057253646, w1=1.10288697462276\n",
      "[ 0.16175258 -0.0162498 ]\n",
      "9.100553280455422\n",
      "[ 0.16175258 -0.0162498 ]\n",
      "Gradient Descent(825/999): loss=9.100553280455422, w0=-3.000067031507915, w1=1.1030494726326259\n",
      "[ 0.161461   -0.01622051]\n",
      "9.100025197792506\n",
      "[ 0.161461   -0.01622051]\n",
      "Gradient Descent(826/999): loss=9.100025197792506, w0=-3.001681641545249, w1=1.1032116777242582\n",
      "[ 0.16116995 -0.01619127]\n",
      "9.099499017252867\n",
      "[ 0.16116995 -0.01619127]\n",
      "Gradient Descent(827/999): loss=9.099499017252867, w0=-3.0032933410932765, w1=1.10337359042567\n",
      "[ 0.16087943 -0.01616208]\n",
      "9.098974731985166\n",
      "[ 0.16087943 -0.01616208]\n",
      "Gradient Descent(828/999): loss=9.098974731985166, w0=-3.0049021353984338, w1=1.1035352112639227\n",
      "[ 0.16058943 -0.01613295]\n",
      "9.098452335162744\n",
      "[ 0.16058943 -0.01613295]\n",
      "Gradient Descent(829/999): loss=9.098452335162744, w0=-3.006508029697699, w1=1.103696540765128\n",
      "[ 0.16029995 -0.01610387]\n",
      "9.097931819983529\n",
      "[ 0.16029995 -0.01610387]\n",
      "Gradient Descent(830/999): loss=9.097931819983529, w0=-3.008111029218611, w1=1.1038575794544485\n",
      "[ 0.160011   -0.01607484]\n",
      "9.09741317966995\n",
      "[ 0.160011   -0.01607484]\n",
      "Gradient Descent(831/999): loss=9.09741317966995, w0=-3.0097111391792852, w1=1.1040183278561004\n",
      "[ 0.15972256 -0.01604586]\n",
      "9.09689640746885\n",
      "[ 0.15972256 -0.01604586]\n",
      "Gradient Descent(832/999): loss=9.09689640746885, w0=-3.0113083647884307, w1=1.1041787864933554\n",
      "[ 0.15943465 -0.01601694]\n",
      "9.096381496651397\n",
      "[ 0.15943465 -0.01601694]\n",
      "Gradient Descent(833/999): loss=9.096381496651397, w0=-3.0129027112453675, w1=1.1043389558885413\n",
      "[ 0.15914725 -0.01598807]\n",
      "9.095868440512994\n",
      "[ 0.15914725 -0.01598807]\n",
      "Gradient Descent(834/999): loss=9.095868440512994, w0=-3.014494183740043, w1=1.1044988365630448\n",
      "[ 0.15886037 -0.01595925]\n",
      "9.095357232373194\n",
      "[ 0.15886037 -0.01595925]\n",
      "Gradient Descent(835/999): loss=9.095357232373194, w0=-3.0160827874530503, w1=1.1046584290373125\n",
      "[ 0.15857401 -0.01593048]\n",
      "9.09484786557561\n",
      "[ 0.15857401 -0.01593048]\n",
      "Gradient Descent(836/999): loss=9.09484786557561, w0=-3.0176685275556423, w1=1.104817733830853\n",
      "[ 0.15828817 -0.01590176]\n",
      "9.094340333487837\n",
      "[ 0.15828817 -0.01590176]\n",
      "Gradient Descent(837/999): loss=9.094340333487837, w0=-3.019251409209752, w1=1.1049767514622382\n",
      "[ 0.15800284 -0.0158731 ]\n",
      "9.093834629501353\n",
      "[ 0.15800284 -0.0158731 ]\n",
      "Gradient Descent(838/999): loss=9.093834629501353, w0=-3.020831437568006, w1=1.1051354824491055\n",
      "[ 0.15771802 -0.01584449]\n",
      "9.093330747031448\n",
      "[ 0.15771802 -0.01584449]\n",
      "Gradient Descent(839/999): loss=9.093330747031448, w0=-3.022408617773744, w1=1.1052939273081595\n",
      "[ 0.15743372 -0.01581592]\n",
      "9.09282867951712\n",
      "[ 0.15743372 -0.01581592]\n",
      "Gradient Descent(840/999): loss=9.09282867951712, w0=-3.023982954961034, w1=1.1054520865551725\n",
      "[ 0.15714993 -0.01578741]\n",
      "9.092328420421007\n",
      "[ 0.15714993 -0.01578741]\n",
      "Gradient Descent(841/999): loss=9.092328420421007, w0=-3.0255544542546886, w1=1.1056099607049876\n",
      "[ 0.15686665 -0.01575896]\n",
      "9.091829963229284\n",
      "[ 0.15686665 -0.01575896]\n",
      "Gradient Descent(842/999): loss=9.091829963229284, w0=-3.0271231207702836, w1=1.10576755027152\n",
      "[ 0.15658388 -0.01573055]\n",
      "9.0913333014516\n",
      "[ 0.15658388 -0.01573055]\n",
      "Gradient Descent(843/999): loss=9.0913333014516, w0=-3.0286889596141724, w1=1.1059248557677583\n",
      "[ 0.15630163 -0.01570219]\n",
      "9.09083842862098\n",
      "[ 0.15630163 -0.01570219]\n",
      "Gradient Descent(844/999): loss=9.09083842862098, w0=-3.0302519758835045, w1=1.1060818777057666\n",
      "[ 0.15601988 -0.01567389]\n",
      "9.090345338293737\n",
      "[ 0.15601988 -0.01567389]\n",
      "Gradient Descent(845/999): loss=9.090345338293737, w0=-3.0318121746662405, w1=1.1062386165966855\n",
      "[ 0.15573864 -0.01564564]\n",
      "9.089854024049394\n",
      "[ 0.15573864 -0.01564564]\n",
      "Gradient Descent(846/999): loss=9.089854024049394, w0=-3.0333695610411704, w1=1.1063950729507346\n",
      "[ 0.1554579  -0.01561743]\n",
      "9.089364479490612\n",
      "[ 0.1554579  -0.01561743]\n",
      "Gradient Descent(847/999): loss=9.089364479490612, w0=-3.034924140077929, w1=1.1065512472772134\n",
      "[ 0.15517768 -0.01558928]\n",
      "9.088876698243075\n",
      "[ 0.15517768 -0.01558928]\n",
      "Gradient Descent(848/999): loss=9.088876698243075, w0=-3.036475916837012, w1=1.106707140084504\n",
      "[ 0.15489795 -0.01556118]\n",
      "9.088390673955443\n",
      "[ 0.15489795 -0.01556118]\n",
      "Gradient Descent(849/999): loss=9.088390673955443, w0=-3.038024896369793, w1=1.106862751880072\n",
      "[ 0.15461873 -0.01553313]\n",
      "9.087906400299248\n",
      "[ 0.15461873 -0.01553313]\n",
      "Gradient Descent(850/999): loss=9.087906400299248, w0=-3.0395710837185415, w1=1.1070180831704672\n",
      "[ 0.15434002 -0.01550513]\n",
      "9.087423870968816\n",
      "[ 0.15434002 -0.01550513]\n",
      "Gradient Descent(851/999): loss=9.087423870968816, w0=-3.0411144839164357, w1=1.1071731344613276\n",
      "[ 0.15406181 -0.01547718]\n",
      "9.086943079681182\n",
      "[ 0.15406181 -0.01547718]\n",
      "Gradient Descent(852/999): loss=9.086943079681182, w0=-3.0426551019875827, w1=1.1073279062573789\n",
      "[ 0.1537841  -0.01544928]\n",
      "9.086464020176024\n",
      "[ 0.1537841  -0.01544928]\n",
      "Gradient Descent(853/999): loss=9.086464020176024, w0=-3.0441929429470327, w1=1.1074823990624372\n",
      "[ 0.15350689 -0.01542143]\n",
      "9.085986686215556\n",
      "[ 0.15350689 -0.01542143]\n",
      "Gradient Descent(854/999): loss=9.085986686215556, w0=-3.045728011800795, w1=1.1076366133794109\n",
      "[ 0.15323017 -0.01539363]\n",
      "9.085511071584465\n",
      "[ 0.15323017 -0.01539363]\n",
      "Gradient Descent(855/999): loss=9.085511071584465, w0=-3.0472603135458565, w1=1.107790549710301\n",
      "[ 0.15295396 -0.01536588]\n",
      "9.085037170089834\n",
      "[ 0.15295396 -0.01536588]\n",
      "Gradient Descent(856/999): loss=9.085037170089834, w0=-3.048789853170195, w1=1.1079442085562046\n",
      "[ 0.15267825 -0.01533819]\n",
      "9.084564975561033\n",
      "[ 0.15267825 -0.01533819]\n",
      "Gradient Descent(857/999): loss=9.084564975561033, w0=-3.0503166356527984, w1=1.1080975904173147\n",
      "[ 0.15240303 -0.01531054]\n",
      "9.084094481849675\n",
      "[ 0.15240303 -0.01531054]\n",
      "Gradient Descent(858/999): loss=9.084094481849675, w0=-3.0518406659636788, w1=1.1082506957929228\n",
      "[ 0.15212831 -0.01528294]\n",
      "9.083625682829515\n",
      "[ 0.15212831 -0.01528294]\n",
      "Gradient Descent(859/999): loss=9.083625682829515, w0=-3.053361949063889, w1=1.1084035251814206\n",
      "[ 0.15185408 -0.01525539]\n",
      "9.083158572396371\n",
      "[ 0.15185408 -0.01525539]\n",
      "Gradient Descent(860/999): loss=9.083158572396371, w0=-3.05488048990554, w1=1.1085560790803013\n",
      "[ 0.15158035 -0.01522789]\n",
      "9.082693144468054\n",
      "[ 0.15158035 -0.01522789]\n",
      "Gradient Descent(861/999): loss=9.082693144468054, w0=-3.056396293431815, w1=1.1087083579861612\n",
      "[ 0.15130711 -0.01520044]\n",
      "9.08222939298427\n",
      "[ 0.15130711 -0.01520044]\n",
      "Gradient Descent(862/999): loss=9.08222939298427, w0=-3.0579093645769877, w1=1.108860362394702\n",
      "[ 0.15103437 -0.01517304]\n",
      "9.081767311906573\n",
      "[ 0.15103437 -0.01517304]\n",
      "Gradient Descent(863/999): loss=9.081767311906573, w0=-3.0594197082664367, w1=1.109012092800731\n",
      "[ 0.15076212 -0.01514569]\n",
      "9.08130689521825\n",
      "[ 0.15076212 -0.01514569]\n",
      "Gradient Descent(864/999): loss=9.08130689521825, w0=-3.0609273294166623, w1=1.109163549698164\n",
      "[ 0.15049035 -0.01511839]\n",
      "9.080848136924265\n",
      "[ 0.15049035 -0.01511839]\n",
      "Gradient Descent(865/999): loss=9.080848136924265, w0=-3.0624322329353024, w1=1.1093147335800269\n",
      "[ 0.15021908 -0.01509114]\n",
      "9.080391031051182\n",
      "[ 0.15021908 -0.01509114]\n",
      "Gradient Descent(866/999): loss=9.080391031051182, w0=-3.0639344237211485, w1=1.109465644938456\n",
      "[ 0.14994829 -0.01506393]\n",
      "9.079935571647075\n",
      "[ 0.14994829 -0.01506393]\n",
      "Gradient Descent(867/999): loss=9.079935571647075, w0=-3.065433906664161, w1=1.1096162842647008\n",
      "[ 0.149678   -0.01503678]\n",
      "9.07948175278146\n",
      "[ 0.149678   -0.01503678]\n",
      "Gradient Descent(868/999): loss=9.07948175278146, w0=-3.066930686645487, w1=1.1097666520491256\n",
      "[ 0.14940819 -0.01500967]\n",
      "9.079029568545208\n",
      "[ 0.14940819 -0.01500967]\n",
      "Gradient Descent(869/999): loss=9.079029568545208, w0=-3.0684247685374726, w1=1.1099167487812107\n",
      "[ 0.14913887 -0.01498262]\n",
      "9.078579013050476\n",
      "[ 0.14913887 -0.01498262]\n",
      "Gradient Descent(870/999): loss=9.078579013050476, w0=-3.069916157203683, w1=1.1100665749495537\n",
      "[ 0.14887003 -0.01495561]\n",
      "9.07813008043064\n",
      "[ 0.14887003 -0.01495561]\n",
      "Gradient Descent(871/999): loss=9.07813008043064, w0=-3.071404857498916, w1=1.1102161310418717\n",
      "[ 0.14860168 -0.01492865]\n",
      "9.077682764840189\n",
      "[ 0.14860168 -0.01492865]\n",
      "Gradient Descent(872/999): loss=9.077682764840189, w0=-3.0728908742692176, w1=1.1103654175450026\n",
      "[ 0.14833381 -0.01490174]\n",
      "9.077237060454685\n",
      "[ 0.14833381 -0.01490174]\n",
      "Gradient Descent(873/999): loss=9.077237060454685, w0=-3.0743742123518984, w1=1.110514434944907\n",
      "[ 0.14806642 -0.01487488]\n",
      "9.076792961470654\n",
      "[ 0.14806642 -0.01487488]\n",
      "Gradient Descent(874/999): loss=9.076792961470654, w0=-3.07585487657555, w1=1.1106631837266687\n",
      "[ 0.14779952 -0.01484806]\n",
      "9.076350462105534\n",
      "[ 0.14779952 -0.01484806]\n",
      "Gradient Descent(875/999): loss=9.076350462105534, w0=-3.0773328717600594, w1=1.1108116643744983\n",
      "[ 0.1475331 -0.0148213]\n",
      "9.075909556597592\n",
      "[ 0.1475331 -0.0148213]\n",
      "Gradient Descent(876/999): loss=9.075909556597592, w0=-3.0788082027166253, w1=1.1109598773717324\n",
      "[ 0.14726715 -0.01479458]\n",
      "9.075470239205846\n",
      "[ 0.14726715 -0.01479458]\n",
      "Gradient Descent(877/999): loss=9.075470239205846, w0=-3.080280874247774, w1=1.1111078232008371\n",
      "[ 0.14700169 -0.01476791]\n",
      "9.07503250420999\n",
      "[ 0.14700169 -0.01476791]\n",
      "Gradient Descent(878/999): loss=9.07503250420999, w0=-3.081750891147374, w1=1.1112555023434083\n",
      "[ 0.14673671 -0.01474129]\n",
      "9.07459634591033\n",
      "[ 0.14673671 -0.01474129]\n",
      "Gradient Descent(879/999): loss=9.07459634591033, w0=-3.0832182582006538, w1=1.1114029152801743\n",
      "[ 0.1464722  -0.01471472]\n",
      "9.074161758627694\n",
      "[ 0.1464722  -0.01471472]\n",
      "Gradient Descent(880/999): loss=9.074161758627694, w0=-3.084682980184215, w1=1.1115500624909966\n",
      "[ 0.14620817 -0.0146882 ]\n",
      "9.073728736703371\n",
      "[ 0.14620817 -0.0146882 ]\n",
      "Gradient Descent(881/999): loss=9.073728736703371, w0=-3.0861450618660493, w1=1.1116969444548714\n",
      "[ 0.14594461 -0.01466172]\n",
      "9.073297274499032\n",
      "[ 0.14594461 -0.01466172]\n",
      "Gradient Descent(882/999): loss=9.073297274499032, w0=-3.0876045080055534, w1=1.111843561649932\n",
      "[ 0.14568153 -0.01463529]\n",
      "9.072867366396652\n",
      "[ 0.14568153 -0.01463529]\n",
      "Gradient Descent(883/999): loss=9.072867366396652, w0=-3.0890613233535453, w1=1.1119899145534495\n",
      "[ 0.14541893 -0.01460891]\n",
      "9.072439006798446\n",
      "[ 0.14541893 -0.01460891]\n",
      "Gradient Descent(884/999): loss=9.072439006798446, w0=-3.0905155126522783, w1=1.1121360036418346\n",
      "[ 0.1451568  -0.01458257]\n",
      "9.072012190126795\n",
      "[ 0.1451568  -0.01458257]\n",
      "Gradient Descent(885/999): loss=9.072012190126795, w0=-3.091967080635458, w1=1.1122818293906394\n",
      "[ 0.14489514 -0.01455629]\n",
      "9.071586910824163\n",
      "[ 0.14489514 -0.01455629]\n",
      "Gradient Descent(886/999): loss=9.071586910824163, w0=-3.0934160320282573, w1=1.112427392274559\n",
      "[ 0.14463395 -0.01453005]\n",
      "9.071163163353036\n",
      "[ 0.14463395 -0.01453005]\n",
      "Gradient Descent(887/999): loss=9.071163163353036, w0=-3.0948623715473302, w1=1.112572692767432\n",
      "[ 0.14437324 -0.01450386]\n",
      "9.070740942195844\n",
      "[ 0.14437324 -0.01450386]\n",
      "Gradient Descent(888/999): loss=9.070740942195844, w0=-3.09630610390083, w1=1.1127177313422438\n",
      "[ 0.14411299 -0.01447771]\n",
      "9.070320241854898\n",
      "[ 0.14411299 -0.01447771]\n",
      "Gradient Descent(889/999): loss=9.070320241854898, w0=-3.097747233788422, w1=1.1128625084711268\n",
      "[ 0.14385321 -0.01445162]\n",
      "9.0699010568523\n",
      "[ 0.14385321 -0.01445162]\n",
      "Gradient Descent(890/999): loss=9.0699010568523, w0=-3.099185765901301, w1=1.113007024625362\n",
      "[ 0.1435939  -0.01442557]\n",
      "9.06948338172989\n",
      "[ 0.1435939  -0.01442557]\n",
      "Gradient Descent(891/999): loss=9.06948338172989, w0=-3.1006217049222045, w1=1.1131512802753818\n",
      "[ 0.14333506 -0.01439956]\n",
      "9.069067211049168\n",
      "[ 0.14333506 -0.01439956]\n",
      "Gradient Descent(892/999): loss=9.069067211049168, w0=-3.102055055525429, w1=1.1132952758907695\n",
      "[ 0.14307669 -0.0143736 ]\n",
      "9.068652539391223\n",
      "[ 0.14307669 -0.0143736 ]\n",
      "Gradient Descent(893/999): loss=9.068652539391223, w0=-3.103485822376846, w1=1.1134390119402626\n",
      "[ 0.14281878 -0.0143477 ]\n",
      "9.068239361356657\n",
      "[ 0.14281878 -0.0143477 ]\n",
      "Gradient Descent(894/999): loss=9.068239361356657, w0=-3.104914010133915, w1=1.1135824888917536\n",
      "[ 0.14256133 -0.01432183]\n",
      "9.067827671565526\n",
      "[ 0.14256133 -0.01432183]\n",
      "Gradient Descent(895/999): loss=9.067827671565526, w0=-3.106339623445701, w1=1.1137257072122915\n",
      "[ 0.14230435 -0.01429602]\n",
      "9.067417464657268\n",
      "[ 0.14230435 -0.01429602]\n",
      "Gradient Descent(896/999): loss=9.067417464657268, w0=-3.1077626669528886, w1=1.113868667368083\n",
      "[ 0.14204783 -0.01427025]\n",
      "9.06700873529062\n",
      "[ 0.14204783 -0.01427025]\n",
      "Gradient Descent(897/999): loss=9.06700873529062, w0=-3.1091831452877967, w1=1.1140113698244951\n",
      "[ 0.14179178 -0.01424452]\n",
      "9.066601478143564\n",
      "[ 0.14179178 -0.01424452]\n",
      "Gradient Descent(898/999): loss=9.066601478143564, w0=-3.110601063074394, w1=1.1141538150460557\n",
      "[ 0.14153619 -0.01421885]\n",
      "9.066195687913247\n",
      "[ 0.14153619 -0.01421885]\n",
      "Gradient Descent(899/999): loss=9.066195687913247, w0=-3.1120164249283144, w1=1.114296003496455\n",
      "[ 0.14128105 -0.01419321]\n",
      "9.065791359315922\n",
      "[ 0.14128105 -0.01419321]\n",
      "Gradient Descent(900/999): loss=9.065791359315922, w0=-3.113429235456871, w1=1.114437935638548\n",
      "[ 0.14102638 -0.01416763]\n",
      "9.065388487086874\n",
      "[ 0.14102638 -0.01416763]\n",
      "Gradient Descent(901/999): loss=9.065388487086874, w0=-3.1148394992590727, w1=1.1145796119343547\n",
      "[ 0.14077217 -0.01414209]\n",
      "9.064987065980343\n",
      "[ 0.14077217 -0.01414209]\n",
      "Gradient Descent(902/999): loss=9.064987065980343, w0=-3.1162472209256373, w1=1.1147210328450625\n",
      "[ 0.14051841 -0.0141166 ]\n",
      "9.064587090769473\n",
      "[ 0.14051841 -0.0141166 ]\n",
      "Gradient Descent(903/999): loss=9.064587090769473, w0=-3.1176524050390086, w1=1.1148621988310277\n",
      "[ 0.14026511 -0.01409115]\n",
      "9.06418855624623\n",
      "[ 0.14026511 -0.01409115]\n",
      "Gradient Descent(904/999): loss=9.06418855624623, w0=-3.119055056173369, w1=1.1150031103517763\n",
      "[ 0.14001227 -0.01406575]\n",
      "9.06379145722134\n",
      "[ 0.14001227 -0.01406575]\n",
      "Gradient Descent(905/999): loss=9.06379145722134, w0=-3.1204551788946557, w1=1.1151437678660068\n",
      "[ 0.13975989 -0.0140404 ]\n",
      "9.063395788524222\n",
      "[ 0.13975989 -0.0140404 ]\n",
      "Gradient Descent(906/999): loss=9.063395788524222, w0=-3.1218527777605756, w1=1.1152841718315896\n",
      "[ 0.13950796 -0.01401509]\n",
      "9.063001545002916\n",
      "[ 0.13950796 -0.01401509]\n",
      "Gradient Descent(907/999): loss=9.063001545002916, w0=-3.12324785732062, w1=1.1154243227055707\n",
      "[ 0.13925648 -0.01398982]\n",
      "9.062608721524025\n",
      "[ 0.13925648 -0.01398982]\n",
      "Gradient Descent(908/999): loss=9.062608721524025, w0=-3.124640422116079, w1=1.115564220944172\n",
      "[ 0.13900546 -0.01396461]\n",
      "9.062217312972637\n",
      "[ 0.13900546 -0.01396461]\n",
      "Gradient Descent(909/999): loss=9.062217312972637, w0=-3.126030476680057, w1=1.1157038670027932\n",
      "[ 0.13875489 -0.01393943]\n",
      "9.061827314252259\n",
      "[ 0.13875489 -0.01393943]\n",
      "Gradient Descent(910/999): loss=9.061827314252259, w0=-3.1274180255374864, w1=1.1158432613360127\n",
      "[ 0.13850477 -0.01391431]\n",
      "9.061438720284766\n",
      "[ 0.13850477 -0.01391431]\n",
      "Gradient Descent(911/999): loss=9.061438720284766, w0=-3.1288030732051437, w1=1.1159824043975894\n",
      "[ 0.1382551  -0.01388922]\n",
      "9.06105152601032\n",
      "[ 0.1382551  -0.01388922]\n",
      "Gradient Descent(912/999): loss=9.06105152601032, w0=-3.130185624191663, w1=1.116121296640465\n",
      "[ 0.13800588 -0.01386419]\n",
      "9.060665726387306\n",
      "[ 0.13800588 -0.01386419]\n",
      "Gradient Descent(913/999): loss=9.060665726387306, w0=-3.131565682997551, w1=1.1162599385167638\n",
      "[ 0.13775711 -0.0138392 ]\n",
      "9.060281316392269\n",
      "[ 0.13775711 -0.0138392 ]\n",
      "Gradient Descent(914/999): loss=9.060281316392269, w0=-3.1329432541152027, w1=1.1163983304777958\n",
      "[ 0.13750879 -0.01381425]\n",
      "9.05989829101985\n",
      "[ 0.13750879 -0.01381425]\n",
      "Gradient Descent(915/999): loss=9.05989829101985, w0=-3.134318342028914, w1=1.116536472974057\n",
      "[ 0.13726092 -0.01378935]\n",
      "9.059516645282718\n",
      "[ 0.13726092 -0.01378935]\n",
      "Gradient Descent(916/999): loss=9.059516645282718, w0=-3.1356909512148983, w1=1.1166743664552319\n",
      "[ 0.13701349 -0.01376449]\n",
      "9.059136374211509\n",
      "[ 0.13701349 -0.01376449]\n",
      "Gradient Descent(917/999): loss=9.059136374211509, w0=-3.1370610861412995, w1=1.1168120113701938\n",
      "[ 0.13676651 -0.01373968]\n",
      "9.05875747285475\n",
      "[ 0.13676651 -0.01373968]\n",
      "Gradient Descent(918/999): loss=9.05875747285475, w0=-3.138428751268208, w1=1.116949408167007\n",
      "[ 0.13651998 -0.01371491]\n",
      "9.058379936278813\n",
      "[ 0.13651998 -0.01371491]\n",
      "Gradient Descent(919/999): loss=9.058379936278813, w0=-3.139793951047673, w1=1.1170865572929285\n",
      "[ 0.13627389 -0.01369019]\n",
      "9.058003759567837\n",
      "[ 0.13627389 -0.01369019]\n",
      "Gradient Descent(920/999): loss=9.058003759567837, w0=-3.141156689923721, w1=1.1172234591944088\n",
      "[ 0.13602824 -0.01366551]\n",
      "9.057628937823667\n",
      "[ 0.13602824 -0.01366551]\n",
      "Gradient Descent(921/999): loss=9.057628937823667, w0=-3.142516972332365, w1=1.1173601143170935\n",
      "[ 0.13578304 -0.01364088]\n",
      "9.057255466165783\n",
      "[ 0.13578304 -0.01364088]\n",
      "Gradient Descent(922/999): loss=9.057255466165783, w0=-3.1438748027016237, w1=1.1174965231058251\n",
      "[ 0.13553827 -0.01361629]\n",
      "9.056883339731263\n",
      "[ 0.13553827 -0.01361629]\n",
      "Gradient Descent(923/999): loss=9.056883339731263, w0=-3.1452301854515325, w1=1.1176326860046442\n",
      "[ 0.13529395 -0.01359175]\n",
      "9.056512553674683\n",
      "[ 0.13529395 -0.01359175]\n",
      "Gradient Descent(924/999): loss=9.056512553674683, w0=-3.14658312499416, w1=1.1177686034567906\n",
      "[ 0.13505007 -0.01356724]\n",
      "9.05614310316808\n",
      "[ 0.13505007 -0.01356724]\n",
      "Gradient Descent(925/999): loss=9.05614310316808, w0=-3.147933625733622, w1=1.117904275904706\n",
      "[ 0.13480663 -0.01354279]\n",
      "9.055774983400884\n",
      "[ 0.13480663 -0.01354279]\n",
      "Gradient Descent(926/999): loss=9.055774983400884, w0=-3.149281692066094, w1=1.1180397037900336\n",
      "[ 0.13456363 -0.01351838]\n",
      "9.055408189579843\n",
      "[ 0.13456363 -0.01351838]\n",
      "Gradient Descent(927/999): loss=9.055408189579843, w0=-3.1506273283798283, w1=1.118174887553621\n",
      "[ 0.13432107 -0.01349401]\n",
      "9.05504271692898\n",
      "[ 0.13432107 -0.01349401]\n",
      "Gradient Descent(928/999): loss=9.05504271692898, w0=-3.1519705390551667, w1=1.1183098276355212\n",
      "[ 0.13407894 -0.01346968]\n",
      "9.054678560689513\n",
      "[ 0.13407894 -0.01346968]\n",
      "Gradient Descent(929/999): loss=9.054678560689513, w0=-3.153311328464554, w1=1.1184445244749937\n",
      "[ 0.13383725 -0.0134454 ]\n",
      "9.054315716119808\n",
      "[ 0.13383725 -0.0134454 ]\n",
      "Gradient Descent(930/999): loss=9.054315716119808, w0=-3.154649700972555, w1=1.1185789785105058\n",
      "[ 0.133596   -0.01342117]\n",
      "9.0539541784953\n",
      "[ 0.133596   -0.01342117]\n",
      "Gradient Descent(931/999): loss=9.0539541784953, w0=-3.155985660935866, w1=1.1187131901797356\n",
      "[ 0.13335518 -0.01339697]\n",
      "9.053593943108455\n",
      "[ 0.13335518 -0.01339697]\n",
      "Gradient Descent(932/999): loss=9.053593943108455, w0=-3.1573192127033294, w1=1.1188471599195713\n",
      "[ 0.13311479 -0.01337282]\n",
      "9.05323500526868\n",
      "[ 0.13311479 -0.01337282]\n",
      "Gradient Descent(933/999): loss=9.05323500526868, w0=-3.1586503606159493, w1=1.1189808881661139\n",
      "[ 0.13287484 -0.01334872]\n",
      "9.052877360302292\n",
      "[ 0.13287484 -0.01334872]\n",
      "Gradient Descent(934/999): loss=9.052877360302292, w0=-3.1599791090069043, w1=1.119114375354678\n",
      "[ 0.13263532 -0.01332466]\n",
      "9.052521003552435\n",
      "[ 0.13263532 -0.01332466]\n",
      "Gradient Descent(935/999): loss=9.052521003552435, w0=-3.1613054622015624, w1=1.1192476219197935\n",
      "[ 0.13239623 -0.01330064]\n",
      "9.052165930379024\n",
      "[ 0.13239623 -0.01330064]\n",
      "Gradient Descent(936/999): loss=9.052165930379024, w0=-3.162629424517494, w1=1.119380628295208\n",
      "[ 0.13215757 -0.01327666]\n",
      "9.051812136158695\n",
      "[ 0.13215757 -0.01327666]\n",
      "Gradient Descent(937/999): loss=9.051812136158695, w0=-3.1639510002644875, w1=1.1195133949138856\n",
      "[ 0.13191935 -0.01325273]\n",
      "9.05145961628473\n",
      "[ 0.13191935 -0.01325273]\n",
      "Gradient Descent(938/999): loss=9.05145961628473, w0=-3.1652701937445618, w1=1.1196459222080113\n",
      "[ 0.13168155 -0.01322884]\n",
      "9.051108366167005\n",
      "[ 0.13168155 -0.01322884]\n",
      "Gradient Descent(939/999): loss=9.051108366167005, w0=-3.1665870092519817, w1=1.1197782106089904\n",
      "[ 0.13144418 -0.01320499]\n",
      "9.050758381231937\n",
      "[ 0.13144418 -0.01320499]\n",
      "Gradient Descent(940/999): loss=9.050758381231937, w0=-3.1679014510732704, w1=1.1199102605474507\n",
      "[ 0.13120724 -0.01318119]\n",
      "9.050409656922406\n",
      "[ 0.13120724 -0.01318119]\n",
      "Gradient Descent(941/999): loss=9.050409656922406, w0=-3.1692135234872247, w1=1.1200420724532436\n",
      "[ 0.13097073 -0.01315743]\n",
      "9.050062188697712\n",
      "[ 0.13097073 -0.01315743]\n",
      "Gradient Descent(942/999): loss=9.050062188697712, w0=-3.1705232307649283, w1=1.1201736467554462\n",
      "[ 0.13073464 -0.01313371]\n",
      "9.049715972033514\n",
      "[ 0.13073464 -0.01313371]\n",
      "Gradient Descent(943/999): loss=9.049715972033514, w0=-3.171830577169766, w1=1.1203049838823613\n",
      "[ 0.13049898 -0.01311004]\n",
      "9.049371002421758\n",
      "[ 0.13049898 -0.01311004]\n",
      "Gradient Descent(944/999): loss=9.049371002421758, w0=-3.1731355669574373, w1=1.1204360842615202\n",
      "[ 0.13026374 -0.01308641]\n",
      "9.049027275370637\n",
      "[ 0.13026374 -0.01308641]\n",
      "Gradient Descent(945/999): loss=9.049027275370637, w0=-3.1744382043759707, w1=1.1205669483196834\n",
      "[ 0.13002893 -0.01306282]\n",
      "9.048684786404518\n",
      "[ 0.13002893 -0.01306282]\n",
      "Gradient Descent(946/999): loss=9.048684786404518, w0=-3.1757384936657367, w1=1.1206975764828424\n",
      "[ 0.12979454 -0.01303927]\n",
      "9.04834353106389\n",
      "[ 0.12979454 -0.01303927]\n",
      "Gradient Descent(947/999): loss=9.04834353106389, w0=-3.1770364390594623, w1=1.1208279691762202\n",
      "[ 0.12956057 -0.01301576]\n",
      "9.048003504905308\n",
      "[ 0.12956057 -0.01301576]\n",
      "Gradient Descent(948/999): loss=9.048003504905308, w0=-3.178332044782245, w1=1.1209581268242739\n",
      "[ 0.12932703 -0.0129923 ]\n",
      "9.047664703501322\n",
      "[ 0.12932703 -0.0129923 ]\n",
      "Gradient Descent(949/999): loss=9.047664703501322, w0=-3.179625315051566, w1=1.121088049850695\n",
      "[ 0.1290939  -0.01296888]\n",
      "9.047327122440445\n",
      "[ 0.1290939  -0.01296888]\n",
      "Gradient Descent(950/999): loss=9.047327122440445, w0=-3.1809162540773035, w1=1.1212177386784117\n",
      "[ 0.1288612  -0.01294551]\n",
      "9.046990757327066\n",
      "[ 0.1288612  -0.01294551]\n",
      "Gradient Descent(951/999): loss=9.046990757327066, w0=-3.1822048660617477, w1=1.1213471937295894\n",
      "[ 0.12862891 -0.01292217]\n",
      "9.046655603781414\n",
      "[ 0.12862891 -0.01292217]\n",
      "Gradient Descent(952/999): loss=9.046655603781414, w0=-3.1834911551996132, w1=1.1214764154256327\n",
      "[ 0.12839705 -0.01289888]\n",
      "9.046321657439492\n",
      "[ 0.12839705 -0.01289888]\n",
      "Gradient Descent(953/999): loss=9.046321657439492, w0=-3.184775125678054, w1=1.1216054041871868\n",
      "[ 0.1281656  -0.01287562]\n",
      "9.045988913953023\n",
      "[ 0.1281656  -0.01287562]\n",
      "Gradient Descent(954/999): loss=9.045988913953023, w0=-3.1860567816766756, w1=1.1217341604341384\n",
      "[ 0.12793457 -0.01285242]\n",
      "9.045657368989387\n",
      "[ 0.12793457 -0.01285242]\n",
      "Gradient Descent(955/999): loss=9.045657368989387, w0=-3.1873361273675496, w1=1.1218626845856172\n",
      "[ 0.12770395 -0.01282925]\n",
      "9.045327018231578\n",
      "[ 0.12770395 -0.01282925]\n",
      "Gradient Descent(956/999): loss=9.045327018231578, w0=-3.1886131669152276, w1=1.1219909770599976\n",
      "[ 0.12747376 -0.01280612]\n",
      "9.044997857378137\n",
      "[ 0.12747376 -0.01280612]\n",
      "Gradient Descent(957/999): loss=9.044997857378137, w0=-3.189887904476753, w1=1.1221190382748998\n",
      "[ 0.12724397 -0.01278304]\n",
      "9.044669882143094\n",
      "[ 0.12724397 -0.01278304]\n",
      "Gradient Descent(958/999): loss=9.044669882143094, w0=-3.191160344201677, w1=1.1222468686471911\n",
      "[ 0.1270146  -0.01275999]\n",
      "9.04434308825592\n",
      "[ 0.1270146  -0.01275999]\n",
      "Gradient Descent(959/999): loss=9.04434308825592, w0=-3.19243049023207, w1=1.1223744685929875\n",
      "[ 0.12678565 -0.01273699]\n",
      "9.044017471461467\n",
      "[ 0.12678565 -0.01273699]\n",
      "Gradient Descent(960/999): loss=9.044017471461467, w0=-3.193698346702536, w1=1.1225018385276548\n",
      "[ 0.1265571  -0.01271403]\n",
      "9.043693027519918\n",
      "[ 0.1265571  -0.01271403]\n",
      "Gradient Descent(961/999): loss=9.043693027519918, w0=-3.194963917740226, w1=1.1226289788658101\n",
      "[ 0.12632897 -0.01269112]\n",
      "9.04336975220672\n",
      "[ 0.12632897 -0.01269112]\n",
      "Gradient Descent(962/999): loss=9.04336975220672, w0=-3.1962272074648523, w1=1.1227558900213233\n",
      "[ 0.12610125 -0.01266824]\n",
      "9.04304764131255\n",
      "[ 0.12610125 -0.01266824]\n",
      "Gradient Descent(963/999): loss=9.04304764131255, w0=-3.1974882199887, w1=1.1228825724073177\n",
      "[ 0.12587394 -0.0126454 ]\n",
      "9.042726690643226\n",
      "[ 0.12587394 -0.0126454 ]\n",
      "Gradient Descent(964/999): loss=9.042726690643226, w0=-3.1987469594166416, w1=1.1230090264361725\n",
      "[ 0.12564704 -0.01262261]\n",
      "9.042406896019697\n",
      "[ 0.12564704 -0.01262261]\n",
      "Gradient Descent(965/999): loss=9.042406896019697, w0=-3.20000342984615, w1=1.1231352525195233\n",
      "[ 0.12542055 -0.01259985]\n",
      "9.042088253277944\n",
      "[ 0.12542055 -0.01259985]\n",
      "Gradient Descent(966/999): loss=9.042088253277944, w0=-3.2012576353673126, w1=1.1232612510682636\n",
      "[ 0.12519447 -0.01257714]\n",
      "9.041770758268957\n",
      "[ 0.12519447 -0.01257714]\n",
      "Gradient Descent(967/999): loss=9.041770758268957, w0=-3.2025095800628436, w1=1.1233870224925466\n",
      "[ 0.12496879 -0.01255447]\n",
      "9.041454406858673\n",
      "[ 0.12496879 -0.01255447]\n",
      "Gradient Descent(968/999): loss=9.041454406858673, w0=-3.203759268008098, w1=1.1235125672017858\n",
      "[ 0.12474353 -0.01253184]\n",
      "9.04113919492791\n",
      "[ 0.12474353 -0.01253184]\n",
      "Gradient Descent(969/999): loss=9.04113919492791, w0=-3.2050067032710845, w1=1.1236378856046567\n",
      "[ 0.12451866 -0.01250925]\n",
      "9.04082511837233\n",
      "[ 0.12451866 -0.01250925]\n",
      "Gradient Descent(970/999): loss=9.04082511837233, w0=-3.2062518899124783, w1=1.123762978109098\n",
      "[ 0.12429421 -0.0124867 ]\n",
      "9.040512173102382\n",
      "[ 0.12429421 -0.0124867 ]\n",
      "Gradient Descent(971/999): loss=9.040512173102382, w0=-3.207494831985636, w1=1.1238878451223138\n",
      "[ 0.12407016 -0.01246419]\n",
      "9.040200355043234\n",
      "[ 0.12407016 -0.01246419]\n",
      "Gradient Descent(972/999): loss=9.040200355043234, w0=-3.2087355335366063, w1=1.1240124870507733\n",
      "[ 0.12384651 -0.01244172]\n",
      "9.03988966013474\n",
      "[ 0.12384651 -0.01244172]\n",
      "Gradient Descent(973/999): loss=9.03988966013474, w0=-3.2099739986041453, w1=1.1241369043002134\n",
      "[ 0.12362326 -0.0124193 ]\n",
      "9.039580084331376\n",
      "[ 0.12362326 -0.0124193 ]\n",
      "Gradient Descent(974/999): loss=9.039580084331376, w0=-3.2112102312197286, w1=1.1242610972756395\n",
      "[ 0.12340042 -0.01239691]\n",
      "9.039271623602193\n",
      "[ 0.12340042 -0.01239691]\n",
      "Gradient Descent(975/999): loss=9.039271623602193, w0=-3.212444235407565, w1=1.124385066381327\n",
      "[ 0.12317798 -0.01237456]\n",
      "9.038964273930754\n",
      "[ 0.12317798 -0.01237456]\n",
      "Gradient Descent(976/999): loss=9.038964273930754, w0=-3.213676015184609, w1=1.1245088120208229\n",
      "[ 0.12295594 -0.01235226]\n",
      "9.038658031315094\n",
      "[ 0.12295594 -0.01235226]\n",
      "Gradient Descent(977/999): loss=9.038658031315094, w0=-3.214905574560574, w1=1.124632334596946\n",
      "[ 0.1227343  -0.01232999]\n",
      "9.038352891767666\n",
      "[ 0.1227343  -0.01232999]\n",
      "Gradient Descent(978/999): loss=9.038352891767666, w0=-3.216132917537946, w1=1.1247556345117895\n",
      "[ 0.12251306 -0.01230777]\n",
      "9.038048851315276\n",
      "[ 0.12251306 -0.01230777]\n",
      "Gradient Descent(979/999): loss=9.038048851315276, w0=-3.2173580481119957, w1=1.1248787121667216\n",
      "[ 0.12229222 -0.01228558]\n",
      "9.03774590599905\n",
      "[ 0.12229222 -0.01228558]\n",
      "Gradient Descent(980/999): loss=9.03774590599905, w0=-3.218580970270792, w1=1.1250015679623875\n",
      "[ 0.12207177 -0.01226343]\n",
      "9.037444051874367\n",
      "[ 0.12207177 -0.01226343]\n",
      "Gradient Descent(981/999): loss=9.037444051874367, w0=-3.219801687995215, w1=1.1251242022987094\n",
      "[ 0.12185173 -0.01224133]\n",
      "9.037143285010826\n",
      "[ 0.12185173 -0.01224133]\n",
      "Gradient Descent(982/999): loss=9.037143285010826, w0=-3.221020205258969, w1=1.125246615574889\n",
      "[ 0.12163208 -0.01221926]\n",
      "9.036843601492167\n",
      "[ 0.12163208 -0.01221926]\n",
      "Gradient Descent(983/999): loss=9.036843601492167, w0=-3.2222365260285954, w1=1.1253688081894087\n",
      "[ 0.12141282 -0.01219724]\n",
      "9.036544997416252\n",
      "[ 0.12141282 -0.01219724]\n",
      "Gradient Descent(984/999): loss=9.036544997416252, w0=-3.223450654263485, w1=1.1254907805400318\n",
      "[ 0.12119397 -0.01217525]\n",
      "9.036247468894983\n",
      "[ 0.12119397 -0.01217525]\n",
      "Gradient Descent(985/999): loss=9.036247468894983, w0=-3.2246625939158915, w1=1.1256125330238056\n",
      "[ 0.1209755 -0.0121533]\n",
      "9.035951012054277\n",
      "[ 0.1209755 -0.0121533]\n",
      "Gradient Descent(986/999): loss=9.035951012054277, w0=-3.2258723489309453, w1=1.1257340660370607\n",
      "[ 0.12075743 -0.01213139]\n",
      "9.035655623034003\n",
      "[ 0.12075743 -0.01213139]\n",
      "Gradient Descent(987/999): loss=9.035655623034003, w0=-3.227079923246664, w1=1.125855379975414\n",
      "[ 0.12053975 -0.01210953]\n",
      "9.035361297987938\n",
      "[ 0.12053975 -0.01210953]\n",
      "Gradient Descent(988/999): loss=9.035361297987938, w0=-3.228285320793967, w1=1.125976475233769\n",
      "[ 0.12032247 -0.0120877 ]\n",
      "9.035068033083698\n",
      "[ 0.12032247 -0.0120877 ]\n",
      "Gradient Descent(989/999): loss=9.035068033083698, w0=-3.2294885454966886, w1=1.1260973522063173\n",
      "[ 0.12010558 -0.01206591]\n",
      "9.034775824502722\n",
      "[ 0.12010558 -0.01206591]\n",
      "Gradient Descent(990/999): loss=9.034775824502722, w0=-3.230689601271589, w1=1.12621801128654\n",
      "[ 0.11988908 -0.01204416]\n",
      "9.034484668440191\n",
      "[ 0.11988908 -0.01204416]\n",
      "Gradient Descent(991/999): loss=9.034484668440191, w0=-3.2318884920283684, w1=1.126338452867209\n",
      "[ 0.11967296 -0.01202245]\n",
      "9.034194561104995\n",
      "[ 0.11967296 -0.01202245]\n",
      "Gradient Descent(992/999): loss=9.034194561104995, w0=-3.2330852216696795, w1=1.126458677340388\n",
      "[ 0.11945724 -0.01200078]\n",
      "9.033905498719678\n",
      "[ 0.11945724 -0.01200078]\n",
      "Gradient Descent(993/999): loss=9.033905498719678, w0=-3.23427979409114, w1=1.1265786850974338\n",
      "[ 0.11924191 -0.01197914]\n",
      "9.033617477520393\n",
      "[ 0.11924191 -0.01197914]\n",
      "Gradient Descent(994/999): loss=9.033617477520393, w0=-3.235472213181345, w1=1.1266984765289985\n",
      "[ 0.11902696 -0.01195755]\n",
      "9.033330493756846\n",
      "[ 0.11902696 -0.01195755]\n",
      "Gradient Descent(995/999): loss=9.033330493756846, w0=-3.236662482821881, w1=1.1268180520250288\n",
      "[ 0.11881241 -0.01193599]\n",
      "9.033044543692252\n",
      "[ 0.11881241 -0.01193599]\n",
      "Gradient Descent(996/999): loss=9.033044543692252, w0=-3.2378506068873367, w1=1.12693741197477\n",
      "[ 0.11859824 -0.01191448]\n",
      "9.032759623603292\n",
      "[ 0.11859824 -0.01191448]\n",
      "Gradient Descent(997/999): loss=9.032759623603292, w0=-3.2390365892453166, w1=1.1270565567667645\n",
      "[ 0.11838445 -0.011893  ]\n",
      "9.032475729780046\n",
      "[ 0.11838445 -0.011893  ]\n",
      "Gradient Descent(998/999): loss=9.032475729780046, w0=-3.2402204337564537, w1=1.1271754867888548\n",
      "[ 0.11817105 -0.01187156]\n",
      "9.032192858525969\n",
      "[ 0.11817105 -0.01187156]\n",
      "Gradient Descent(999/999): loss=9.032192858525969, w0=-3.241402144274422, w1=1.1272942024281842\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-3.24140214,  1.1272942 ])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g, cost = gradientDescent(X, y, theta, alpha, iters)\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can compute the cost (error) of the trained model using our fitted parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(9.031911006157827)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeCost(X, y, g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the linear model along with the data to visually see how well it fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Predicted Profit vs. Population Size')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(data.Population.min(), data.Population.max(), 100)\n",
    "f = g[0] + (g[1] * x)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.plot(x, f, 'r', label='Prediction')\n",
    "ax.scatter(data.Population, data.Profit, label='Traning Data')\n",
    "ax.legend(loc=2)\n",
    "ax.set_xlabel('Population')\n",
    "ax.set_ylabel('Profit')\n",
    "ax.set_title('Predicted Profit vs. Population Size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks pretty good!  Since the gradient decent function also outputs a vector with the cost at each training iteration, we can plot that as well.  Notice that the cost always decreases - this is an example of a convex optimization problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Error vs. Training Epoch')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.plot(np.arange(iters), cost, 'r')\n",
    "ax.set_xlabel('Iterations')\n",
    "ax.set_ylabel('Cost')\n",
    "ax.set_title('Error vs. Training Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlav",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
